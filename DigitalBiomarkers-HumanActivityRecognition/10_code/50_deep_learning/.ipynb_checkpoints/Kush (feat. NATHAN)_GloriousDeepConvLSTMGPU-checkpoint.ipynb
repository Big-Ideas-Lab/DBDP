{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Pytorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "-k5uDodYO7YZ",
    "outputId": "ac82039f-d9fc-4f27-e6fc-360af71a219e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: window-slider in /opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install window-slider #install package to do the sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vD1yuHINcnT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "mtklaOvbNoyQ",
    "outputId": "a7d6bf89-a1a9-4903-837c-22160e67653b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.275354</td>\n",
       "      <td>15.25</td>\n",
       "      <td>78.9800</td>\n",
       "      <td>63.410094</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>78.8350</td>\n",
       "      <td>63.453054</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>78.6900</td>\n",
       "      <td>63.496142</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>18.39</td>\n",
       "      <td>78.5450</td>\n",
       "      <td>63.539358</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.34</td>\n",
       "      <td>0.268950</td>\n",
       "      <td>13.61</td>\n",
       "      <td>78.4000</td>\n",
       "      <td>63.582702</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293035</th>\n",
       "      <td>21.176471</td>\n",
       "      <td>-11.176471</td>\n",
       "      <td>64.823529</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.708502</td>\n",
       "      <td>0.85</td>\n",
       "      <td>92.8275</td>\n",
       "      <td>69.104605</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293036</th>\n",
       "      <td>24.235294</td>\n",
       "      <td>-12.235294</td>\n",
       "      <td>62.764706</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.694414</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>92.8800</td>\n",
       "      <td>68.384649</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293037</th>\n",
       "      <td>27.294118</td>\n",
       "      <td>-13.294118</td>\n",
       "      <td>60.705882</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>5.22</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>67.874197</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293038</th>\n",
       "      <td>30.352941</td>\n",
       "      <td>-14.352941</td>\n",
       "      <td>58.647059</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.664957</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>93.0000</td>\n",
       "      <td>67.577995</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293039</th>\n",
       "      <td>33.411765</td>\n",
       "      <td>-15.411765</td>\n",
       "      <td>56.588235</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.648308</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.0600</td>\n",
       "      <td>67.498866</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293040 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ACC1       ACC2       ACC3   TEMP       EDA    BVP       HR  \\\n",
       "0       41.000000  27.200000  40.000000  32.39  0.275354  15.25  78.9800   \n",
       "1       41.000000  27.300000  40.000000  32.39  0.276634 -12.75  78.8350   \n",
       "2       41.000000  27.400000  40.000000  32.39  0.270231 -42.99  78.6900   \n",
       "3       41.000000  27.500000  40.000000  32.39  0.270231  18.39  78.5450   \n",
       "4       41.000000  27.600000  40.000000  32.34  0.268950  13.61  78.4000   \n",
       "...           ...        ...        ...    ...       ...    ...      ...   \n",
       "293035  21.176471 -11.176471  64.823529  32.09  0.708502   0.85  92.8275   \n",
       "293036  24.235294 -12.235294  62.764706  32.09  0.694414  -1.00  92.8800   \n",
       "293037  27.294118 -13.294118  60.705882  32.09  0.672642   5.22  92.9400   \n",
       "293038  30.352941 -14.352941  58.647059  32.09  0.664957  -1.47  93.0000   \n",
       "293039  33.411765 -15.411765  56.588235  32.09  0.648308   1.37  93.0600   \n",
       "\n",
       "        Magnitude  Activity Subject_ID  Round  \n",
       "0       63.410094  Baseline     19-001      1  \n",
       "1       63.453054  Baseline     19-001      1  \n",
       "2       63.496142  Baseline     19-001      1  \n",
       "3       63.539358  Baseline     19-001      1  \n",
       "4       63.582702  Baseline     19-001      1  \n",
       "...           ...       ...        ...    ...  \n",
       "293035  69.104605      Type     19-056      1  \n",
       "293036  68.384649      Type     19-056      1  \n",
       "293037  67.874197      Type     19-056      1  \n",
       "293038  67.577995      Type     19-056      1  \n",
       "293039  67.498866      Type     19-056      1  \n",
       "\n",
       "[293040 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data-2020/10_code/csv/final_data_rounds.csv') #read in the csv file\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDx4Yw2ZNtiO"
   },
   "outputs": [],
   "source": [
    "NB_SENSOR_CHANNELS = 8\n",
    "SLIDING_WINDOW_LENGTH = 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m9rNB-RYz7je",
    "outputId": "6571efbf-d402-4514-fff0-443f4ad73159"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41....</td>\n",
       "      <td>[27.2, 27.3, 27.4, 27.5, 27.6, 27.7, 27.8, 27....</td>\n",
       "      <td>[40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40....</td>\n",
       "      <td>[32.39, 32.39, 32.39, 32.39, 32.34, 32.34, 32....</td>\n",
       "      <td>[0.275354, 0.276634, 0.270231, 0.270231, 0.268...</td>\n",
       "      <td>[15.25, -12.75, -42.99, 18.39, 13.61, -9.66, -...</td>\n",
       "      <td>[78.98, 78.83500000000002, 78.69, 78.545, 78.4...</td>\n",
       "      <td>[63.410093833710725, 63.453053512025726, 63.49...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0, 39.06521739130435, 39.130434782608695, ...</td>\n",
       "      <td>[29.0, 28.93478260869565, 28.869565217391305, ...</td>\n",
       "      <td>[38.0, 38.02173913043478, 38.04347826086956, 3...</td>\n",
       "      <td>[32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....</td>\n",
       "      <td>[0.259985, 0.259985, 0.258704, 0.258704, 0.261...</td>\n",
       "      <td>[-18.83, -0.3, 11.03, 6.09, -15.3, 14.61, 6.75...</td>\n",
       "      <td>[73.52, 73.435, 73.35, 73.265, 73.18, 73.0925,...</td>\n",
       "      <td>[61.69278726074872, 61.7168170027034, 61.74098...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392, 41.67391304347826, 41.7391...</td>\n",
       "      <td>[26.39130434782609, 26.32608695652174, 26.2608...</td>\n",
       "      <td>[38.869565217391305, 38.89130434782609, 38.913...</td>\n",
       "      <td>[32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....</td>\n",
       "      <td>[0.265108, 0.263827, 0.266389, 0.265108, 0.266...</td>\n",
       "      <td>[-27.69, 30.51, 14.64, 1.97, -13.65, -48.52, 1...</td>\n",
       "      <td>[69.63, 69.515, 69.4, 69.285, 69.17, 69.04, 68...</td>\n",
       "      <td>[62.758486272725364, 62.78782873035957, 62.817...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575, 44.14285714285715, 44.314...</td>\n",
       "      <td>[24.514285714285712, 24.42857142857143, 24.342...</td>\n",
       "      <td>[37.17142857142857, 37.142857142857146, 37.114...</td>\n",
       "      <td>[32.33, 32.33, 32.33, 32.33, 32.34, 32.34, 32....</td>\n",
       "      <td>[0.258704, 0.258704, 0.258704, 0.257424, 0.257...</td>\n",
       "      <td>[17.18, 2.41, -22.11, 5.12, 18.43, 5.34, -14.3...</td>\n",
       "      <td>[64.68, 64.555, 64.43, 64.305, 64.18, 64.0475,...</td>\n",
       "      <td>[62.579164557660036, 62.649331804179724, 62.72...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418, 45.564516129032256, 45.580...</td>\n",
       "      <td>[25.64516129032258, 25.69354838709677, 25.7419...</td>\n",
       "      <td>[37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37....</td>\n",
       "      <td>[32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....</td>\n",
       "      <td>[0.253581, 0.253581, 0.253581, 0.252301, 0.252...</td>\n",
       "      <td>[-32.0, 15.14, 24.41, 3.88, -22.97, -0.34, 17....</td>\n",
       "      <td>[60.92, 60.8475, 60.77500000000001, 60.7025, 6...</td>\n",
       "      <td>[64.04162603123257, 64.07248675362091, 64.1033...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14.344827586206891, 14.06896551724138, 13.793...</td>\n",
       "      <td>[-11.965517241379313, -11.79310344827586, -11....</td>\n",
       "      <td>[49.96551724137931, 50.793103448275865, 51.620...</td>\n",
       "      <td>[32.21, 32.21, 32.21, 32.21, 32.21, 32.21, 32....</td>\n",
       "      <td>[0.5432899999999999, 0.531763, 0.545851, 0.543...</td>\n",
       "      <td>[2.83, -80.79, 14.21, 42.24, -9.61, -17.43, 7....</td>\n",
       "      <td>[97.15, 97.08, 97.01, 96.94, 96.87, 96.8025, 9...</td>\n",
       "      <td>[53.34323382145407, 54.00881814643157, 54.6807...</td>\n",
       "      <td>19-056</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[16.53333333333333, 16.266666666666666, 16.0, ...</td>\n",
       "      <td>[2.2666666666666675, 3.1333333333333333, 4.0, ...</td>\n",
       "      <td>[69.2, 69.6, 70.0, 70.04545454545455, 70.09090...</td>\n",
       "      <td>[32.21, 32.21, 32.21, 32.21, 32.16, 32.16, 32....</td>\n",
       "      <td>[0.539447, 0.54457, 0.521517, 0.52536, 0.51895...</td>\n",
       "      <td>[-1.4, -3.26, -2.79, -4.55, 1.99, 1.45, -5.31,...</td>\n",
       "      <td>[94.62, 94.5775, 94.535, 94.4925, 94.45, 94.41...</td>\n",
       "      <td>[71.18376843697506, 71.5442675706602, 71.91661...</td>\n",
       "      <td>19-056</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12.26315789473684, 12.342105263157894, 12.421...</td>\n",
       "      <td>[-6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6....</td>\n",
       "      <td>[57.526315789473685, 56.684210526315795, 55.84...</td>\n",
       "      <td>[32.16, 32.16, 32.16, 32.16, 32.18, 32.18, 32....</td>\n",
       "      <td>[0.530482, 0.530482, 0.542009, 0.545851, 0.558...</td>\n",
       "      <td>[1.59, 1.72, 0.36, -1.73, 0.05, 3.58, -0.84, -...</td>\n",
       "      <td>[93.68, 93.64, 93.6, 93.56, 93.52, 93.4575, 93...</td>\n",
       "      <td>[59.12412409382059, 58.321756534920645, 57.520...</td>\n",
       "      <td>19-056</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[16.16, 16.28, 16.4, 16.52, 16.64, 16.75999999...</td>\n",
       "      <td>[-8.879999999999997, -9.04, -9.2, -9.36, -9.52...</td>\n",
       "      <td>[43.32, 43.56, 43.8, 44.04, 44.28, 44.52, 44.7...</td>\n",
       "      <td>[32.15, 32.15, 32.15, 32.15, 32.15, 32.15, 32....</td>\n",
       "      <td>[0.531763, 0.5189560000000001, 0.521517, 0.520...</td>\n",
       "      <td>[-1.31, 1.69, -2.65, -3.59, 6.38, -6.17, 2.61,...</td>\n",
       "      <td>[91.7, 91.6875, 91.675, 91.6625, 91.65, 91.632...</td>\n",
       "      <td>[47.08101953016735, 47.37334271507554, 47.6659...</td>\n",
       "      <td>19-056</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[14.461538461538465, 14.384615384615385, 14.30...</td>\n",
       "      <td>[-8.984615384615385, -8.953846153846154, -8.92...</td>\n",
       "      <td>[58.2, 58.6, 59.0, 59.4, 59.8, 60.2, 60.6, 61....</td>\n",
       "      <td>[32.13, 32.13, 32.13, 32.13, 32.13, 32.13, 32....</td>\n",
       "      <td>[0.567623, 0.565062, 0.565062, 0.5637810000000...</td>\n",
       "      <td>[2.18, 0.31, 0.88, -0.86, 1.16, 3.49, -2.19, 3...</td>\n",
       "      <td>[91.8, 91.8175, 91.835, 91.8525, 91.87, 91.89,...</td>\n",
       "      <td>[60.639091420337294, 61.00039770944169, 61.362...</td>\n",
       "      <td>19-056</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7326 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ACC1  \\\n",
       "0   [41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41....   \n",
       "1   [39.0, 39.06521739130435, 39.130434782608695, ...   \n",
       "2   [41.60869565217392, 41.67391304347826, 41.7391...   \n",
       "3   [43.971428571428575, 44.14285714285715, 44.314...   \n",
       "4   [45.54838709677418, 45.564516129032256, 45.580...   \n",
       "..                                                ...   \n",
       "1   [14.344827586206891, 14.06896551724138, 13.793...   \n",
       "2   [16.53333333333333, 16.266666666666666, 16.0, ...   \n",
       "3   [12.26315789473684, 12.342105263157894, 12.421...   \n",
       "4   [16.16, 16.28, 16.4, 16.52, 16.64, 16.75999999...   \n",
       "5   [14.461538461538465, 14.384615384615385, 14.30...   \n",
       "\n",
       "                                                 ACC2  \\\n",
       "0   [27.2, 27.3, 27.4, 27.5, 27.6, 27.7, 27.8, 27....   \n",
       "1   [29.0, 28.93478260869565, 28.869565217391305, ...   \n",
       "2   [26.39130434782609, 26.32608695652174, 26.2608...   \n",
       "3   [24.514285714285712, 24.42857142857143, 24.342...   \n",
       "4   [25.64516129032258, 25.69354838709677, 25.7419...   \n",
       "..                                                ...   \n",
       "1   [-11.965517241379313, -11.79310344827586, -11....   \n",
       "2   [2.2666666666666675, 3.1333333333333333, 4.0, ...   \n",
       "3   [-6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6....   \n",
       "4   [-8.879999999999997, -9.04, -9.2, -9.36, -9.52...   \n",
       "5   [-8.984615384615385, -8.953846153846154, -8.92...   \n",
       "\n",
       "                                                 ACC3  \\\n",
       "0   [40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40....   \n",
       "1   [38.0, 38.02173913043478, 38.04347826086956, 3...   \n",
       "2   [38.869565217391305, 38.89130434782609, 38.913...   \n",
       "3   [37.17142857142857, 37.142857142857146, 37.114...   \n",
       "4   [37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37....   \n",
       "..                                                ...   \n",
       "1   [49.96551724137931, 50.793103448275865, 51.620...   \n",
       "2   [69.2, 69.6, 70.0, 70.04545454545455, 70.09090...   \n",
       "3   [57.526315789473685, 56.684210526315795, 55.84...   \n",
       "4   [43.32, 43.56, 43.8, 44.04, 44.28, 44.52, 44.7...   \n",
       "5   [58.2, 58.6, 59.0, 59.4, 59.8, 60.2, 60.6, 61....   \n",
       "\n",
       "                                                 TEMP  \\\n",
       "0   [32.39, 32.39, 32.39, 32.39, 32.34, 32.34, 32....   \n",
       "1   [32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....   \n",
       "2   [32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....   \n",
       "3   [32.33, 32.33, 32.33, 32.33, 32.34, 32.34, 32....   \n",
       "4   [32.34, 32.34, 32.34, 32.34, 32.33, 32.33, 32....   \n",
       "..                                                ...   \n",
       "1   [32.21, 32.21, 32.21, 32.21, 32.21, 32.21, 32....   \n",
       "2   [32.21, 32.21, 32.21, 32.21, 32.16, 32.16, 32....   \n",
       "3   [32.16, 32.16, 32.16, 32.16, 32.18, 32.18, 32....   \n",
       "4   [32.15, 32.15, 32.15, 32.15, 32.15, 32.15, 32....   \n",
       "5   [32.13, 32.13, 32.13, 32.13, 32.13, 32.13, 32....   \n",
       "\n",
       "                                                  EDA  \\\n",
       "0   [0.275354, 0.276634, 0.270231, 0.270231, 0.268...   \n",
       "1   [0.259985, 0.259985, 0.258704, 0.258704, 0.261...   \n",
       "2   [0.265108, 0.263827, 0.266389, 0.265108, 0.266...   \n",
       "3   [0.258704, 0.258704, 0.258704, 0.257424, 0.257...   \n",
       "4   [0.253581, 0.253581, 0.253581, 0.252301, 0.252...   \n",
       "..                                                ...   \n",
       "1   [0.5432899999999999, 0.531763, 0.545851, 0.543...   \n",
       "2   [0.539447, 0.54457, 0.521517, 0.52536, 0.51895...   \n",
       "3   [0.530482, 0.530482, 0.542009, 0.545851, 0.558...   \n",
       "4   [0.531763, 0.5189560000000001, 0.521517, 0.520...   \n",
       "5   [0.567623, 0.565062, 0.565062, 0.5637810000000...   \n",
       "\n",
       "                                                  BVP  \\\n",
       "0   [15.25, -12.75, -42.99, 18.39, 13.61, -9.66, -...   \n",
       "1   [-18.83, -0.3, 11.03, 6.09, -15.3, 14.61, 6.75...   \n",
       "2   [-27.69, 30.51, 14.64, 1.97, -13.65, -48.52, 1...   \n",
       "3   [17.18, 2.41, -22.11, 5.12, 18.43, 5.34, -14.3...   \n",
       "4   [-32.0, 15.14, 24.41, 3.88, -22.97, -0.34, 17....   \n",
       "..                                                ...   \n",
       "1   [2.83, -80.79, 14.21, 42.24, -9.61, -17.43, 7....   \n",
       "2   [-1.4, -3.26, -2.79, -4.55, 1.99, 1.45, -5.31,...   \n",
       "3   [1.59, 1.72, 0.36, -1.73, 0.05, 3.58, -0.84, -...   \n",
       "4   [-1.31, 1.69, -2.65, -3.59, 6.38, -6.17, 2.61,...   \n",
       "5   [2.18, 0.31, 0.88, -0.86, 1.16, 3.49, -2.19, 3...   \n",
       "\n",
       "                                                   HR  \\\n",
       "0   [78.98, 78.83500000000002, 78.69, 78.545, 78.4...   \n",
       "1   [73.52, 73.435, 73.35, 73.265, 73.18, 73.0925,...   \n",
       "2   [69.63, 69.515, 69.4, 69.285, 69.17, 69.04, 68...   \n",
       "3   [64.68, 64.555, 64.43, 64.305, 64.18, 64.0475,...   \n",
       "4   [60.92, 60.8475, 60.77500000000001, 60.7025, 6...   \n",
       "..                                                ...   \n",
       "1   [97.15, 97.08, 97.01, 96.94, 96.87, 96.8025, 9...   \n",
       "2   [94.62, 94.5775, 94.535, 94.4925, 94.45, 94.41...   \n",
       "3   [93.68, 93.64, 93.6, 93.56, 93.52, 93.4575, 93...   \n",
       "4   [91.7, 91.6875, 91.675, 91.6625, 91.65, 91.632...   \n",
       "5   [91.8, 91.8175, 91.835, 91.8525, 91.87, 91.89,...   \n",
       "\n",
       "                                            Magnitude Subject_ID  Activity  \n",
       "0   [63.410093833710725, 63.453053512025726, 63.49...     19-001  Baseline  \n",
       "1   [61.69278726074872, 61.7168170027034, 61.74098...     19-001  Baseline  \n",
       "2   [62.758486272725364, 62.78782873035957, 62.817...     19-001  Baseline  \n",
       "3   [62.579164557660036, 62.649331804179724, 62.72...     19-001  Baseline  \n",
       "4   [64.04162603123257, 64.07248675362091, 64.1033...     19-001  Baseline  \n",
       "..                                                ...        ...       ...  \n",
       "1   [53.34323382145407, 54.00881814643157, 54.6807...     19-056      Type  \n",
       "2   [71.18376843697506, 71.5442675706602, 71.91661...     19-056      Type  \n",
       "3   [59.12412409382059, 58.321756534920645, 57.520...     19-056      Type  \n",
       "4   [47.08101953016735, 47.37334271507554, 47.6659...     19-056      Type  \n",
       "5   [60.639091420337294, 61.00039770944169, 61.362...     19-056      Type  \n",
       "\n",
       "[7326 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code will make a dataframe of windows from the dataframe of individual timepoints\n",
    "\n",
    "from window_slider import Slider\n",
    "window_list = []\n",
    "final = pd.DataFrame()\n",
    "activity_list = list(df['Activity'].unique()) #list of the four activities\n",
    "sub_id_list = list(df['Subject_ID'].unique()) #list of the subject ids\n",
    "round_list = list(df['Round'].unique()) #list of the round numbers\n",
    "df_list = []\n",
    "bucket_size = 40 #this is the size of the windows in quarter seconds. 40 = 10 seconds\n",
    "overlap_count = 0 #this is how much overlap each window has with each other\n",
    "                    #an overlap of 30 is identical to stepping by 10\n",
    "\n",
    "    \n",
    "for i in sub_id_list:\n",
    "    df_subject = df[df['Subject_ID'] == i] #isolate a single subject id\n",
    "    for j in activity_list:\n",
    "        df_subject_activity = df_subject[df_subject['Activity'] == j] #further isolate by activity\n",
    "        for k in round_list:\n",
    "            final_df = pd.DataFrame()\n",
    "            df_subject_activity_round = df_subject_activity[df_subject_activity['Round'] == k] #further isolate by round\n",
    "            if df_subject_activity_round.empty:\n",
    "              pass\n",
    "            else:\n",
    "              df_flat = df_subject_activity_round[['ACC1', 'ACC2','ACC3','TEMP','EDA','BVP','HR','Magnitude']].T.values #array of arrays, each row is every single reading in an array for a sensor in that isolation \n",
    "            \n",
    "              slider = Slider(bucket_size,overlap_count)\n",
    "              slider.fit(df_flat)\n",
    "              while True:\n",
    "                  window_data = slider.slide()\n",
    "                  window_list.append(list(window_data))\n",
    "                \n",
    "                  if slider.reached_end_of_list(): break #the slider runs on a data block of a single subject during a single activity round\n",
    "              final_df = final.append(window_list)  \n",
    "              final_df.columns = [['ACC1', 'ACC2','ACC3','TEMP','EDA','BVP','HR','Magnitude']]\n",
    "              final_df.insert(8, \"Subject_ID\", [i]*len(final_df), True)\n",
    "              final_df.insert(9, \"Activity\", [j]*len(final_df), True)\n",
    "                #the windows for the single subject's single activity round are in a single dataframe\n",
    "              df_list.append(final_df) #we're putting each one of those dataframes in a list as you go through the loop\n",
    "              window_list = []\n",
    "    \n",
    "final = pd.DataFrame(columns = df_list[0].columns)\n",
    "\n",
    "for l in df_list:\n",
    "        final = final.append(l) #take the dataframes in the list and make them into one big dataframe\n",
    "        \n",
    "\n",
    "final #this one big dataframe contains all of the data\n",
    "        #we have looked at the values for the output and the code appears to be working fine\n",
    "        #each sensor reading sequence is represented as a numpy array\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvVjIAZzeD01"
   },
   "source": [
    "# Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4JbLO6MNyZD"
   },
   "outputs": [],
   "source": [
    "# Get rid of multi-index \n",
    "final.columns = final.columns.map(''.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "id": "CbDCQvPjYpax",
    "outputId": "f57465dd-dc45-40ca-958c-6b3e10b21f35"
   },
   "outputs": [],
   "source": [
    "#run this if you want to see how many windows each participant has \n",
    "#final['Subject_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjsfToiwNzvv"
   },
   "outputs": [],
   "source": [
    "ID_list = list(final['Subject_ID'].unique())\n",
    "random.shuffle(ID_list) #makes a list of ID's in a random order\n",
    "\n",
    "train = pd.DataFrame() #initialize training set\n",
    "test = pd.DataFrame() #initialize testing set\n",
    "#val = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vk6w0kDQeRLt"
   },
   "source": [
    "### ***TRAIN/TEST SPLIT SIZE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgn9BvgUNzyf"
   },
   "outputs": [],
   "source": [
    "#change size of train/test split\n",
    "train = final[final['Subject_ID'].isin(ID_list[:47])] #subset based on ID\n",
    "test = final[final['Subject_ID'].isin(ID_list[47:56])]\n",
    "#val = df[df['Subject_ID'].isin(ID_list[50:56])] #this is for a later validation set. no code here uses this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPtpMBz2eZvK"
   },
   "source": [
    "### ***SENSOR INPUTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvADiILgN38k"
   },
   "outputs": [],
   "source": [
    "X_train = train[['TEMP', 'EDA', 'HR', 'BVP', 'Magnitude', 'ACC1', 'ACC2', 'ACC3']] #get only sensor values\n",
    "X_train = X_train.apply(pd.Series.explode).reset_index()\n",
    "\n",
    "X_test = test[['TEMP', 'EDA', 'HR', 'BVP', 'Magnitude', 'ACC1', 'ACC2', 'ACC3']]\n",
    "X_test = X_test.apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finish making Test/Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMtz_ju_N61J"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['index'], axis = 1)\n",
    "X_train = X_train.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi-i2jjrN-s8"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.T\n",
    "X_train = X_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1fJItcLN-u2"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "y_train = train['Activity'].values #only want labels for y\n",
    "y_test = test['Activity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DEMiE1jODVD"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder #but we want the labels to be numbers instead so they the model understands them\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "y_test = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keROqh-WOGln"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train = X_train.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)) # for input to Conv1D\n",
    "X_test = X_test.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)) # for input to Conv1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jpkfpyq7q4cE",
    "outputId": "8d24bd4c-5c56-46ba-bfe0-798d243b0ae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code goes through the y_train labels and takes every 40th label and puts it in a new list\n",
    "#select the nth label where n is the window size\n",
    "y_train_fit = [0 for i in range(int(len(y_train)/40))] \n",
    "for i in range(int(len(y_train)/40)):\n",
    "  y_train_fit[i] = y_train[i*40]\n",
    "\n",
    "y_test_fit = [0 for i in range(int(len(y_test)/40))]\n",
    "for i in range(int(len(y_test)/40)):\n",
    "  y_test_fit[i] = y_test[i*40]\n",
    "\n",
    "y_train_fit = np.array(y_train_fit)\n",
    "y_test_fit = np.array(y_test_fit)\n",
    "\n",
    "len(y_train_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6136</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6138 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "...  ..\n",
       "6133  3\n",
       "6134  3\n",
       "6135  3\n",
       "6136  3\n",
       "6137  3\n",
       "\n",
       "[6138 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8aMmddUSYpbG",
    "outputId": "2e8b3cb5-a22d-434a-9bdc-2dcb31f4ced3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: (6138, 40, 8)\n",
      "X_test length: (1188, 40, 8)\n",
      "y_train length: (6138,)\n",
      "y_test length: (1188,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train length:', X_train.shape)\n",
    "print('X_test length:', X_test.shape)\n",
    "print('y_train length:', y_train.shape)\n",
    "print('y_test length:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XfxwZZbYpbI"
   },
   "outputs": [],
   "source": [
    "# y_train = y_train_fit\n",
    "# y_test = y_test_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSX7j69Kex3v"
   },
   "source": [
    "# ****The model itself****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DYIJ4K4fEta"
   },
   "source": [
    "**Change batch size according to error. Later find the \"rule\" so can make this automatic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "5M0sX-qPOGne",
    "outputId": "41ce4140-d29d-49c8-d8f9-b33b9872f44d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HARModel(\n",
       "  (conv1): Conv1d(8, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv3): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (lstm1): LSTM(64, 10, num_layers=2)\n",
       "  (lstm2): LSTM(10, 10, num_layers=2)\n",
       "  (lstm3): LSTM(10, 10, num_layers=2)\n",
       "  (lstm4): LSTM(10, 10, num_layers=2)\n",
       "  (lstm5): LSTM(10, 10, num_layers=2)\n",
       "  (fc): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HARModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=10, n_layers=2, n_filters=64, \n",
    "                 n_classes=4, filter_size=5, drop_prob=0.5):\n",
    "        super(HARModel, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_filters = n_filters\n",
    "        self.n_classes = n_classes\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "        #Layers 2-5\n",
    "        self.conv1 = nn.Conv1d(NB_SENSOR_CHANNELS, n_filters, filter_size)\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv3 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv4 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "\n",
    "\n",
    "        #Layers 6-7 - each dense layer has LSTM cells\n",
    "        self.lstm1  = nn.LSTM(n_filters, n_hidden, n_layers)\n",
    "        self.lstm2  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        self.lstm3  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        self.lstm4  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        self.lstm5  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        \n",
    "        #Layer 9 - prepare for softmax\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "\n",
    "        #During training, this layer randomly replaces some of the input tensor with zeroes with the probability as denoted by drop_prob (Bernoulli distribution). Each channel is zeroed out independently on every feed-forward cell.\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "       \n",
    "        #Layer 1 - flatten (see -1)\n",
    "        x = x.view(-1, NB_SENSOR_CHANNELS, SLIDING_WINDOW_LENGTH)\n",
    "        #print(x.size())\n",
    "        #Layers 2-5 - RELU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #print(x.size())\n",
    "        #Layers 5 and 6 - flatten\n",
    "        x = x.view(24, -1, self.n_filters)\n",
    "        #print(x.size())\n",
    "\n",
    "        #Layers 6-8 - hidden layers\n",
    "        x, hidden = self.lstm1(x, hidden)\n",
    "        #print(x.size())\n",
    "        x, hidden = self.lstm2(x, hidden)\n",
    "        #print(x.size())\n",
    "        x, hidden = self.lstm3(x, hidden)\n",
    "        #print(x.size())\n",
    "        x, hidden = self.lstm4(x, hidden)\n",
    "        #print(x.size())\n",
    "        x, hidden = self.lstm5(x, hidden)\n",
    "        #print(x.size())\n",
    "\n",
    "        \n",
    "        #Layers 8 - flatten, fully connected for softmax. Not sure what dropout does here\n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        #print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        #print(x.size())\n",
    "        #View flattened output layer      \n",
    "        out = x.view(batch_size, -1, self.n_classes)[:,-1,:]\n",
    "        #print(x.size())\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            # changed this from batch_size to 3*batch_size\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            # changed this from batch_size to 3*batch_size\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "net = HARModel()\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            \n",
    "            #the learnable input-hidden weights of the kth layer\n",
    "            if 'weight_ih' in name:\n",
    "                #fills input with semi-orthogonal tensor\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "\n",
    "            #the learnable hidden-hidden weights of the kth layer\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "\n",
    "            #the learnable input-hidden and hidden-hidden bias of the kth layer\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "#Recursively apply weights\n",
    "net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "KZMAmksrONUE",
    "outputId": "d3de5e0c-da4a-4248-efd0-c95d742de555"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HARModel(\n",
       "  (conv1): Conv1d(8, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv3): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (lstm1): LSTM(64, 10, num_layers=2)\n",
       "  (lstm2): LSTM(10, 10, num_layers=2)\n",
       "  (lstm3): LSTM(10, 10, num_layers=2)\n",
       "  (lstm4): LSTM(10, 10, num_layers=2)\n",
       "  (lstm5): LSTM(10, 10, num_layers=2)\n",
       "  (fc): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m): \n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aIZrFceORXq"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C6ocfju0OTmF",
    "outputId": "79d77048-cb24-442c-d336-4c7e0641a3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "## check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INwiUVcvffXQ"
   },
   "source": [
    "# **Function to train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23_2WKpImeCW"
   },
   "source": [
    "**Must change earlier batch size value when error shows up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62bSnchwOXBn"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=100, batch_size=32, lr=0.002):\n",
    "    \n",
    "    opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "     \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)         \n",
    "        train_losses = []    \n",
    "        net.train()\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "            x, y = batch\n",
    "\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            opt.zero_grad()   \n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h, batch_size)\n",
    "            \n",
    "            loss = criterion(output, targets.long())\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        accuracy=0\n",
    "        f1score=0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "                x, y = batch     \n",
    "\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                output, val_h= net(inputs, val_h, batch_size)\n",
    "\n",
    "                val_loss = criterion(output, targets.long())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                top_p, top_class = output.topk(1, dim=1)\n",
    "                equals = top_class == targets.view(*top_class.shape).long()\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='weighted')\n",
    "            \n",
    "        net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        \"Train Loss: {:.4f}...\".format(np.mean(train_losses)),\n",
    "        \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "        \"Val Acc: {:.4f}...\".format(accuracy/(len(X_test)//batch_size)),\n",
    "        \"F1-Score: {:.4f}...\".format(f1score/(len(X_test)//batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Train Loss: 1.2587... Val Loss: 1.1877... Val Acc: 0.4544... F1-Score: 0.6205...\n",
      "Epoch: 2/100... Train Loss: 1.1753... Val Loss: 1.1656... Val Acc: 0.4561... F1-Score: 0.6227...\n",
      "Epoch: 3/100... Train Loss: 1.1703... Val Loss: 1.1644... Val Acc: 0.4552... F1-Score: 0.6213...\n",
      "Epoch: 4/100... Train Loss: 1.1674... Val Loss: 1.1620... Val Acc: 0.4544... F1-Score: 0.6225...\n",
      "Epoch: 5/100... Train Loss: 1.1660... Val Loss: 1.1608... Val Acc: 0.4552... F1-Score: 0.6203...\n",
      "Epoch: 6/100... Train Loss: 1.1658... Val Loss: 1.1617... Val Acc: 0.4561... F1-Score: 0.6228...\n",
      "Epoch: 7/100... Train Loss: 1.1652... Val Loss: 1.1622... Val Acc: 0.4544... F1-Score: 0.6198...\n",
      "Epoch: 8/100... Train Loss: 1.1647... Val Loss: 1.1624... Val Acc: 0.4535... F1-Score: 0.6189...\n",
      "Epoch: 9/100... Train Loss: 1.1628... Val Loss: 1.1629... Val Acc: 0.4552... F1-Score: 0.6207...\n",
      "Epoch: 10/100... Train Loss: 1.1648... Val Loss: 1.1623... Val Acc: 0.4552... F1-Score: 0.6206...\n",
      "Epoch: 11/100... Train Loss: 1.1627... Val Loss: 1.1617... Val Acc: 0.4552... F1-Score: 0.6213...\n",
      "Epoch: 12/100... Train Loss: 1.1639... Val Loss: 1.1636... Val Acc: 0.4535... F1-Score: 0.6199...\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other train function uses Adam. I am not sure of other differences - Noah\n",
    "\n",
    "Highlight all and then press ctrl+/ to uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jAfLj0pFYpbZ",
    "outputId": "b28eda00-e9cc-4062-c255-e4c97d73a088"
   },
   "outputs": [],
   "source": [
    "# def train2(net, epochs=20, batch_size = 32, lr = 0.001):\n",
    "#     t_loss = list()\n",
    "#     v_loss = list()\n",
    "#     opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     if(train_on_gpu):\n",
    "#         net.cuda()\n",
    "     \n",
    "#     for e in range(epochs):\n",
    "        \n",
    "#         # initialize hidden state\n",
    "#         h = net.init_hidden(batch_size)         \n",
    "#         train_losses = []    \n",
    "#         net.train()\n",
    "#         for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "#             x, y = batch\n",
    "\n",
    "#             inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "#             if(train_on_gpu):\n",
    "#                     inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "#             # Creating new variables for the hidden state, otherwise\n",
    "#             # we'd backprop through the entire training history\n",
    "#             h = tuple([each.data for each in h])\n",
    "            \n",
    "#             # zero accumulated gradients\n",
    "#             opt.zero_grad()   \n",
    "            \n",
    "#             # get the output from the model\n",
    "#             output, h = net(inputs, h, batch_size)\n",
    "            \n",
    "#             loss = criterion(output, targets.long())\n",
    "#             train_losses.append(loss.item())\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "            \n",
    "#         val_h = net.init_hidden(batch_size)\n",
    "#         val_losses = []\n",
    "#         accuracy=0\n",
    "#         f1score=0\n",
    "#         net.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "#                 x, y = batch     \n",
    "\n",
    "#                 inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "#                 val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "#                 if(train_on_gpu):\n",
    "#                     inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "#                 output, val_h= net(inputs, val_h, batch_size)\n",
    "\n",
    "#                 val_loss = criterion(output, targets.long())\n",
    "#                 val_losses.append(val_loss.item())\n",
    "\n",
    "#                 top_p, top_class = output.topk(1, dim=1)\n",
    "#                 equals = top_class == targets.view(*top_class.shape).long()\n",
    "#                 accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "#                 f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='weighted')\n",
    "            \n",
    "#         net.train() # reset to train mode after iterationg through validation data\n",
    "        \n",
    "#         print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "#         \"Train Loss: {:.4f}...\".format(np.mean(train_losses)),\n",
    "#         \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "#         \"Val Acc: {:.4f}...\".format(accuracy/(len(X_test)//batch_size)),\n",
    "#         \"F1-Score: {:.4f}...\".format(f1score/(len(X_test)//batch_size)))\n",
    "#         t_loss.append(np.mean(train_losses))\n",
    "#         v_loss.append(np.mean(val_losses))\n",
    "\n",
    "# train2(net) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Kush-GloriousDeepConvLSTMGPU.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

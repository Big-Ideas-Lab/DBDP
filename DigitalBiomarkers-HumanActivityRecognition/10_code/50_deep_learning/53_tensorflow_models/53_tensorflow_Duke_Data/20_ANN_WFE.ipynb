{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Feature Classification Model: ANN with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is composed of an artifical neural network classification model to evaluate if using features from windows of time (20 seconds with 10 second overlap), would generate a better model than our simple timepoint classifier. Leave-One-Person-Out (LOPO) Cross-Validation is used to validate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INPUT: .csv files containing the rolled sensor data with feature engineering (engineered_features.csv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT: Neural Network Multi-Classification Window Featuer Model (F1 Score = 0.871)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded dataset contains windows of data that are 20 seconds long with a 10 second overlap. These are stored as arrays in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('/Users/N1/Data7/Data-2020/10_code/40_usable_data_for_models/41_Duke_Data/engineered_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a window number that changes everytime there is a new activity present, as we wish to use this as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(count=df.groupby(df.Activity.ne(df.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>ACC1_skew</th>\n",
       "      <th>ACC2_skew</th>\n",
       "      <th>ACC3_skew</th>\n",
       "      <th>TEMP_skew</th>\n",
       "      <th>EDA_skew</th>\n",
       "      <th>BVP_skew</th>\n",
       "      <th>HR_skew</th>\n",
       "      <th>Magnitude_skew</th>\n",
       "      <th>ACC1_min</th>\n",
       "      <th>ACC2_min</th>\n",
       "      <th>ACC3_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>Magnitude_min</th>\n",
       "      <th>ACC1_max</th>\n",
       "      <th>ACC2_max</th>\n",
       "      <th>ACC3_max</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>Magnitude_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...</td>\n",
       "      <td>[27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...</td>\n",
       "      <td>[40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...</td>\n",
       "      <td>[32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.275354 0.276634 0.270231 0.270231 0.26895 0...</td>\n",
       "      <td>[15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...</td>\n",
       "      <td>[78.98 78.83500000000002 78.69 78.545 78.4 78....</td>\n",
       "      <td>[63.410093833710725 63.453053512025726 63.4961...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>32.350</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>-0.082592</td>\n",
       "      <td>-0.558848</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>-0.392823</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.531557</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.456522</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>69.7650</td>\n",
       "      <td>61.692787</td>\n",
       "      <td>41.543478</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>34.83</td>\n",
       "      <td>78.98</td>\n",
       "      <td>63.757353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0 39.06521739130435 39.130434782608695 39....</td>\n",
       "      <td>[29.0 28.93478260869565 28.869565217391305 28....</td>\n",
       "      <td>[38.0 38.02173913043478 38.04347826086956 38.0...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.25998499999999997 0.25998499999999997 0.258...</td>\n",
       "      <td>[-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...</td>\n",
       "      <td>[73.52 73.435 73.35 73.265 73.18 73.0925 73.00...</td>\n",
       "      <td>[61.69278726074872 61.7168170027034 61.7409828...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>32.339</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.170923</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>64.8025</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>37.72</td>\n",
       "      <td>73.52</td>\n",
       "      <td>62.936476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392 41.67391304347826 41.739130...</td>\n",
       "      <td>[26.39130434782609 26.32608695652174 26.260869...</td>\n",
       "      <td>[38.869565217391305 38.89130434782609 38.91304...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.265108 0.263827 0.266389 0.265108 0.266389 ...</td>\n",
       "      <td>[-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...</td>\n",
       "      <td>[69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...</td>\n",
       "      <td>[62.758486272725364 62.78782873035957 62.81730...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>32.337</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>0.227966</td>\n",
       "      <td>1.329886</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>-0.274279</td>\n",
       "      <td>0.185657</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.252301</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>60.9950</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>45.532258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>47.14</td>\n",
       "      <td>69.63</td>\n",
       "      <td>64.010791</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575 44.14285714285715 44.31428...</td>\n",
       "      <td>[24.514285714285712 24.42857142857143 24.34285...</td>\n",
       "      <td>[37.17142857142857 37.142857142857146 37.11428...</td>\n",
       "      <td>[32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.258704 0.258704 0.258704 0.257424 0.257424 ...</td>\n",
       "      <td>[17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....</td>\n",
       "      <td>[64.68 64.555 64.43 64.305 64.18 64.0475 63.91...</td>\n",
       "      <td>[62.579164557660036 62.649331804179724 62.7200...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>-4.941414</td>\n",
       "      <td>-1.040349</td>\n",
       "      <td>3.435987</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>-0.828441</td>\n",
       "      <td>0.406263</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.985816</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.8025</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>47.14</td>\n",
       "      <td>64.68</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418 45.564516129032256 45.58064...</td>\n",
       "      <td>[25.64516129032258 25.69354838709677 25.741935...</td>\n",
       "      <td>[37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.253581 0.253581 0.253581 0.252301 0.252301 ...</td>\n",
       "      <td>[-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...</td>\n",
       "      <td>[60.92 60.8475 60.77500000000001 60.7025 60.63...</td>\n",
       "      <td>[64.04162603123257 64.07248675362091 64.103373...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>32.389</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-1.857224</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>1.380509</td>\n",
       "      <td>-0.686481</td>\n",
       "      <td>1.239716</td>\n",
       "      <td>-0.833856</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.408229</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.702128</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.249739</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.5300</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.43</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>39.00</td>\n",
       "      <td>60.92</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...   \n",
       "1  [39.0 39.06521739130435 39.130434782608695 39....   \n",
       "2  [41.60869565217392 41.67391304347826 41.739130...   \n",
       "3  [43.971428571428575 44.14285714285715 44.31428...   \n",
       "4  [45.54838709677418 45.564516129032256 45.58064...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...   \n",
       "1  [29.0 28.93478260869565 28.869565217391305 28....   \n",
       "2  [26.39130434782609 26.32608695652174 26.260869...   \n",
       "3  [24.514285714285712 24.42857142857143 24.34285...   \n",
       "4  [25.64516129032258 25.69354838709677 25.741935...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...   \n",
       "1  [38.0 38.02173913043478 38.04347826086956 38.0...   \n",
       "2  [38.869565217391305 38.89130434782609 38.91304...   \n",
       "3  [37.17142857142857 37.142857142857146 37.11428...   \n",
       "4  [37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....   \n",
       "1  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "2  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "3  [32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....   \n",
       "4  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.275354 0.276634 0.270231 0.270231 0.26895 0...   \n",
       "1  [0.25998499999999997 0.25998499999999997 0.258...   \n",
       "2  [0.265108 0.263827 0.266389 0.265108 0.266389 ...   \n",
       "3  [0.258704 0.258704 0.258704 0.257424 0.257424 ...   \n",
       "4  [0.253581 0.253581 0.253581 0.252301 0.252301 ...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...   \n",
       "1  [-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...   \n",
       "2  [-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...   \n",
       "3  [17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....   \n",
       "4  [-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [78.98 78.83500000000002 78.69 78.545 78.4 78....   \n",
       "1  [73.52 73.435 73.35 73.265 73.18 73.0925 73.00...   \n",
       "2  [69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...   \n",
       "3  [64.68 64.555 64.43 64.305 64.18 64.0475 63.91...   \n",
       "4  [60.92 60.8475 60.77500000000001 60.7025 60.63...   \n",
       "\n",
       "                                           Magnitude Subject_ID  Activity  \\\n",
       "0  [63.410093833710725 63.453053512025726 63.4961...     19-001  Baseline   \n",
       "1  [61.69278726074872 61.7168170027034 61.7409828...     19-001  Baseline   \n",
       "2  [62.758486272725364 62.78782873035957 62.81730...     19-001  Baseline   \n",
       "3  [62.579164557660036 62.649331804179724 62.7200...     19-001  Baseline   \n",
       "4  [64.04162603123257 64.07248675362091 64.103373...     19-001  Baseline   \n",
       "\n",
       "   Round  ACC1_mean  ACC2_mean  ACC3_mean  TEMP_mean  EDA_mean  BVP_mean  \\\n",
       "0      1  40.248370  28.012880  38.824457     32.350  0.262354 -0.109875   \n",
       "1      1  40.820000  26.815000  38.192500     32.339  0.261058  0.321375   \n",
       "2      1  43.252235  25.312684  37.488043     32.337  0.259585  0.684000   \n",
       "3      1  44.905798  24.915984  37.638218     32.356  0.254510 -0.180875   \n",
       "4      1  43.577055  22.974382  38.971144     32.389  0.252733 -0.209750   \n",
       "\n",
       "     HR_mean  Magnitude_mean  ACC1_std  ACC2_std  ACC3_std  TEMP_std  \\\n",
       "0  73.931187       62.553853  0.701573  0.687590  0.632616  0.017607   \n",
       "1  69.481750       62.021872  1.192214  1.149559  0.529382  0.012610   \n",
       "2  64.893188       62.621785  2.109896  0.815025  0.647914  0.010536   \n",
       "3  61.157687       63.734171  1.832017  1.509593  1.773398  0.025377   \n",
       "4  59.226438       62.913435  2.115371  2.585687  1.809092  0.027000   \n",
       "\n",
       "    EDA_std    BVP_std    HR_std  Magnitude_std  ACC1_skew  ACC2_skew  \\\n",
       "0  0.004877  18.439453  2.574676       0.609756  -0.082592  -0.558848   \n",
       "1  0.003007  20.104717  2.608254       0.542348   0.515544   0.109446   \n",
       "2  0.004337  23.756276  2.639037       0.942868  -0.473020   0.227966   \n",
       "3  0.002396  25.635645  1.674001       0.841361  -4.941414  -1.040349   \n",
       "4  0.002055  25.593597  0.684349       1.365652  -1.857224   0.511935   \n",
       "\n",
       "   ACC3_skew  TEMP_skew  EDA_skew  BVP_skew   HR_skew  Magnitude_skew  \\\n",
       "0   0.705668   0.714533  0.896382 -0.392823  0.296262        0.531557   \n",
       "1  -0.188071   0.787066  0.212943 -0.322900 -0.170923       -0.438037   \n",
       "2   1.329886   0.620801  0.072564 -0.274279  0.185657       -0.382833   \n",
       "3   3.435987   0.672586  0.734482 -0.828441  0.406263       -0.532117   \n",
       "4   1.380509  -0.686481  1.239716 -0.833856  0.996232        0.408229   \n",
       "\n",
       "   ACC1_min   ACC2_min  ACC3_min  TEMP_min   EDA_min  BVP_min   HR_min  \\\n",
       "0      39.0  26.456522      38.0     32.33  0.254862   -42.99  69.7650   \n",
       "1      39.0  24.600000      37.2     32.31  0.254862   -48.52  64.8025   \n",
       "2      39.0  24.000000      37.0     32.31  0.252301   -48.52  60.9950   \n",
       "3      32.0  20.985816      37.0     32.33  0.251020  -101.74  58.8025   \n",
       "4      32.0  20.702128      37.0     32.33  0.249739  -101.74  58.5300   \n",
       "\n",
       "   Magnitude_min   ACC1_max  ACC2_max  ACC3_max  TEMP_max   EDA_max  BVP_max  \\\n",
       "0      61.692787  41.543478      29.0      40.0     32.39  0.276634    34.83   \n",
       "1      60.778286  43.800000      29.0      39.0     32.37  0.266389    37.72   \n",
       "2      60.778286  45.532258      27.0      39.0     32.37  0.266389    47.14   \n",
       "3      61.392182  46.000000      27.0      48.0     32.41  0.262546    47.14   \n",
       "4      61.392182  46.000000      27.0      48.0     32.43  0.258704    39.00   \n",
       "\n",
       "   HR_max  Magnitude_max  count  \n",
       "0   78.98      63.757353      1  \n",
       "1   73.52      62.936476      2  \n",
       "2   69.63      64.010791      3  \n",
       "3   64.68      65.711491      4  \n",
       "4   60.92      65.711491      5  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n",
    "#what the sensor names correspond to\n",
    "#['ACC1', 'ACC2', 'ACC3','TEMP', 'EDA', 'HR', 'BVP', 'Magnitude',] \n",
    "#Accelerometry Axes, Body Temperature, Electrodermal Activity, Heart Rate, Blood Volume Pulse, Magnitude of Axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These will be our Subjects in our test set: [39 17 45]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rands = np.random.choice(df.Subject_ID.unique(),3, replace=False)\n",
    "print(f' These will be our Subjects in our test set: {rands}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Subjects into Test and Train Sets (n=52, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Subject_ID'].isin(rands)] \n",
    "train = df[-df['Subject_ID'].isin(rands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that can be used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking one of the three following code cells to chooses what features are used in the model. For this model, we chose to use all features. You can request the code to choose to run the model on different features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'Magnitude_mean', 'ACC1_std', 'ACC2_std', \n",
    "#                'ACC3_std', 'TEMP_std', 'EDA_std', 'BVP_std', 'HR_std', 'Magnitude_std', 'ACC1_min', 'ACC2_min', 'ACC3_min', 'TEMP_min', 'EDA_min', 'BVP_min', 'HR_min', 'Magnitude_min', \n",
    "#                'ACC1_max', 'ACC2_max', 'ACC3_max', 'TEMP_max', 'EDA_max', 'BVP_max', 'HR_max', 'Magnitude_max', 'Subject_ID', 'count', 'Activity']]   \n",
    "\n",
    "# test = test [['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'Magnitude_mean', 'ACC1_std', 'ACC2_std', \n",
    "#                'ACC3_std', 'TEMP_std', 'EDA_std', 'BVP_std', 'HR_std', 'Magnitude_std', 'ACC1_min', 'ACC2_min', 'ACC3_min', 'TEMP_min', 'EDA_min', 'BVP_min', 'HR_min', 'Magnitude_min', \n",
    "#                'ACC1_max', 'ACC2_max', 'ACC3_max', 'TEMP_max', 'EDA_max', 'BVP_max', 'HR_max', 'Magnitude_max', 'Subject_ID', 'count', 'Activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mechanical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'Magnitude_mean', 'ACC1_std', 'ACC2_std',  'ACC3_std', 'Magnitude_std',\n",
    "#                'ACC1_min', 'ACC2_min', 'ACC3_min', 'Magnitude_min', \n",
    "#                'ACC1_max', 'ACC2_max', 'ACC3_max', 'Magnitude_max', 'Subject_ID', 'count', 'Activity']]   \n",
    "               \n",
    "# test = test[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'Magnitude_mean', 'ACC1_std', 'ACC2_std',  'ACC3_std', 'Magnitude_std',\n",
    "#                 'ACC1_min', 'ACC2_min', 'ACC3_min', 'Magnitude_min', \n",
    "#                'ACC1_max', 'ACC2_max', 'ACC3_max', 'Magnitude_max', 'Subject_ID', 'count', 'Activity']]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Physiological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std', 'EDA_std', 'BVP_std', 'HR_std', 'TEMP_min', 'EDA_min', 'BVP_min', 'HR_min', \n",
    "                'TEMP_max', 'EDA_max', 'BVP_max', 'HR_max', 'Subject_ID', 'count', 'Activity']]   \n",
    "               \n",
    "test = test[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std', 'EDA_std', 'BVP_std', 'HR_std', 'TEMP_min', 'EDA_min', 'BVP_min', 'HR_min', \n",
    "                'TEMP_max', 'EDA_max', 'BVP_max', 'HR_max', 'Subject_ID', 'count', 'Activity']]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cells, we randomly sample data from our majority classes to balance our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2929\n",
       "1    2323\n",
       "3     505\n",
       "2     505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n",
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = train[train['Activity'] == 0]\n",
    "one = train[train['Activity'] == 1]\n",
    "two = train[train['Activity'] == 2]\n",
    "three =train[train['Activity'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = zero.sample(505)\n",
    "one = one.sample(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([zero, one, two, three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    505\n",
       "2    505\n",
       "1    505\n",
       "0    505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train_SID is made so we can use the Subject_ID values to perform LOPO (leave one person out) later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SID = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID and window count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject_ID and window count must be one-hot encoded to be used as features in our model. Test and train dataframes must be concatenated before we one-hot encode, so that we do not get different encodings for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] =1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'], prefix = 'SID')], axis =1).drop('Subject_ID', axis =1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['count'], prefix = 'count')], axis =1).drop('count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 130) (310, 130)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/N1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop([\"train\"], axis = 1, inplace = True)\n",
    "test.drop([\"train\"], axis = 1, inplace = True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove activity from our train and test datasets as this is the y variable (target variable) and we are only interested in keeping the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train.drop(\"Activity\", axis =1)\n",
    "test_f = test.drop(\"Activity\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X (features) and y (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_f\n",
    "y_train = train.Activity\n",
    "X_test = test_f\n",
    "y_test = test.Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:16] = sc.fit_transform(X_train.iloc[:,:16])\n",
    "X_test.iloc[:,:16] = sc.transform(X_test.iloc[:,:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>SID_0</th>\n",
       "      <th>SID_1</th>\n",
       "      <th>SID_2</th>\n",
       "      <th>SID_3</th>\n",
       "      <th>SID_4</th>\n",
       "      <th>SID_5</th>\n",
       "      <th>SID_6</th>\n",
       "      <th>SID_7</th>\n",
       "      <th>SID_8</th>\n",
       "      <th>SID_9</th>\n",
       "      <th>SID_10</th>\n",
       "      <th>SID_11</th>\n",
       "      <th>SID_12</th>\n",
       "      <th>SID_13</th>\n",
       "      <th>SID_14</th>\n",
       "      <th>SID_15</th>\n",
       "      <th>SID_16</th>\n",
       "      <th>SID_17</th>\n",
       "      <th>SID_18</th>\n",
       "      <th>SID_19</th>\n",
       "      <th>SID_20</th>\n",
       "      <th>SID_21</th>\n",
       "      <th>SID_22</th>\n",
       "      <th>SID_23</th>\n",
       "      <th>SID_24</th>\n",
       "      <th>SID_25</th>\n",
       "      <th>SID_26</th>\n",
       "      <th>SID_27</th>\n",
       "      <th>SID_28</th>\n",
       "      <th>SID_29</th>\n",
       "      <th>SID_30</th>\n",
       "      <th>SID_31</th>\n",
       "      <th>SID_32</th>\n",
       "      <th>SID_33</th>\n",
       "      <th>SID_34</th>\n",
       "      <th>SID_35</th>\n",
       "      <th>SID_36</th>\n",
       "      <th>SID_37</th>\n",
       "      <th>SID_38</th>\n",
       "      <th>SID_39</th>\n",
       "      <th>SID_40</th>\n",
       "      <th>SID_41</th>\n",
       "      <th>SID_42</th>\n",
       "      <th>SID_43</th>\n",
       "      <th>SID_44</th>\n",
       "      <th>SID_45</th>\n",
       "      <th>SID_46</th>\n",
       "      <th>SID_47</th>\n",
       "      <th>SID_48</th>\n",
       "      <th>SID_49</th>\n",
       "      <th>SID_50</th>\n",
       "      <th>SID_51</th>\n",
       "      <th>SID_52</th>\n",
       "      <th>SID_53</th>\n",
       "      <th>SID_54</th>\n",
       "      <th>count_1</th>\n",
       "      <th>count_2</th>\n",
       "      <th>count_3</th>\n",
       "      <th>count_4</th>\n",
       "      <th>count_5</th>\n",
       "      <th>count_6</th>\n",
       "      <th>count_7</th>\n",
       "      <th>count_8</th>\n",
       "      <th>count_9</th>\n",
       "      <th>count_10</th>\n",
       "      <th>count_11</th>\n",
       "      <th>count_12</th>\n",
       "      <th>count_13</th>\n",
       "      <th>count_14</th>\n",
       "      <th>count_15</th>\n",
       "      <th>count_16</th>\n",
       "      <th>count_17</th>\n",
       "      <th>count_18</th>\n",
       "      <th>count_19</th>\n",
       "      <th>count_20</th>\n",
       "      <th>count_21</th>\n",
       "      <th>count_22</th>\n",
       "      <th>count_23</th>\n",
       "      <th>count_24</th>\n",
       "      <th>count_25</th>\n",
       "      <th>count_26</th>\n",
       "      <th>count_27</th>\n",
       "      <th>count_28</th>\n",
       "      <th>count_29</th>\n",
       "      <th>count_30</th>\n",
       "      <th>count_31</th>\n",
       "      <th>count_32</th>\n",
       "      <th>count_33</th>\n",
       "      <th>count_34</th>\n",
       "      <th>count_35</th>\n",
       "      <th>count_36</th>\n",
       "      <th>count_37</th>\n",
       "      <th>count_38</th>\n",
       "      <th>count_39</th>\n",
       "      <th>count_40</th>\n",
       "      <th>count_41</th>\n",
       "      <th>count_42</th>\n",
       "      <th>count_43</th>\n",
       "      <th>count_44</th>\n",
       "      <th>count_45</th>\n",
       "      <th>count_46</th>\n",
       "      <th>count_47</th>\n",
       "      <th>count_48</th>\n",
       "      <th>count_49</th>\n",
       "      <th>count_50</th>\n",
       "      <th>count_51</th>\n",
       "      <th>count_52</th>\n",
       "      <th>count_53</th>\n",
       "      <th>count_54</th>\n",
       "      <th>count_55</th>\n",
       "      <th>count_56</th>\n",
       "      <th>count_57</th>\n",
       "      <th>count_58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.399745</td>\n",
       "      <td>0.029140</td>\n",
       "      <td>-0.183949</td>\n",
       "      <td>1.075343</td>\n",
       "      <td>0.581375</td>\n",
       "      <td>0.903210</td>\n",
       "      <td>-0.319164</td>\n",
       "      <td>-0.188250</td>\n",
       "      <td>-0.437075</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.385497</td>\n",
       "      <td>1.113247</td>\n",
       "      <td>-0.384519</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.343239</td>\n",
       "      <td>1.017378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>-0.972260</td>\n",
       "      <td>-0.223688</td>\n",
       "      <td>-0.196012</td>\n",
       "      <td>-1.044870</td>\n",
       "      <td>-0.357831</td>\n",
       "      <td>0.019664</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>2.502464</td>\n",
       "      <td>-0.961745</td>\n",
       "      <td>-0.217144</td>\n",
       "      <td>0.247992</td>\n",
       "      <td>-1.344220</td>\n",
       "      <td>-0.981383</td>\n",
       "      <td>-0.184292</td>\n",
       "      <td>-0.140508</td>\n",
       "      <td>-0.745165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>-0.800788</td>\n",
       "      <td>-0.147607</td>\n",
       "      <td>-0.390530</td>\n",
       "      <td>1.755619</td>\n",
       "      <td>-0.568289</td>\n",
       "      <td>-0.170301</td>\n",
       "      <td>-0.146181</td>\n",
       "      <td>1.073712</td>\n",
       "      <td>-0.792236</td>\n",
       "      <td>-0.147213</td>\n",
       "      <td>0.442241</td>\n",
       "      <td>1.689482</td>\n",
       "      <td>-0.820069</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>-0.139378</td>\n",
       "      <td>1.863551</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>1.066445</td>\n",
       "      <td>-0.502610</td>\n",
       "      <td>1.394650</td>\n",
       "      <td>1.039490</td>\n",
       "      <td>-0.386223</td>\n",
       "      <td>-0.621193</td>\n",
       "      <td>1.280438</td>\n",
       "      <td>-0.669718</td>\n",
       "      <td>1.080433</td>\n",
       "      <td>-0.486814</td>\n",
       "      <td>-0.956389</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>1.051179</td>\n",
       "      <td>-0.523553</td>\n",
       "      <td>1.294868</td>\n",
       "      <td>0.957663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>0.894165</td>\n",
       "      <td>-0.494258</td>\n",
       "      <td>2.330439</td>\n",
       "      <td>-0.315775</td>\n",
       "      <td>-0.954636</td>\n",
       "      <td>-0.620790</td>\n",
       "      <td>0.458492</td>\n",
       "      <td>-0.749923</td>\n",
       "      <td>0.918996</td>\n",
       "      <td>-0.479156</td>\n",
       "      <td>-0.603667</td>\n",
       "      <td>-0.243640</td>\n",
       "      <td>0.865668</td>\n",
       "      <td>-0.515411</td>\n",
       "      <td>0.441764</td>\n",
       "      <td>-0.387828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>-0.835083</td>\n",
       "      <td>-0.464322</td>\n",
       "      <td>0.676323</td>\n",
       "      <td>0.781455</td>\n",
       "      <td>-0.070712</td>\n",
       "      <td>-0.252864</td>\n",
       "      <td>-0.642175</td>\n",
       "      <td>0.150470</td>\n",
       "      <td>-0.832595</td>\n",
       "      <td>-0.466519</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.766218</td>\n",
       "      <td>-0.836200</td>\n",
       "      <td>-0.464452</td>\n",
       "      <td>-0.661647</td>\n",
       "      <td>0.749296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>-0.854449</td>\n",
       "      <td>-0.452544</td>\n",
       "      <td>-0.214922</td>\n",
       "      <td>0.653460</td>\n",
       "      <td>-0.124706</td>\n",
       "      <td>-0.387766</td>\n",
       "      <td>-0.824123</td>\n",
       "      <td>-0.119505</td>\n",
       "      <td>-0.840667</td>\n",
       "      <td>-0.455989</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.703422</td>\n",
       "      <td>-0.852331</td>\n",
       "      <td>-0.461343</td>\n",
       "      <td>-0.713805</td>\n",
       "      <td>0.655277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>-0.863729</td>\n",
       "      <td>-0.448422</td>\n",
       "      <td>-0.152039</td>\n",
       "      <td>0.547513</td>\n",
       "      <td>-0.332953</td>\n",
       "      <td>-0.426364</td>\n",
       "      <td>-0.995979</td>\n",
       "      <td>-0.193360</td>\n",
       "      <td>-0.856811</td>\n",
       "      <td>-0.440514</td>\n",
       "      <td>0.906954</td>\n",
       "      <td>0.570228</td>\n",
       "      <td>-0.852331</td>\n",
       "      <td>-0.457346</td>\n",
       "      <td>-0.873793</td>\n",
       "      <td>0.494555</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>-0.876236</td>\n",
       "      <td>-0.446389</td>\n",
       "      <td>-0.130725</td>\n",
       "      <td>0.465331</td>\n",
       "      <td>-0.155604</td>\n",
       "      <td>-0.400024</td>\n",
       "      <td>-1.011801</td>\n",
       "      <td>-0.446012</td>\n",
       "      <td>-0.872954</td>\n",
       "      <td>-0.437961</td>\n",
       "      <td>0.923705</td>\n",
       "      <td>0.556182</td>\n",
       "      <td>-0.876529</td>\n",
       "      <td>-0.456162</td>\n",
       "      <td>-0.938881</td>\n",
       "      <td>0.434840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>-0.896006</td>\n",
       "      <td>-0.445094</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>0.454889</td>\n",
       "      <td>-0.132694</td>\n",
       "      <td>-0.365435</td>\n",
       "      <td>-1.007294</td>\n",
       "      <td>-0.679936</td>\n",
       "      <td>-0.905242</td>\n",
       "      <td>-0.437961</td>\n",
       "      <td>0.917881</td>\n",
       "      <td>0.556182</td>\n",
       "      <td>-0.900726</td>\n",
       "      <td>-0.452756</td>\n",
       "      <td>-0.938881</td>\n",
       "      <td>0.395454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2020 rows  129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEMP_mean  EDA_mean  BVP_mean   HR_mean  TEMP_std   EDA_std   BVP_std  \\\n",
       "800   -0.399745  0.029140 -0.183949  1.075343  0.581375  0.903210 -0.319164   \n",
       "606   -0.972260 -0.223688 -0.196012 -1.044870 -0.357831  0.019664  0.059072   \n",
       "823   -0.800788 -0.147607 -0.390530  1.755619 -0.568289 -0.170301 -0.146181   \n",
       "4291   1.066445 -0.502610  1.394650  1.039490 -0.386223 -0.621193  1.280438   \n",
       "4326   0.894165 -0.494258  2.330439 -0.315775 -0.954636 -0.620790  0.458492   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "6567  -0.835083 -0.464322  0.676323  0.781455 -0.070712 -0.252864 -0.642175   \n",
       "6568  -0.854449 -0.452544 -0.214922  0.653460 -0.124706 -0.387766 -0.824123   \n",
       "6569  -0.863729 -0.448422 -0.152039  0.547513 -0.332953 -0.426364 -0.995979   \n",
       "6570  -0.876236 -0.446389 -0.130725  0.465331 -0.155604 -0.400024 -1.011801   \n",
       "6571  -0.896006 -0.445094 -0.040903  0.454889 -0.132694 -0.365435 -1.007294   \n",
       "\n",
       "        HR_std  TEMP_min   EDA_min   BVP_min    HR_min  TEMP_max   EDA_max  \\\n",
       "800  -0.188250 -0.437075 -0.033332  0.385497  1.113247 -0.384519  0.069073   \n",
       "606   2.502464 -0.961745 -0.217144  0.247992 -1.344220 -0.981383 -0.184292   \n",
       "823   1.073712 -0.792236 -0.147213  0.442241  1.689482 -0.820069 -0.153942   \n",
       "4291 -0.669718  1.080433 -0.486814 -0.956389  1.151090  1.051179 -0.523553   \n",
       "4326 -0.749923  0.918996 -0.479156 -0.603667 -0.243640  0.865668 -0.515411   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567  0.150470 -0.832595 -0.466519  0.510800  0.766218 -0.836200 -0.464452   \n",
       "6568 -0.119505 -0.840667 -0.455989  0.510800  0.703422 -0.852331 -0.461343   \n",
       "6569 -0.193360 -0.856811 -0.440514  0.906954  0.570228 -0.852331 -0.457346   \n",
       "6570 -0.446012 -0.872954 -0.437961  0.923705  0.556182 -0.876529 -0.456162   \n",
       "6571 -0.679936 -0.905242 -0.437961  0.917881  0.556182 -0.900726 -0.452756   \n",
       "\n",
       "       BVP_max    HR_max  SID_0  SID_1  SID_2  SID_3  SID_4  SID_5  SID_6  \\\n",
       "800  -0.343239  1.017378      0      0      0      0      0      0      1   \n",
       "606  -0.140508 -0.745165      0      0      0      0      1      0      0   \n",
       "823  -0.139378  1.863551      0      0      0      0      0      0      1   \n",
       "4291  1.294868  0.957663      0      0      0      0      0      0      0   \n",
       "4326  0.441764 -0.387828      0      0      0      0      0      0      0   \n",
       "...        ...       ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "6567 -0.661647  0.749296      0      0      0      0      0      0      0   \n",
       "6568 -0.713805  0.655277      0      0      0      0      0      0      0   \n",
       "6569 -0.873793  0.494555      0      0      0      0      0      0      0   \n",
       "6570 -0.938881  0.434840      0      0      0      0      0      0      0   \n",
       "6571 -0.938881  0.395454      0      0      0      0      0      0      0   \n",
       "\n",
       "      SID_7  SID_8  SID_9  SID_10  SID_11  SID_12  SID_13  SID_14  SID_15  \\\n",
       "800       0      0      0       0       0       0       0       0       0   \n",
       "606       0      0      0       0       0       0       0       0       0   \n",
       "823       0      0      0       0       0       0       0       0       0   \n",
       "4291      0      0      0       0       0       0       0       0       0   \n",
       "4326      0      0      0       0       0       0       0       0       0   \n",
       "...     ...    ...    ...     ...     ...     ...     ...     ...     ...   \n",
       "6567      0      0      0       0       0       0       0       0       0   \n",
       "6568      0      0      0       0       0       0       0       0       0   \n",
       "6569      0      0      0       0       0       0       0       0       0   \n",
       "6570      0      0      0       0       0       0       0       0       0   \n",
       "6571      0      0      0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_16  SID_17  SID_18  SID_19  SID_20  SID_21  SID_22  SID_23  SID_24  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_25  SID_26  SID_27  SID_28  SID_29  SID_30  SID_31  SID_32  SID_33  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_34  SID_35  SID_36  SID_37  SID_38  SID_39  SID_40  SID_41  SID_42  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       1       0       0       0       0       0       0       0   \n",
       "4326       0       1       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_43  SID_44  SID_45  SID_46  SID_47  SID_48  SID_49  SID_50  SID_51  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_52  SID_53  SID_54  count_1  count_2  count_3  count_4  count_5  \\\n",
       "800        0       0       0        1        0        0        0        0   \n",
       "606        0       0       0        0        0        0        0        0   \n",
       "823        0       0       0        0        0        0        0        0   \n",
       "4291       0       0       0        0        0        0        0        0   \n",
       "4326       0       0       0        0        0        0        0        0   \n",
       "...      ...     ...     ...      ...      ...      ...      ...      ...   \n",
       "6567       0       0       1        1        0        0        0        0   \n",
       "6568       0       0       1        0        1        0        0        0   \n",
       "6569       0       0       1        0        0        1        0        0   \n",
       "6570       0       0       1        0        0        0        1        0   \n",
       "6571       0       0       1        0        0        0        0        1   \n",
       "\n",
       "      count_6  count_7  count_8  count_9  count_10  count_11  count_12  \\\n",
       "800         0        0        0        0         0         0         0   \n",
       "606         0        0        0        0         0         0         0   \n",
       "823         0        0        0        0         0         0         0   \n",
       "4291        0        0        0        0         0         0         0   \n",
       "4326        0        0        0        0         0         0         0   \n",
       "...       ...      ...      ...      ...       ...       ...       ...   \n",
       "6567        0        0        0        0         0         0         0   \n",
       "6568        0        0        0        0         0         0         0   \n",
       "6569        0        0        0        0         0         0         0   \n",
       "6570        0        0        0        0         0         0         0   \n",
       "6571        0        0        0        0         0         0         0   \n",
       "\n",
       "      count_13  count_14  count_15  count_16  count_17  count_18  count_19  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_20  count_21  count_22  count_23  count_24  count_25  count_26  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         1         0         0   \n",
       "4291         1         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_27  count_28  count_29  count_30  count_31  count_32  count_33  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_34  count_35  count_36  count_37  count_38  count_39  count_40  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_41  count_42  count_43  count_44  count_45  count_46  count_47  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_48  count_49  count_50  count_51  count_52  count_53  count_54  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_55  count_56  count_57  count_58  \n",
       "800          0         0         0         0  \n",
       "606          1         0         0         0  \n",
       "823          0         0         0         0  \n",
       "4291         0         0         0         0  \n",
       "4326         1         0         0         0  \n",
       "...        ...       ...       ...       ...  \n",
       "6567         0         0         0         0  \n",
       "6568         0         0         0         0  \n",
       "6569         0         0         0         0  \n",
       "6570         0         0         0         0  \n",
       "6571         0         0         0         0  \n",
       "\n",
       "[2020 rows x 129 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "- 6 hidden **fully connected** layers with 256 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training and validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2943 - accuracy: 0.3921\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0806 - accuracy: 0.5488\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7433 - accuracy: 0.6983\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6003 - accuracy: 0.7736\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5082 - accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8474\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8762\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8853\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8994\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9277\n",
      "Score for fold 1: loss of 0.4869212210178375; accuracy of 85.36585569381714%, F1 of 0.8534713991641194\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2803 - accuracy: 0.4075\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9858 - accuracy: 0.5971\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.7290\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.7765\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.8170\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8519\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8868\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.9166\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.9090\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9312\n",
      "Score for fold 2: loss of 2.1405515670776367; accuracy of 66.66666865348816%, F1 of 0.62548945307566\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2845 - accuracy: 0.4016\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9895 - accuracy: 0.5928\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7105 - accuracy: 0.7245\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.7770\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8587\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8673\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8961\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9117\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9228\n",
      "Score for fold 3: loss of 1.298040747642517; accuracy of 63.15789222717285%, F1 of 0.640759507464313\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2915 - accuracy: 0.3963\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0107 - accuracy: 0.5870\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7304 - accuracy: 0.7298\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.7788\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8112\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8512\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8725\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.9013\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.9109\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.9185\n",
      "Score for fold 4: loss of 1.9513981342315674; accuracy of 56.81818127632141%, F1 of 0.5692974063385632\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2842 - accuracy: 0.3982\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0430 - accuracy: 0.5643\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7009 - accuracy: 0.7204\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7888\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.8247\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8465\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8880\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.9043\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9245\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9179\n",
      "Score for fold 5: loss of 2.1546151638031006; accuracy of 52.173912525177%, F1 of 0.5015007875419775\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2819 - accuracy: 0.3965\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0358 - accuracy: 0.5742\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7073 - accuracy: 0.7227\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.7707\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.8141\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8449\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8828\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.9000\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.9152\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1969 - accuracy: 0.9318\n",
      "Score for fold 6: loss of 1.3011457920074463; accuracy of 60.00000238418579%, F1 of 0.5218390804597701\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2874 - accuracy: 0.4134\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0345 - accuracy: 0.5634\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7292 - accuracy: 0.7234\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7739\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8294\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8546\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8758\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8935\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.9086\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9202\n",
      "Score for fold 7: loss of 1.9404150247573853; accuracy of 53.84615659713745%, F1 of 0.5082417582417582\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2831 - accuracy: 0.3917\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0766 - accuracy: 0.5447\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7479 - accuracy: 0.7309\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.7850\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.8157\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8526\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8894\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8738\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2619 - accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9263\n",
      "Score for fold 8: loss of 1.15140962600708; accuracy of 58.974361419677734%, F1 of 0.5874125874125874\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2852 - accuracy: 0.4051\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0613 - accuracy: 0.5652\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7140 - accuracy: 0.7212\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.7630\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4837 - accuracy: 0.8143\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8566\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8556\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8697\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.9044\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.9235\n",
      "Score for fold 9: loss of 1.7327150106430054; accuracy of 36.36363744735718%, F1 of 0.41413128781549835\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2987 - accuracy: 0.4113\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0807 - accuracy: 0.5449\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7756 - accuracy: 0.6951\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.7742\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8483\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8553\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8690\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.9068\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9229\n",
      "Score for fold 10: loss of 0.868728518486023; accuracy of 69.44444179534912%, F1 of 0.6795695045695046\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2934 - accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0947 - accuracy: 0.5259\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7528 - accuracy: 0.6967\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.7683\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8166\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8544\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.9033\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8756\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.9199\n",
      "Score for fold 11: loss of 2.075812578201294; accuracy of 45.71428596973419%, F1 of 0.39536796536796537\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2724 - accuracy: 0.4147\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9962 - accuracy: 0.5919\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7268 - accuracy: 0.7180\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7813\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.8091\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8522\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8714\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.8997\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.9104\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.9246\n",
      "Score for fold 12: loss of 1.3870940208435059; accuracy of 64.4444465637207%, F1 of 0.6382896713223809\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2877 - accuracy: 0.3845\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9958 - accuracy: 0.5969\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.7351\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7800\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.8320\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8613\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8698\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8855\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.9142\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.9294\n",
      "Score for fold 13: loss of 2.513340711593628; accuracy of 39.47368562221527%, F1 of 0.3209971569239304\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2742 - accuracy: 0.4006\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0132 - accuracy: 0.5883\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6822 - accuracy: 0.7477\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.7911\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.8264\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4109 - accuracy: 0.8471\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8865\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.9021\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9152\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9253\n",
      "Score for fold 14: loss of 2.073312520980835; accuracy of 44.736841320991516%, F1 of 0.405953798523458\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2763 - accuracy: 0.4195\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0456 - accuracy: 0.5638\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.7234\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7801\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8531\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8794\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8972\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8987\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.9164\n",
      "Score for fold 15: loss of 3.0631604194641113; accuracy of 45.652174949645996%, F1 of 0.3926603949487245\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2925 - accuracy: 0.3871\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9884 - accuracy: 0.5941\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7229 - accuracy: 0.7108\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.7874\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.8250\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8559\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8818\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8955\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9249\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9335\n",
      "Score for fold 16: loss of 5.023345470428467; accuracy of 38.77550959587097%, F1 of 0.3100097181729835\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3004 - accuracy: 0.4095\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9907 - accuracy: 0.5966\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7272 - accuracy: 0.7068\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.7710\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.8064\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8453\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8842\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8933\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9135\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9237\n",
      "Score for fold 17: loss of 1.4555354118347168; accuracy of 73.8095223903656%, F1 of 0.6880145997793057\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2873 - accuracy: 0.3913\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0651 - accuracy: 0.5609\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7191 - accuracy: 0.7243\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.7837\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.8068\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8466\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8843\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.9024\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9120\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9281\n",
      "Score for fold 18: loss of 12.327570915222168; accuracy of 34.375%, F1 of 0.20833333333333334\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2890 - accuracy: 0.3755\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1059 - accuracy: 0.5418\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7772 - accuracy: 0.7051\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.7757\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8125\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8458\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8785\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8982\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9183\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9294\n",
      "Score for fold 19: loss of 0.36363184452056885; accuracy of 91.66666865348816%, F1 of 0.9147058823529411\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2790 - accuracy: 0.4133\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0134 - accuracy: 0.5877\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7072 - accuracy: 0.7365\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7881\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8302\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8447\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8712\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8833\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.8998\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9178\n",
      "Score for fold 20: loss of 3.138822555541992; accuracy of 45.83333432674408%, F1 of 0.41738816738816736\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2727 - accuracy: 0.4097\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9689 - accuracy: 0.6085\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.7190\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7714\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.8113\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8416\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8809\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8845\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2723 - accuracy: 0.8966\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2208 - accuracy: 0.9263\n",
      "Score for fold 21: loss of 2.7139389514923096; accuracy of 65.78947305679321%, F1 of 0.5715849969751966\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2839 - accuracy: 0.3912\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0613 - accuracy: 0.5617\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7381 - accuracy: 0.7227\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6002 - accuracy: 0.7758\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.8254\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8436\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8907\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9109\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2444 - accuracy: 0.9160\n",
      "Score for fold 22: loss of 3.7485945224761963; accuracy of 40.909090638160706%, F1 of 0.35999027710257653\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2992 - accuracy: 0.3974\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0601 - accuracy: 0.5443\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7329 - accuracy: 0.7025\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7755\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.8175\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8596\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8814\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8865\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9057\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2228 - accuracy: 0.9204\n",
      "Score for fold 23: loss of 1.2282100915908813; accuracy of 68.08510422706604%, F1 of 0.6651262796444273\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3023 - accuracy: 0.4031\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0892 - accuracy: 0.5456\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7531 - accuracy: 0.6998\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.7738\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.8053\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8489\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9219\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.9113\n",
      "Score for fold 24: loss of 1.2030895948410034; accuracy of 56.25%, F1 of 0.5687344614879445\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2900 - accuracy: 0.3974\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0649 - accuracy: 0.5551\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7346 - accuracy: 0.7169\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.7755\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.8023\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8443\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8696\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.9019\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9237\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9130\n",
      "Score for fold 25: loss of 0.777774453163147; accuracy of 80.95238208770752%, F1 of 0.7784679089026916\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2723 - accuracy: 0.4095\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9645 - accuracy: 0.6026\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.7346\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7912\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8256\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8731\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8761\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.8898\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9146\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9262\n",
      "Score for fold 26: loss of 0.7926125526428223; accuracy of 66.66666865348816%, F1 of 0.655308347595331\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2778 - accuracy: 0.4081\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0210 - accuracy: 0.5768\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7004 - accuracy: 0.7359\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7899\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.8308\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8601\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8823\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8949\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9076\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9374\n",
      "Score for fold 27: loss of 2.2594056129455566; accuracy of 64.99999761581421%, F1 of 0.5841238471673253\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2894 - accuracy: 0.3996\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0175 - accuracy: 0.5964\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.7358\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.7792\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7972\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8447\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8666\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8866\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9026\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9181\n",
      "Score for fold 28: loss of 1.0526771545410156; accuracy of 77.77777910232544%, F1 of 0.7777777777777778\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2878 - accuracy: 0.3894\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0174 - accuracy: 0.5717\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7426 - accuracy: 0.7010\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.7692\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.8131\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8470\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8980\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9061\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1968 - accuracy: 0.9323\n",
      "Score for fold 29: loss of 2.6556098461151123; accuracy of 57.499998807907104%, F1 of 0.529201680672269\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2941 - accuracy: 0.3877\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0932 - accuracy: 0.5427\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7366 - accuracy: 0.7208\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.7643\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.8112\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8536\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8607\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8829\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9253\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9223\n",
      "Score for fold 30: loss of 2.8561668395996094; accuracy of 56.41025900840759%, F1 of 0.4916173570019724\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2829 - accuracy: 0.4020\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9779 - accuracy: 0.6041\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.7218\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.7822\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7904\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8294\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8939\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9056\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.9046\n",
      "Score for fold 31: loss of 0.9130913615226746; accuracy of 80.0000011920929%, F1 of 0.802\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2682 - accuracy: 0.4391\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0242 - accuracy: 0.5816\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.7196\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7893\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4764 - accuracy: 0.8201\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8525\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8686\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8919\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.9227\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1773 - accuracy: 0.9389\n",
      "Score for fold 32: loss of 1.7487666606903076; accuracy of 53.658539056777954%, F1 of 0.5059137213763029\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2798 - accuracy: 0.4053\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0102 - accuracy: 0.5886\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7363 - accuracy: 0.7090\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7966\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8132\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8489\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8691\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8676\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.9008\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8822\n",
      "Score for fold 33: loss of 2.7545549869537354; accuracy of 26.470589637756348%, F1 of 0.2853098458634791\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2724 - accuracy: 0.3973\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0073 - accuracy: 0.5962\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7228 - accuracy: 0.7274\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7809\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.8112\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8450\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8748\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8930\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.8970\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9293\n",
      "Score for fold 34: loss of 2.7988808155059814; accuracy of 46.15384638309479%, F1 of 0.3719717064544651\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3025 - accuracy: 0.3888\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0107 - accuracy: 0.5915\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7056 - accuracy: 0.7285\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.7634\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.8049\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8589\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8655\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8888\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.9171\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9130\n",
      "Score for fold 35: loss of 1.9069231748580933; accuracy of 57.14285969734192%, F1 of 0.48551448551448556\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2915 - accuracy: 0.3945\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0453 - accuracy: 0.5889\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.7184\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.7723\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.7919\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8297\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8615\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8741\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2711 - accuracy: 0.8972\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.9199\n",
      "Score for fold 36: loss of 1.0937706232070923; accuracy of 71.42857313156128%, F1 of 0.6978872520668187\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3019 - accuracy: 0.4024\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0756 - accuracy: 0.5606\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7465 - accuracy: 0.7217\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.7831\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.8130\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.8510\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8723\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8880\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.9149\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9230\n",
      "Score for fold 37: loss of 1.639822006225586; accuracy of 61.702126264572144%, F1 of 0.5941158505548203\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2795 - accuracy: 0.4112\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0174 - accuracy: 0.5817\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.7341\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.7699\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.8244\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8512\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8749\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.9026\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9178\n",
      "Score for fold 38: loss of 1.5600093603134155; accuracy of 60.52631735801697%, F1 of 0.5876899851969103\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2897 - accuracy: 0.3991\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0306 - accuracy: 0.5868\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7339 - accuracy: 0.7200\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6110 - accuracy: 0.7679\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4973 - accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8375\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8522\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8920\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.8961\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9137\n",
      "Score for fold 39: loss of 1.5848954916000366; accuracy of 63.15789222717285%, F1 of 0.6309041835357625\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2887 - accuracy: 0.4070\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9997 - accuracy: 0.5825\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.7045\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7776\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8169\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8462\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8739\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8840\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9067\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9022\n",
      "Score for fold 40: loss of 0.5727351903915405; accuracy of 91.89189076423645%, F1 of 0.917022285443338\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2926 - accuracy: 0.4014\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0243 - accuracy: 0.5738\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7118 - accuracy: 0.7270\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.7735\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8205\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8509\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8736\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8847\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8984\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9257\n",
      "Score for fold 41: loss of 1.1057188510894775; accuracy of 66.66666865348816%, F1 of 0.596447906931778\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2871 - accuracy: 0.3963\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0190 - accuracy: 0.5760\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7481 - accuracy: 0.7153\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.7754\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.8198\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8491\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3647 - accuracy: 0.8597\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8889\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9076\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9258\n",
      "Score for fold 42: loss of 0.5735176801681519; accuracy of 82.05128312110901%, F1 of 0.8184953448111342\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2723 - accuracy: 0.4044\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0402 - accuracy: 0.5614\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7130 - accuracy: 0.7128\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7792\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.8194\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8415\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8727\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8707\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9180\n",
      "Score for fold 43: loss of 0.5011085867881775; accuracy of 81.25%, F1 of 0.8139097744360902\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2827 - accuracy: 0.3991\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9387 - accuracy: 0.6115\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6802 - accuracy: 0.7407\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7952\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.8204\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8602\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8673\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.8961\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9183\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9218\n",
      "Score for fold 44: loss of 4.375453472137451; accuracy of 26.31579041481018%, F1 of 0.2265278581068055\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2966 - accuracy: 0.3927\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0934 - accuracy: 0.5477\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7252 - accuracy: 0.7138\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.7784\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.8193\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8541\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8783\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8930\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9253\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2004 - accuracy: 0.9278\n",
      "Score for fold 45: loss of 1.2836986780166626; accuracy of 56.41025900840759%, F1 of 0.5409120242867141\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 46 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2849 - accuracy: 0.4087\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0363 - accuracy: 0.5665\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.7284\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.7962\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.8326\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8543\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8877\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8978\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.9216\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.1928 - accuracy: 0.9292\n",
      "Score for fold 46: loss of 3.1707873344421387; accuracy of 27.906978130340576%, F1 of 0.2248414376321353\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 47 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2980 - accuracy: 0.3874\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1123 - accuracy: 0.5224\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7710 - accuracy: 0.6917\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6111 - accuracy: 0.7708\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5139 - accuracy: 0.8111\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8343\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8761\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2988 - accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.9033\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.9164\n",
      "Score for fold 47: loss of 0.39625969529151917; accuracy of 85.71428656578064%, F1 of 0.8608327541542933\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 48 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2875 - accuracy: 0.4200\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0019 - accuracy: 0.5875\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7186 - accuracy: 0.7138\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.7772\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.7963\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8400\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8581\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8919\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9110\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9210\n",
      "Score for fold 48: loss of 1.2932060956954956; accuracy of 65.625%, F1 of 0.6636904761904763\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 49 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2777 - accuracy: 0.4220\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9718 - accuracy: 0.6093\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.7274\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5502 - accuracy: 0.7956\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.8142\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8410\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8778\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3116 - accuracy: 0.8869\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9223\n",
      "Score for fold 49: loss of 2.2045092582702637; accuracy of 61.538463830947876%, F1 of 0.5742705570291777\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 50 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2942 - accuracy: 0.4002\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0740 - accuracy: 0.5565\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7517 - accuracy: 0.7092\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6202 - accuracy: 0.7737\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5262 - accuracy: 0.8059\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8332\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8659\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8836\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.9068\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9163\n",
      "Score for fold 50: loss of 0.532996654510498; accuracy of 86.11111044883728%, F1 of 0.8465166539537249\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 51 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2900 - accuracy: 0.3817\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.0617 - accuracy: 0.5450\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7348 - accuracy: 0.7108\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7826\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8109\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8428\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3519 - accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.8969\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.9156\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8918\n",
      "Score for fold 51: loss of 1.319130539894104; accuracy of 61.90476417541504%, F1 of 0.59610022498702\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 52 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2799 - accuracy: 0.4037\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9629 - accuracy: 0.6003\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.7349\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.7684\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.8199\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8464\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8669\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8854\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.9150\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9270\n",
      "Score for fold 52: loss of 1.6002461910247803; accuracy of 52.3809552192688%, F1 of 0.4381868131868132\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4869212210178375 - Accuracy: 85.36585569381714% - F1:0.8534713991641194%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.1405515670776367 - Accuracy: 66.66666865348816% - F1:0.62548945307566%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.298040747642517 - Accuracy: 63.15789222717285% - F1:0.640759507464313%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.9513981342315674 - Accuracy: 56.81818127632141% - F1:0.5692974063385632%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.1546151638031006 - Accuracy: 52.173912525177% - F1:0.5015007875419775%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.3011457920074463 - Accuracy: 60.00000238418579% - F1:0.5218390804597701%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.9404150247573853 - Accuracy: 53.84615659713745% - F1:0.5082417582417582%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.15140962600708 - Accuracy: 58.974361419677734% - F1:0.5874125874125874%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.7327150106430054 - Accuracy: 36.36363744735718% - F1:0.41413128781549835%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.868728518486023 - Accuracy: 69.44444179534912% - F1:0.6795695045695046%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 2.075812578201294 - Accuracy: 45.71428596973419% - F1:0.39536796536796537%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.3870940208435059 - Accuracy: 64.4444465637207% - F1:0.6382896713223809%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 2.513340711593628 - Accuracy: 39.47368562221527% - F1:0.3209971569239304%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.073312520980835 - Accuracy: 44.736841320991516% - F1:0.405953798523458%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 3.0631604194641113 - Accuracy: 45.652174949645996% - F1:0.3926603949487245%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 5.023345470428467 - Accuracy: 38.77550959587097% - F1:0.3100097181729835%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 1.4555354118347168 - Accuracy: 73.8095223903656% - F1:0.6880145997793057%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 12.327570915222168 - Accuracy: 34.375% - F1:0.20833333333333334%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.36363184452056885 - Accuracy: 91.66666865348816% - F1:0.9147058823529411%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 3.138822555541992 - Accuracy: 45.83333432674408% - F1:0.41738816738816736%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 2.7139389514923096 - Accuracy: 65.78947305679321% - F1:0.5715849969751966%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 3.7485945224761963 - Accuracy: 40.909090638160706% - F1:0.35999027710257653%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.2282100915908813 - Accuracy: 68.08510422706604% - F1:0.6651262796444273%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 1.2030895948410034 - Accuracy: 56.25% - F1:0.5687344614879445%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.777774453163147 - Accuracy: 80.95238208770752% - F1:0.7784679089026916%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.7926125526428223 - Accuracy: 66.66666865348816% - F1:0.655308347595331%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 2.2594056129455566 - Accuracy: 64.99999761581421% - F1:0.5841238471673253%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 1.0526771545410156 - Accuracy: 77.77777910232544% - F1:0.7777777777777778%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 2.6556098461151123 - Accuracy: 57.499998807907104% - F1:0.529201680672269%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 2.8561668395996094 - Accuracy: 56.41025900840759% - F1:0.4916173570019724%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.9130913615226746 - Accuracy: 80.0000011920929% - F1:0.802%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 1.7487666606903076 - Accuracy: 53.658539056777954% - F1:0.5059137213763029%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 2.7545549869537354 - Accuracy: 26.470589637756348% - F1:0.2853098458634791%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 2.7988808155059814 - Accuracy: 46.15384638309479% - F1:0.3719717064544651%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 1.9069231748580933 - Accuracy: 57.14285969734192% - F1:0.48551448551448556%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 1.0937706232070923 - Accuracy: 71.42857313156128% - F1:0.6978872520668187%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 1.639822006225586 - Accuracy: 61.702126264572144% - F1:0.5941158505548203%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.5600093603134155 - Accuracy: 60.52631735801697% - F1:0.5876899851969103%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.5848954916000366 - Accuracy: 63.15789222717285% - F1:0.6309041835357625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 0.5727351903915405 - Accuracy: 91.89189076423645% - F1:0.917022285443338%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 1.1057188510894775 - Accuracy: 66.66666865348816% - F1:0.596447906931778%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.5735176801681519 - Accuracy: 82.05128312110901% - F1:0.8184953448111342%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.5011085867881775 - Accuracy: 81.25% - F1:0.8139097744360902%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 4.375453472137451 - Accuracy: 26.31579041481018% - F1:0.2265278581068055%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 1.2836986780166626 - Accuracy: 56.41025900840759% - F1:0.5409120242867141%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 46 - Loss: 3.1707873344421387 - Accuracy: 27.906978130340576% - F1:0.2248414376321353%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 47 - Loss: 0.39625969529151917 - Accuracy: 85.71428656578064% - F1:0.8608327541542933%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 48 - Loss: 1.2932060956954956 - Accuracy: 65.625% - F1:0.6636904761904763%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 49 - Loss: 2.2045092582702637 - Accuracy: 61.538463830947876% - F1:0.5742705570291777%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 50 - Loss: 0.532996654510498 - Accuracy: 86.11111044883728% - F1:0.8465166539537249%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 51 - Loss: 1.319130539894104 - Accuracy: 61.90476417541504% - F1:0.59610022498702%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 52 - Loss: 1.6002461910247803 - Accuracy: 52.3809552192688% - F1:0.4381868131868132%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 60.16618322867613 (+- 16.340434961744588)\n",
      "> F1: 0.5702774526199422 (+- 0.1794976783990618)\n",
      "> Loss: 1.9743409535059562\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = train_SID\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please edit the name of the model below. This will be used to save the model and figures associated with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '20_TF_FE_balanced_Phys_Only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/20_TF_FE_balanced_Phys_Only/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save(f'saved_model/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'saved_model/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 2.6477 - accuracy: 0.4000\n",
      "Test loss, Test acc: [2.6476845741271973, 0.4000000059604645]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=32)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 77, 20, 21],\n",
       "       [16, 60, 15, 24],\n",
       "       [ 0,  0, 17,  8],\n",
       "       [ 0,  0,  5, 20]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1862069 , 0.53103448, 0.13793103, 0.14482759],\n",
       "       [0.13913043, 0.52173913, 0.13043478, 0.20869565],\n",
       "       [0.        , 0.        , 0.68      , 0.32      ],\n",
       "       [0.        , 0.        , 0.2       , 0.8       ]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Feature Engineered STEP balanced - Confusion Matrix')\n",
    "plt.savefig(f'20_figures/{model_name}_CF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.400 \n",
      "F1 Score: 0.377\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

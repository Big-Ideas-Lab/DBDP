{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 2: Convolutional Neural Network - No Mechanical Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **plain_data.csv**\n",
    "\n",
    "This notebook converts our individual timepoint data into windows to be input into our model. It then label encodes Subject_ID and Activity (our y variable).  The Subject_ID is then one-hot encoded to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.275354</td>\n",
       "      <td>15.25</td>\n",
       "      <td>78.9800</td>\n",
       "      <td>63.410094</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>78.8350</td>\n",
       "      <td>63.453054</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>78.6900</td>\n",
       "      <td>63.496142</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>18.39</td>\n",
       "      <td>78.5450</td>\n",
       "      <td>63.539358</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.34</td>\n",
       "      <td>0.268950</td>\n",
       "      <td>13.61</td>\n",
       "      <td>78.4000</td>\n",
       "      <td>63.582702</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279835</th>\n",
       "      <td>21.176471</td>\n",
       "      <td>-11.176471</td>\n",
       "      <td>64.823529</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.708502</td>\n",
       "      <td>0.85</td>\n",
       "      <td>92.8275</td>\n",
       "      <td>69.104605</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279836</th>\n",
       "      <td>24.235294</td>\n",
       "      <td>-12.235294</td>\n",
       "      <td>62.764706</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.694414</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>92.8800</td>\n",
       "      <td>68.384649</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279837</th>\n",
       "      <td>27.294118</td>\n",
       "      <td>-13.294118</td>\n",
       "      <td>60.705882</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>5.22</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>67.874197</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279838</th>\n",
       "      <td>30.352941</td>\n",
       "      <td>-14.352941</td>\n",
       "      <td>58.647059</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.664957</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>93.0000</td>\n",
       "      <td>67.577995</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279839</th>\n",
       "      <td>33.411765</td>\n",
       "      <td>-15.411765</td>\n",
       "      <td>56.588235</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.648308</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.0600</td>\n",
       "      <td>67.498866</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279840 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ACC1       ACC2       ACC3   TEMP       EDA    BVP       HR  \\\n",
       "0       41.000000  27.200000  40.000000  32.39  0.275354  15.25  78.9800   \n",
       "1       41.000000  27.300000  40.000000  32.39  0.276634 -12.75  78.8350   \n",
       "2       41.000000  27.400000  40.000000  32.39  0.270231 -42.99  78.6900   \n",
       "3       41.000000  27.500000  40.000000  32.39  0.270231  18.39  78.5450   \n",
       "4       41.000000  27.600000  40.000000  32.34  0.268950  13.61  78.4000   \n",
       "...           ...        ...        ...    ...       ...    ...      ...   \n",
       "279835  21.176471 -11.176471  64.823529  32.09  0.708502   0.85  92.8275   \n",
       "279836  24.235294 -12.235294  62.764706  32.09  0.694414  -1.00  92.8800   \n",
       "279837  27.294118 -13.294118  60.705882  32.09  0.672642   5.22  92.9400   \n",
       "279838  30.352941 -14.352941  58.647059  32.09  0.664957  -1.47  93.0000   \n",
       "279839  33.411765 -15.411765  56.588235  32.09  0.648308   1.37  93.0600   \n",
       "\n",
       "        Magnitude  Activity Subject_ID  Round  \n",
       "0       63.410094  Baseline     19-001      1  \n",
       "1       63.453054  Baseline     19-001      1  \n",
       "2       63.496142  Baseline     19-001      1  \n",
       "3       63.539358  Baseline     19-001      1  \n",
       "4       63.582702  Baseline     19-001      1  \n",
       "...           ...       ...        ...    ...  \n",
       "279835  69.104605      Type     19-056      1  \n",
       "279836  68.384649      Type     19-056      1  \n",
       "279837  67.874197      Type     19-056      1  \n",
       "279838  67.577995      Type     19-056      1  \n",
       "279839  67.498866      Type     19-056      1  \n",
       "\n",
       "[279840 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('plain_data.csv') #read in the csv file\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_windows function below, subsets the data by Subject_ID, then by Activity, and finally by Activity Round. It then subsets for only columns that contain features that we will use in our model. This is done to only make windows of rows with the same Subject_ID, Activity, and Round. A window is a selection of readings from consecutive timepoints; the length of each window can be defined in the function below. Since our timepoints occur every 0.25 seconds, a window length of 40 timepoints would be equivalent to 10 second windows. \n",
    "\n",
    "The reason why we are making windows is to allow us to both engineer and extract features from our data, so that our model is able to quantify changes or patterns over time for each sensor reading as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = list(df['Activity'].unique()) \n",
    "sub_id_list = list(df['Subject_ID'].unique())\n",
    "round_list = list(df['Round'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_slider import Slider\n",
    "\n",
    "def make_windows(df, bucket_size, overlap_count):\n",
    "    \"\"\"\n",
    "    bucket_size = the window length (in rows) the data should be aggregated by\n",
    "    overlap_count = the number of overlapping timepoints from one window to the next\n",
    "    \"\"\"\n",
    "    window_list = []\n",
    "    final = pd.DataFrame()\n",
    "    activity_list = list(df['Activity'].unique()) #list of the four activities\n",
    "    sub_id_list = list(df['Subject_ID'].unique()) #list of the subject ids\n",
    "    round_list = list(df['Round'].unique())\n",
    "    df_list = []\n",
    "\n",
    "    for i in sub_id_list:\n",
    "        df_subject = df[df['Subject_ID'] == i] #isolate a single subject id\n",
    "        for j in activity_list:\n",
    "            df_subject_activity = df_subject[df_subject['Activity'] == j] #isolate by activity\n",
    "            for k in round_list:\n",
    "                df_subject_activity_round = df_subject_activity[df_subject_activity['Round'] == k]\n",
    "                final_df = pd.DataFrame()\n",
    "                if df_subject_activity_round.empty:\n",
    "                      pass\n",
    "                else:\n",
    "                    df_flat = df_subject_activity_round[['ACC1', 'ACC2','ACC3','Magnitude', 'Subject_ID']].T.values #array of arrays, each row is every single reading in an array for a sensor in that isolation \n",
    "\n",
    "                    slider = Slider(bucket_size,overlap_count)\n",
    "                    slider.fit(df_flat)\n",
    "                    while True:\n",
    "                        window_data = slider.slide()\n",
    "\n",
    "                        if slider.reached_end_of_list(): break\n",
    "                        window_list.append(list(window_data))\n",
    "                    final_df = final.append(window_list)\n",
    "                    final_df.columns = [['ACC1', 'ACC2','ACC3','Magnitude', 'SID']]\n",
    "                    final_df.insert(4, \"Subject_ID\", [i]*len(final_df), True)\n",
    "                    final_df.insert(4, \"Activity\", [j]*len(final_df), True)\n",
    "                    final_df.insert(4, \"Round\", [k]*len(final_df), True)\n",
    "                    df_list.append(final_df)\n",
    "                    window_list = []\n",
    "\n",
    "    final = pd.DataFrame(columns = df_list[0].columns)\n",
    "\n",
    "    for l in df_list:\n",
    "        final = final.append(l)\n",
    "\n",
    "    final\n",
    "    final.columns = final.columns.map(''.join)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the window length and the window overlap below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 40\n",
    "window_overlap = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed = make_windows(df, window_size, window_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a window number to each window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a window number to each window gives us another feature to use in the model. As a participant performs an activity, it their physiological sensor readings change overtime. In order to allow our model to better understand where in time a window is relative to the activity being performed, we have added window numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed=windowed.assign(count=windowed.groupby(windowed.Activity.ne(windowed.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_arr = []\n",
    "for i in range(len(windowed)):\n",
    "    rep_arr.append(np.repeat(np.array(windowed['count'].iloc[i]),window_size))\n",
    "windowed['count'] = rep_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11136, 9) (2432, 9)\n"
     ]
    }
   ],
   "source": [
    "ID_list = list(windowed['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "#change size of train/test split\n",
    "train = windowed[windowed['Subject_ID'].isin(ID_list[:45])]\n",
    "test = windowed[windowed['Subject_ID'].isin(ID_list[45:])]\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output below displays arrays of size 40 which are our window size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Round</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>SID</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-61.83333333333334, -61.75, -61.6666666666666...</td>\n",
       "      <td>[18.166666666666668, 18.25, 18.33333333333333,...</td>\n",
       "      <td>[-8.166666666666666, -8.25, -8.333333333333334...</td>\n",
       "      <td>[64.96216847776354, 64.91677364133247, 64.8716...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...</td>\n",
       "      <td>[78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [-61.83333333333334, -61.75, -61.6666666666666...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [18.166666666666668, 18.25, 18.33333333333333,...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [-8.166666666666666, -8.25, -8.333333333333334...   \n",
       "\n",
       "                                           Magnitude Round Activity  \\\n",
       "0  [64.96216847776354, 64.91677364133247, 64.8716...     2        1   \n",
       "\n",
       "  Subject_ID                                                SID  \\\n",
       "0          3  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...   \n",
       "\n",
       "                                               count  \n",
       "0  [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 7...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create SID_list below so that we are able to use the Subject_ID values, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SID_list = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not able to feed arrays from a dataframe into our model. Therefore, we first select the columns of interest (those containing features), and then make each value in the array, a single row corresponding to other sensors at the same window timepoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['Magnitude', 'ACC1', 'ACC2', 'ACC3', 'SID', 'count']]\n",
    "X_test = test[['Magnitude', 'ACC1', 'ACC2', 'ACC3', 'SID', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(445440, 6) (97280, 6) 542720\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(pd.Series.explode).reset_index().drop('index', axis =1)\n",
    "X_test = X_test.apply(pd.Series.explode).reset_index().drop('index', axis =1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train and y_test values are selected here as they will be our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11136,) (2432,) 13568\n"
     ]
    }
   ],
   "source": [
    "y_train = train['Activity'].values\n",
    "y_test = test['Activity'].values\n",
    "print(y_train.shape, y_test.shape, y_train.shape[0] + y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Subject_ID and Window Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID and window count, so that they may be used as variables in our model. Adding Subject_ID allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>SID</th>\n",
       "      <th>count</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.9622</td>\n",
       "      <td>-61.8333</td>\n",
       "      <td>18.1667</td>\n",
       "      <td>-8.16667</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.9168</td>\n",
       "      <td>-61.75</td>\n",
       "      <td>18.25</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.8717</td>\n",
       "      <td>-61.6667</td>\n",
       "      <td>18.3333</td>\n",
       "      <td>-8.33333</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.8269</td>\n",
       "      <td>-61.5833</td>\n",
       "      <td>18.4167</td>\n",
       "      <td>-8.41667</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64.7823</td>\n",
       "      <td>-61.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>-8.5</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445435</th>\n",
       "      <td>80.154</td>\n",
       "      <td>-78.516</td>\n",
       "      <td>12.7694</td>\n",
       "      <td>9.84139</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445436</th>\n",
       "      <td>80.1285</td>\n",
       "      <td>-78.4804</td>\n",
       "      <td>12.7976</td>\n",
       "      <td>9.88058</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445437</th>\n",
       "      <td>80.1031</td>\n",
       "      <td>-78.4448</td>\n",
       "      <td>12.8259</td>\n",
       "      <td>9.91978</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445438</th>\n",
       "      <td>80.0776</td>\n",
       "      <td>-78.4093</td>\n",
       "      <td>12.8541</td>\n",
       "      <td>9.95898</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445439</th>\n",
       "      <td>80.0523</td>\n",
       "      <td>-78.3737</td>\n",
       "      <td>12.8824</td>\n",
       "      <td>9.99818</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445440 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Magnitude     ACC1     ACC2     ACC3 SID count  train\n",
       "0        64.9622 -61.8333  18.1667 -8.16667   3    78      1\n",
       "1        64.9168   -61.75    18.25    -8.25   3    78      1\n",
       "2        64.8717 -61.6667  18.3333 -8.33333   3    78      1\n",
       "3        64.8269 -61.5833  18.4167 -8.41667   3    78      1\n",
       "4        64.7823    -61.5     18.5     -8.5   3    78      1\n",
       "...          ...      ...      ...      ...  ..   ...    ...\n",
       "445435    80.154  -78.516  12.7694  9.84139  21     3      1\n",
       "445436   80.1285 -78.4804  12.7976  9.88058  21     3      1\n",
       "445437   80.1031 -78.4448  12.8259  9.91978  21     3      1\n",
       "445438   80.0776 -78.4093  12.8541  9.95898  21     3      1\n",
       "445439   80.0523 -78.3737  12.8824  9.99818  21     3      1\n",
       "\n",
       "[445440 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined = pd.concat([combined, pd.get_dummies(combined['count'])], axis =1).drop('count', axis = 1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['SID'])], axis =1).drop('SID', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(445440, 177) (97280, 177) 542720\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\"], axis = 1, inplace = True)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>45.0</th>\n",
       "      <th>46.0</th>\n",
       "      <th>47.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>49.0</th>\n",
       "      <th>50.0</th>\n",
       "      <th>51.0</th>\n",
       "      <th>52.0</th>\n",
       "      <th>53.0</th>\n",
       "      <th>54.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.9622</td>\n",
       "      <td>-61.8333</td>\n",
       "      <td>18.1667</td>\n",
       "      <td>-8.16667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Magnitude     ACC1     ACC2     ACC3  1  2  3  4  5  6  ...  45.0  46.0  \\\n",
       "0   64.9622 -61.8333  18.1667 -8.16667  0  0  0  0  0  0  ...     0     0   \n",
       "\n",
       "   47.0  48.0  49.0  50.0  51.0  52.0  53.0  54.0  \n",
       "0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[1 rows x 177 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set. The scaling only applied to sensor features and not to values which are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:4] = ss.fit_transform(X_train.iloc[:,:4])\n",
    "X_test.iloc[:,:4] = ss.transform(X_test.iloc[:,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping windows as arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have our CNN model, accept the windows, they need to be reshaped correctly. Here we take the data from the dataframe and reshape it into sizes of (-1 (inferred from other dimensions), window_size (40), # of features (117)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to transposed arrays\n",
    "X_test = X_test.T.values\n",
    "X_train = X_train.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11136, 40, 177) (11136,) (2432, 40, 177) (2432,)\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.astype('float64')\n",
    "X_train = X_train.astype('float64')\n",
    "\n",
    "# Reshape to -1, window_size, # features\n",
    "X_train = X_train.reshape((-1, window_size, X_train.shape[0]))\n",
    "X_test = X_test.reshape((-1, window_size, X_test.shape[0])) \n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode y (activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed, Conv1D, AveragePooling1D, Conv2D, MaxPooling2D, MaxPool2D, Lambda, GlobalAveragePooling1D, Reshape, MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import tensorflow.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2335 - accuracy: 0.4337\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1827 - accuracy: 0.4503\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1655 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1571 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1506 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1454 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1448 - accuracy: 0.4601\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1411 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1408 - accuracy: 0.4608\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1396 - accuracy: 0.4607\n",
      "Score for fold 1: loss of 1.1515334844589233; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2299 - accuracy: 0.4415\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1836 - accuracy: 0.4556\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1612 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1555 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1502 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1473 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1447 - accuracy: 0.4608\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1441 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 1.1409 - accuracy: 0.4615\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1405 - accuracy: 0.4608\n",
      "Score for fold 2: loss of 1.1448345184326172; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2479 - accuracy: 0.4259\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 1.1755 - accuracy: 0.4483\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1642 - accuracy: 0.4576\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1546 - accuracy: 0.4602\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1457 - accuracy: 0.4625\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1442 - accuracy: 0.4618\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1421 - accuracy: 0.4618\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1395 - accuracy: 0.4619\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1377 - accuracy: 0.4631\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1356 - accuracy: 0.4634\n",
      "Score for fold 3: loss of 1.142795443534851; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.2354 - accuracy: 0.4176\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1781 - accuracy: 0.4414\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1632 - accuracy: 0.4595\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1543 - accuracy: 0.4610\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1500 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1435 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1447 - accuracy: 0.4609\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1438 - accuracy: 0.4612\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1400 - accuracy: 0.4615\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1381 - accuracy: 0.4615\n",
      "Score for fold 4: loss of 1.1628879308700562; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.2483 - accuracy: 0.4305\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1752 - accuracy: 0.4513\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1639 - accuracy: 0.4535\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1544 - accuracy: 0.4597\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1491 - accuracy: 0.4618\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1455 - accuracy: 0.4599\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1417 - accuracy: 0.4607\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1403 - accuracy: 0.4634\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1390 - accuracy: 0.4628\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1357 - accuracy: 0.4642\n",
      "Score for fold 5: loss of 1.1517921686172485; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2336 - accuracy: 0.4313\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1809 - accuracy: 0.4431\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1658 - accuracy: 0.4563\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1514 - accuracy: 0.4612\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1498 - accuracy: 0.4612\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1479 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1469 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1438 - accuracy: 0.4631\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1423 - accuracy: 0.4642\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1389 - accuracy: 0.4647\n",
      "Score for fold 6: loss of 1.1545259952545166; accuracy of 44.140625%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2288 - accuracy: 0.4276\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1714 - accuracy: 0.4408\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1583 - accuracy: 0.4559\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1531 - accuracy: 0.4607\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1502 - accuracy: 0.4606\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1449 - accuracy: 0.4631\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1402 - accuracy: 0.4631\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1392 - accuracy: 0.4635\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 1.1342 - accuracy: 0.4635\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 4s 11ms/step - loss: 1.1347 - accuracy: 0.4639\n",
      "Score for fold 7: loss of 1.153257966041565; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2444 - accuracy: 0.4359\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1878 - accuracy: 0.4329\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1648 - accuracy: 0.4441\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1540 - accuracy: 0.4561\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1489 - accuracy: 0.4623\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1463 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1420 - accuracy: 0.4626\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1412 - accuracy: 0.4652\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1399 - accuracy: 0.4635\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1393 - accuracy: 0.4648\n",
      "Score for fold 8: loss of 1.1509320735931396; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2058 - accuracy: 0.4317\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1698 - accuracy: 0.4606\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1619 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1550 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1490 - accuracy: 0.4615\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1461 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1429 - accuracy: 0.4631\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1411 - accuracy: 0.4626\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1363 - accuracy: 0.4605\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1358 - accuracy: 0.4635\n",
      "Score for fold 9: loss of 1.1733607053756714; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2639 - accuracy: 0.4352\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1762 - accuracy: 0.4581\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1615 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1538 - accuracy: 0.4611\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1514 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1472 - accuracy: 0.4614\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1431 - accuracy: 0.4614\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1401 - accuracy: 0.4618\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1375 - accuracy: 0.4631\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1370 - accuracy: 0.4629\n",
      "Score for fold 10: loss of 1.1609916687011719; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2066 - accuracy: 0.4390\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1724 - accuracy: 0.4509\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1584 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1551 - accuracy: 0.4615\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1498 - accuracy: 0.4611\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1463 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1417 - accuracy: 0.4617\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1415 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1405 - accuracy: 0.4628\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1382 - accuracy: 0.4627\n",
      "Score for fold 11: loss of 1.137539267539978; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2318 - accuracy: 0.4337\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1770 - accuracy: 0.4442\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1608 - accuracy: 0.4540\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1546 - accuracy: 0.4622\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1507 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1449 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1409 - accuracy: 0.4608\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1409 - accuracy: 0.4623\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1395 - accuracy: 0.4606\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1379 - accuracy: 0.4608\n",
      "Score for fold 12: loss of 1.1555237770080566; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2124 - accuracy: 0.4372\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1766 - accuracy: 0.4527\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1623 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 4s 13ms/step - loss: 1.1525 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1481 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1460 - accuracy: 0.4606\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1443 - accuracy: 0.4597\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1404 - accuracy: 0.4616\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1395 - accuracy: 0.4629\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1390 - accuracy: 0.4624\n",
      "Score for fold 13: loss of 1.1409986019134521; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2120 - accuracy: 0.4281\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1741 - accuracy: 0.4449\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1614 - accuracy: 0.4544\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1542 - accuracy: 0.4625\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1457 - accuracy: 0.4604\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1435 - accuracy: 0.4614\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1418 - accuracy: 0.4623\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1397 - accuracy: 0.4626\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1394 - accuracy: 0.4637\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1372 - accuracy: 0.4614\n",
      "Score for fold 14: loss of 1.1775798797607422; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2185 - accuracy: 0.4343\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1722 - accuracy: 0.4551\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1617 - accuracy: 0.4598\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1540 - accuracy: 0.4599\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1510 - accuracy: 0.4615\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1478 - accuracy: 0.4641\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1441 - accuracy: 0.4622\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1395 - accuracy: 0.4644\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1350 - accuracy: 0.4685\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1348 - accuracy: 0.4686\n",
      "Score for fold 15: loss of 1.1638792753219604; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2162 - accuracy: 0.4367\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1751 - accuracy: 0.4482\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1613 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1534 - accuracy: 0.4593\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1478 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1481 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1449 - accuracy: 0.4606\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1401 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1390 - accuracy: 0.4614\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1377 - accuracy: 0.4614\n",
      "Score for fold 16: loss of 1.1505030393600464; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1988 - accuracy: 0.4498\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1725 - accuracy: 0.4598\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1612 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1518 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1492 - accuracy: 0.4612\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1460 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1438 - accuracy: 0.4618\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1401 - accuracy: 0.4624\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1386 - accuracy: 0.4620\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1358 - accuracy: 0.4630\n",
      "Score for fold 17: loss of 1.1559524536132812; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 2s 7ms/step - loss: 1.2143 - accuracy: 0.4366\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1766 - accuracy: 0.4582\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 3s 9ms/step - loss: 1.1607 - accuracy: 0.4611\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 3s 7ms/step - loss: 1.1551 - accuracy: 0.4598\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 3s 7ms/step - loss: 1.1494 - accuracy: 0.4605\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 2s 6ms/step - loss: 1.1470 - accuracy: 0.4615\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 3s 7ms/step - loss: 1.1423 - accuracy: 0.4618\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 3s 7ms/step - loss: 1.1429 - accuracy: 0.4606\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 2s 7ms/step - loss: 1.1392 - accuracy: 0.4616\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 2s 6ms/step - loss: 1.1380 - accuracy: 0.4625\n",
      "Score for fold 18: loss of 1.144383430480957; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2420 - accuracy: 0.4289\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1779 - accuracy: 0.4457\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1624 - accuracy: 0.4496\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1561 - accuracy: 0.4531\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1486 - accuracy: 0.4574\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 4s 12ms/step - loss: 1.1458 - accuracy: 0.4605\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1438 - accuracy: 0.4620\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1416 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1392 - accuracy: 0.4607\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1358 - accuracy: 0.4610\n",
      "Score for fold 19: loss of 1.1496964693069458; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2413 - accuracy: 0.4394\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1730 - accuracy: 0.4435\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1641 - accuracy: 0.4528\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1542 - accuracy: 0.4594\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1473 - accuracy: 0.4630\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1444 - accuracy: 0.4664\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1418 - accuracy: 0.4639\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1400 - accuracy: 0.4665\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1372 - accuracy: 0.4688\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1348 - accuracy: 0.4686\n",
      "Score for fold 20: loss of 1.147792935371399; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2151 - accuracy: 0.4369\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1737 - accuracy: 0.4498\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1637 - accuracy: 0.4576\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1540 - accuracy: 0.4612\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1488 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1458 - accuracy: 0.4605\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1421 - accuracy: 0.4606\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1373 - accuracy: 0.4664\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1374 - accuracy: 0.4653\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1317 - accuracy: 0.4646\n",
      "Score for fold 21: loss of 1.1471004486083984; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2099 - accuracy: 0.4327\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1724 - accuracy: 0.4457\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1611 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1537 - accuracy: 0.4611\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1492 - accuracy: 0.4627\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1473 - accuracy: 0.4645\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1400 - accuracy: 0.4663\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1386 - accuracy: 0.4673\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1353 - accuracy: 0.4680\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1329 - accuracy: 0.4719\n",
      "Score for fold 22: loss of 1.161963939666748; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2424 - accuracy: 0.4298\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1831 - accuracy: 0.4515\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1639 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1594 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1503 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1465 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1432 - accuracy: 0.4592\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1410 - accuracy: 0.4616\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1425 - accuracy: 0.4625\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1405 - accuracy: 0.4631\n",
      "Score for fold 23: loss of 1.139615535736084; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2365 - accuracy: 0.4433\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1824 - accuracy: 0.4609\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1655 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1566 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1496 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1475 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1440 - accuracy: 0.4609\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1425 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1415 - accuracy: 0.4607\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1402 - accuracy: 0.4612\n",
      "Score for fold 24: loss of 1.1622451543807983; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2378 - accuracy: 0.4078\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1780 - accuracy: 0.4433\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1630 - accuracy: 0.4605\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1544 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1513 - accuracy: 0.4613\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1455 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1429 - accuracy: 0.4617\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1445 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1418 - accuracy: 0.4633\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1383 - accuracy: 0.4640\n",
      "Score for fold 25: loss of 1.1549303531646729; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.2402 - accuracy: 0.4320\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 3s 7ms/step - loss: 1.1768 - accuracy: 0.4401\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 2s 7ms/step - loss: 1.1620 - accuracy: 0.4437\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1519 - accuracy: 0.4544\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 2s 7ms/step - loss: 1.1481 - accuracy: 0.4623\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1422 - accuracy: 0.4618\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 3s 9ms/step - loss: 1.1405 - accuracy: 0.4586\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 3s 9ms/step - loss: 1.1400 - accuracy: 0.4626\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1374 - accuracy: 0.4650\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1374 - accuracy: 0.4653\n",
      "Score for fold 26: loss of 1.1590380668640137; accuracy of 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2469 - accuracy: 0.4252\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1788 - accuracy: 0.4509\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1678 - accuracy: 0.4556\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1579 - accuracy: 0.4566\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1505 - accuracy: 0.4595\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1469 - accuracy: 0.4617\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1449 - accuracy: 0.4614\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1434 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1409 - accuracy: 0.4636\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1394 - accuracy: 0.4643\n",
      "Score for fold 27: loss of 1.1510485410690308; accuracy of 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2024 - accuracy: 0.4494\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1707 - accuracy: 0.4541\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1604 - accuracy: 0.4578\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1542 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1489 - accuracy: 0.4608\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1455 - accuracy: 0.4601\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1432 - accuracy: 0.4607\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1406 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1382 - accuracy: 0.4610\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1349 - accuracy: 0.4624\n",
      "Score for fold 28: loss of 1.1631008386611938; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2435 - accuracy: 0.4336\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1836 - accuracy: 0.4386\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1627 - accuracy: 0.4567\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1552 - accuracy: 0.4531\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1475 - accuracy: 0.4618\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1423 - accuracy: 0.4619\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1440 - accuracy: 0.4624\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1390 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1396 - accuracy: 0.4622\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1352 - accuracy: 0.4629\n",
      "Score for fold 29: loss of 1.1778311729431152; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2983 - accuracy: 0.4416\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1949 - accuracy: 0.4243\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1635 - accuracy: 0.4434\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1545 - accuracy: 0.4590\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1492 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1442 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1441 - accuracy: 0.4615\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1440 - accuracy: 0.4607\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1416 - accuracy: 0.4605\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1385 - accuracy: 0.4622\n",
      "Score for fold 30: loss of 1.1475027799606323; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2098 - accuracy: 0.4386\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1775 - accuracy: 0.4508\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1604 - accuracy: 0.4567\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1559 - accuracy: 0.4614\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1520 - accuracy: 0.4621\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1471 - accuracy: 0.4639\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1453 - accuracy: 0.4639\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1417 - accuracy: 0.4630\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1407 - accuracy: 0.4639\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1404 - accuracy: 0.4630\n",
      "Score for fold 31: loss of 1.1421773433685303; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2039 - accuracy: 0.4410\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1709 - accuracy: 0.4602\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1627 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1529 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1515 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1459 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1451 - accuracy: 0.4617\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1420 - accuracy: 0.4602\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1385 - accuracy: 0.4608\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1379 - accuracy: 0.4639\n",
      "Score for fold 32: loss of 1.1696611642837524; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2285 - accuracy: 0.4376\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1800 - accuracy: 0.4522\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1640 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1552 - accuracy: 0.4616\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1507 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1480 - accuracy: 0.4617\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1458 - accuracy: 0.4625\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1415 - accuracy: 0.4624\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1399 - accuracy: 0.4657\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1412 - accuracy: 0.4646\n",
      "Score for fold 33: loss of 1.1443960666656494; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2433 - accuracy: 0.4301\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1764 - accuracy: 0.4478\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1636 - accuracy: 0.4567\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1537 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1487 - accuracy: 0.4597\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1454 - accuracy: 0.4632\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1421 - accuracy: 0.4636\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1404 - accuracy: 0.4664\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1373 - accuracy: 0.4682\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1340 - accuracy: 0.4661\n",
      "Score for fold 34: loss of 1.1682722568511963; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2229 - accuracy: 0.4308\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1775 - accuracy: 0.4437\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1645 - accuracy: 0.4516\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1552 - accuracy: 0.4588\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1482 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1460 - accuracy: 0.4619\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1429 - accuracy: 0.4620\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1418 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1414 - accuracy: 0.4608\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1393 - accuracy: 0.4626\n",
      "Score for fold 35: loss of 1.1552400588989258; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 3s 7ms/step - loss: 1.2119 - accuracy: 0.4270\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 5s 14ms/step - loss: 1.1753 - accuracy: 0.4538\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1603 - accuracy: 0.4592\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1555 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1511 - accuracy: 0.4621\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1448 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1447 - accuracy: 0.4601\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1394 - accuracy: 0.4613\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1392 - accuracy: 0.4613\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1358 - accuracy: 0.4625\n",
      "Score for fold 36: loss of 1.165761947631836; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.2146 - accuracy: 0.4324\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1735 - accuracy: 0.4528\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1608 - accuracy: 0.4584\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1570 - accuracy: 0.4607\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1508 - accuracy: 0.4613\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1495 - accuracy: 0.4608\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1421 - accuracy: 0.4602\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1391 - accuracy: 0.4627\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1369 - accuracy: 0.4647\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1348 - accuracy: 0.4619\n",
      "Score for fold 37: loss of 1.1543880701065063; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.2051 - accuracy: 0.4404\n",
      "Epoch 2/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1662 - accuracy: 0.4582\n",
      "Epoch 3/10\n",
      "344/344 [==============================] - 3s 9ms/step - loss: 1.1590 - accuracy: 0.4600\n",
      "Epoch 4/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1548 - accuracy: 0.4611\n",
      "Epoch 5/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1521 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1467 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1444 - accuracy: 0.4608\n",
      "Epoch 8/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1424 - accuracy: 0.4611\n",
      "Epoch 9/10\n",
      "344/344 [==============================] - 3s 8ms/step - loss: 1.1402 - accuracy: 0.4614\n",
      "Epoch 10/10\n",
      "344/344 [==============================] - 3s 9ms/step - loss: 1.1394 - accuracy: 0.4607\n",
      "Score for fold 38: loss of 1.1445728540420532; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.2666 - accuracy: 0.4222\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1878 - accuracy: 0.4379\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1634 - accuracy: 0.4599\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1540 - accuracy: 0.4598\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1486 - accuracy: 0.4615\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1467 - accuracy: 0.4606\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1426 - accuracy: 0.4608\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1401 - accuracy: 0.4604\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1408 - accuracy: 0.4636\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1391 - accuracy: 0.4619\n",
      "Score for fold 39: loss of 1.150741457939148; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2020 - accuracy: 0.4321\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1717 - accuracy: 0.4494\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1614 - accuracy: 0.4562\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1541 - accuracy: 0.4595\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1499 - accuracy: 0.4616\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1435 - accuracy: 0.4612\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1419 - accuracy: 0.4637\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1380 - accuracy: 0.4637\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1347 - accuracy: 0.4665\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1327 - accuracy: 0.4672\n",
      "Score for fold 40: loss of 1.1600227355957031; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.2140 - accuracy: 0.4481\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1753 - accuracy: 0.4601\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1645 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1567 - accuracy: 0.4610\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1509 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1476 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1438 - accuracy: 0.4609\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1432 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1412 - accuracy: 0.4612\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1393 - accuracy: 0.4614\n",
      "Score for fold 41: loss of 1.1495716571807861; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2493 - accuracy: 0.4235\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1779 - accuracy: 0.4508\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1624 - accuracy: 0.4599\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1554 - accuracy: 0.4614\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1487 - accuracy: 0.4607\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1457 - accuracy: 0.4606\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1436 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1402 - accuracy: 0.4633\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1394 - accuracy: 0.4637\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1380 - accuracy: 0.4645\n",
      "Score for fold 42: loss of 1.1532776355743408; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2084 - accuracy: 0.4511\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1777 - accuracy: 0.4554\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1620 - accuracy: 0.4604\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1525 - accuracy: 0.4610\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1509 - accuracy: 0.4602\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1464 - accuracy: 0.4625\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1426 - accuracy: 0.4626\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1416 - accuracy: 0.4629\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1346 - accuracy: 0.4682\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1352 - accuracy: 0.4663\n",
      "Score for fold 43: loss of 1.1394258737564087; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.2190 - accuracy: 0.4338\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1773 - accuracy: 0.4466\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1637 - accuracy: 0.4555\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1557 - accuracy: 0.4594\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1495 - accuracy: 0.4606\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1453 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1427 - accuracy: 0.4613\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1390 - accuracy: 0.4669\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1390 - accuracy: 0.4664\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1376 - accuracy: 0.4664\n",
      "Score for fold 44: loss of 1.1523386240005493; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.2213 - accuracy: 0.4476\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 1.1771 - accuracy: 0.4609\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 3s 7ms/step - loss: 1.1648 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1535 - accuracy: 0.4611\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1510 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1473 - accuracy: 0.4608\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1435 - accuracy: 0.4612\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1432 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 1.1397 - accuracy: 0.4612\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 1.1390 - accuracy: 0.4640\n",
      "Score for fold 45: loss of 1.1540687084197998; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.1515334844589233 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.1448345184326172 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.142795443534851 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1628879308700562 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.1517921686172485 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.1545259952545166 - Accuracy: 44.140625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.153257966041565 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.1509320735931396 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.1733607053756714 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.1609916687011719 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 1.137539267539978 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.1555237770080566 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 1.1409986019134521 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 1.1775798797607422 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 1.1638792753219604 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 1.1505030393600464 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 1.1559524536132812 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 1.144383430480957 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 1.1496964693069458 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 1.147792935371399 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 1.1471004486083984 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 1.161963939666748 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.139615535736084 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 1.1622451543807983 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 1.1549303531646729 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 1.1590380668640137 - Accuracy: 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 1.1510485410690308 - Accuracy: 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 1.1631008386611938 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.1778311729431152 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 1.1475027799606323 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 1.1421773433685303 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 1.1696611642837524 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 1.1443960666656494 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.1682722568511963 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 1.1552400588989258 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 1.165761947631836 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 1.1543880701065063 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.1445728540420532 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.150741457939148 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 1.1600227355957031 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 1.1495716571807861 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 1.1532776355743408 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 1.1394258737564087 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.1523386240005493 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 1.1540687084197998 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 45.98090277777778 (+- 0.6358766170768209)\n",
      "> Loss: 1.154201208220588\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = SID_list\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    #model.add(Lambda(lambda v: tf.cast(tf.signal.fft(tf.cast(v,dtype=tf.complex64)),tf.float32)))\n",
    "    #model.add(Conv1D(160, 2, activation='relu'))\n",
    "    #model.add(Conv1D(160, 3, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/N1/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/10_TF_CNN/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/10_TF_CNN_ACC_Only') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 1.1500 - accuracy: 0.4656\n",
      "Test loss, Test acc: [1.150010585784912, 0.46562498807907104]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1171,  919,  216,  217],\n",
       "       [   9,   21,    4,    3],\n",
       "       [   0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred.astype(int), y_test.astype(int))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm/cm.astype(np.float).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data CNN - Confusion Matrix')\n",
    "plt.savefig('Duke_Data_CNN_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.466 \n",
      "F1 Score: 0.624\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_pred.astype(int), y_test.astype(int))\n",
    "f1_s = f1_score(y_pred.astype(int), y_test.astype(int), average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

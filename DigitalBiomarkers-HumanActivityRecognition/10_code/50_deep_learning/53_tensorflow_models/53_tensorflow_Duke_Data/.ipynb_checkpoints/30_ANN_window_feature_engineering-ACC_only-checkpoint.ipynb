{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Classification Model: Random Forest w/ Feature Engineering - ACC Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is composed of a random forest classification model to evaluate a general accuracy level of traditional ML methods in classifying our HAR data based on activity. We also used Leave-One-Out Cross-Validation to validate our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INPUT: .csv files containing the rolled sensor data with feature engineering (engineered_features.csv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT: Random Forest Multi-Classification Model (F1 Score = )__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('/Users/N1/Data7/Data-2020/10_code/40_usable_data_for_models/41_Duke_Data/engineered_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(count=df.groupby(df.Activity.ne(df.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>ACC1_skew</th>\n",
       "      <th>ACC2_skew</th>\n",
       "      <th>ACC3_skew</th>\n",
       "      <th>TEMP_skew</th>\n",
       "      <th>EDA_skew</th>\n",
       "      <th>BVP_skew</th>\n",
       "      <th>HR_skew</th>\n",
       "      <th>Magnitude_skew</th>\n",
       "      <th>ACC1_min</th>\n",
       "      <th>ACC2_min</th>\n",
       "      <th>ACC3_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>Magnitude_min</th>\n",
       "      <th>ACC1_max</th>\n",
       "      <th>ACC2_max</th>\n",
       "      <th>ACC3_max</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>Magnitude_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...</td>\n",
       "      <td>[27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...</td>\n",
       "      <td>[40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...</td>\n",
       "      <td>[32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.275354 0.276634 0.270231 0.270231 0.26895 0...</td>\n",
       "      <td>[15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...</td>\n",
       "      <td>[78.98 78.83500000000002 78.69 78.545 78.4 78....</td>\n",
       "      <td>[63.410093833710725 63.453053512025726 63.4961...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>32.350</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>-0.082592</td>\n",
       "      <td>-0.558848</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>-0.392823</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.531557</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.456522</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>69.7650</td>\n",
       "      <td>61.692787</td>\n",
       "      <td>41.543478</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>34.83</td>\n",
       "      <td>78.98</td>\n",
       "      <td>63.757353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0 39.06521739130435 39.130434782608695 39....</td>\n",
       "      <td>[29.0 28.93478260869565 28.869565217391305 28....</td>\n",
       "      <td>[38.0 38.02173913043478 38.04347826086956 38.0...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.25998499999999997 0.25998499999999997 0.258...</td>\n",
       "      <td>[-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...</td>\n",
       "      <td>[73.52 73.435 73.35 73.265 73.18 73.0925 73.00...</td>\n",
       "      <td>[61.69278726074872 61.7168170027034 61.7409828...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>32.339</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.170923</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>64.8025</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>37.72</td>\n",
       "      <td>73.52</td>\n",
       "      <td>62.936476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392 41.67391304347826 41.739130...</td>\n",
       "      <td>[26.39130434782609 26.32608695652174 26.260869...</td>\n",
       "      <td>[38.869565217391305 38.89130434782609 38.91304...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.265108 0.263827 0.266389 0.265108 0.266389 ...</td>\n",
       "      <td>[-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...</td>\n",
       "      <td>[69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...</td>\n",
       "      <td>[62.758486272725364 62.78782873035957 62.81730...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>32.337</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>0.227966</td>\n",
       "      <td>1.329886</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>-0.274279</td>\n",
       "      <td>0.185657</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.252301</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>60.9950</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>45.532258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>47.14</td>\n",
       "      <td>69.63</td>\n",
       "      <td>64.010791</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575 44.14285714285715 44.31428...</td>\n",
       "      <td>[24.514285714285712 24.42857142857143 24.34285...</td>\n",
       "      <td>[37.17142857142857 37.142857142857146 37.11428...</td>\n",
       "      <td>[32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.258704 0.258704 0.258704 0.257424 0.257424 ...</td>\n",
       "      <td>[17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....</td>\n",
       "      <td>[64.68 64.555 64.43 64.305 64.18 64.0475 63.91...</td>\n",
       "      <td>[62.579164557660036 62.649331804179724 62.7200...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>-4.941414</td>\n",
       "      <td>-1.040349</td>\n",
       "      <td>3.435987</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>-0.828441</td>\n",
       "      <td>0.406263</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.985816</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.8025</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>47.14</td>\n",
       "      <td>64.68</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418 45.564516129032256 45.58064...</td>\n",
       "      <td>[25.64516129032258 25.69354838709677 25.741935...</td>\n",
       "      <td>[37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.253581 0.253581 0.253581 0.252301 0.252301 ...</td>\n",
       "      <td>[-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...</td>\n",
       "      <td>[60.92 60.8475 60.77500000000001 60.7025 60.63...</td>\n",
       "      <td>[64.04162603123257 64.07248675362091 64.103373...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>32.389</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-1.857224</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>1.380509</td>\n",
       "      <td>-0.686481</td>\n",
       "      <td>1.239716</td>\n",
       "      <td>-0.833856</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.408229</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.702128</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.249739</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.5300</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.43</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>39.00</td>\n",
       "      <td>60.92</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...   \n",
       "1  [39.0 39.06521739130435 39.130434782608695 39....   \n",
       "2  [41.60869565217392 41.67391304347826 41.739130...   \n",
       "3  [43.971428571428575 44.14285714285715 44.31428...   \n",
       "4  [45.54838709677418 45.564516129032256 45.58064...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...   \n",
       "1  [29.0 28.93478260869565 28.869565217391305 28....   \n",
       "2  [26.39130434782609 26.32608695652174 26.260869...   \n",
       "3  [24.514285714285712 24.42857142857143 24.34285...   \n",
       "4  [25.64516129032258 25.69354838709677 25.741935...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...   \n",
       "1  [38.0 38.02173913043478 38.04347826086956 38.0...   \n",
       "2  [38.869565217391305 38.89130434782609 38.91304...   \n",
       "3  [37.17142857142857 37.142857142857146 37.11428...   \n",
       "4  [37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....   \n",
       "1  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "2  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "3  [32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....   \n",
       "4  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.275354 0.276634 0.270231 0.270231 0.26895 0...   \n",
       "1  [0.25998499999999997 0.25998499999999997 0.258...   \n",
       "2  [0.265108 0.263827 0.266389 0.265108 0.266389 ...   \n",
       "3  [0.258704 0.258704 0.258704 0.257424 0.257424 ...   \n",
       "4  [0.253581 0.253581 0.253581 0.252301 0.252301 ...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...   \n",
       "1  [-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...   \n",
       "2  [-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...   \n",
       "3  [17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....   \n",
       "4  [-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [78.98 78.83500000000002 78.69 78.545 78.4 78....   \n",
       "1  [73.52 73.435 73.35 73.265 73.18 73.0925 73.00...   \n",
       "2  [69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...   \n",
       "3  [64.68 64.555 64.43 64.305 64.18 64.0475 63.91...   \n",
       "4  [60.92 60.8475 60.77500000000001 60.7025 60.63...   \n",
       "\n",
       "                                           Magnitude Subject_ID  Activity  \\\n",
       "0  [63.410093833710725 63.453053512025726 63.4961...     19-001  Baseline   \n",
       "1  [61.69278726074872 61.7168170027034 61.7409828...     19-001  Baseline   \n",
       "2  [62.758486272725364 62.78782873035957 62.81730...     19-001  Baseline   \n",
       "3  [62.579164557660036 62.649331804179724 62.7200...     19-001  Baseline   \n",
       "4  [64.04162603123257 64.07248675362091 64.103373...     19-001  Baseline   \n",
       "\n",
       "   Round  ACC1_mean  ACC2_mean  ACC3_mean  TEMP_mean  EDA_mean  BVP_mean  \\\n",
       "0      1  40.248370  28.012880  38.824457     32.350  0.262354 -0.109875   \n",
       "1      1  40.820000  26.815000  38.192500     32.339  0.261058  0.321375   \n",
       "2      1  43.252235  25.312684  37.488043     32.337  0.259585  0.684000   \n",
       "3      1  44.905798  24.915984  37.638218     32.356  0.254510 -0.180875   \n",
       "4      1  43.577055  22.974382  38.971144     32.389  0.252733 -0.209750   \n",
       "\n",
       "     HR_mean  Magnitude_mean  ACC1_std  ACC2_std  ACC3_std  TEMP_std  \\\n",
       "0  73.931187       62.553853  0.701573  0.687590  0.632616  0.017607   \n",
       "1  69.481750       62.021872  1.192214  1.149559  0.529382  0.012610   \n",
       "2  64.893188       62.621785  2.109896  0.815025  0.647914  0.010536   \n",
       "3  61.157687       63.734171  1.832017  1.509593  1.773398  0.025377   \n",
       "4  59.226438       62.913435  2.115371  2.585687  1.809092  0.027000   \n",
       "\n",
       "    EDA_std    BVP_std    HR_std  Magnitude_std  ACC1_skew  ACC2_skew  \\\n",
       "0  0.004877  18.439453  2.574676       0.609756  -0.082592  -0.558848   \n",
       "1  0.003007  20.104717  2.608254       0.542348   0.515544   0.109446   \n",
       "2  0.004337  23.756276  2.639037       0.942868  -0.473020   0.227966   \n",
       "3  0.002396  25.635645  1.674001       0.841361  -4.941414  -1.040349   \n",
       "4  0.002055  25.593597  0.684349       1.365652  -1.857224   0.511935   \n",
       "\n",
       "   ACC3_skew  TEMP_skew  EDA_skew  BVP_skew   HR_skew  Magnitude_skew  \\\n",
       "0   0.705668   0.714533  0.896382 -0.392823  0.296262        0.531557   \n",
       "1  -0.188071   0.787066  0.212943 -0.322900 -0.170923       -0.438037   \n",
       "2   1.329886   0.620801  0.072564 -0.274279  0.185657       -0.382833   \n",
       "3   3.435987   0.672586  0.734482 -0.828441  0.406263       -0.532117   \n",
       "4   1.380509  -0.686481  1.239716 -0.833856  0.996232        0.408229   \n",
       "\n",
       "   ACC1_min   ACC2_min  ACC3_min  TEMP_min   EDA_min  BVP_min   HR_min  \\\n",
       "0      39.0  26.456522      38.0     32.33  0.254862   -42.99  69.7650   \n",
       "1      39.0  24.600000      37.2     32.31  0.254862   -48.52  64.8025   \n",
       "2      39.0  24.000000      37.0     32.31  0.252301   -48.52  60.9950   \n",
       "3      32.0  20.985816      37.0     32.33  0.251020  -101.74  58.8025   \n",
       "4      32.0  20.702128      37.0     32.33  0.249739  -101.74  58.5300   \n",
       "\n",
       "   Magnitude_min   ACC1_max  ACC2_max  ACC3_max  TEMP_max   EDA_max  BVP_max  \\\n",
       "0      61.692787  41.543478      29.0      40.0     32.39  0.276634    34.83   \n",
       "1      60.778286  43.800000      29.0      39.0     32.37  0.266389    37.72   \n",
       "2      60.778286  45.532258      27.0      39.0     32.37  0.266389    47.14   \n",
       "3      61.392182  46.000000      27.0      48.0     32.41  0.262546    47.14   \n",
       "4      61.392182  46.000000      27.0      48.0     32.43  0.258704    39.00   \n",
       "\n",
       "   HR_max  Magnitude_max  count  \n",
       "0   78.98      63.757353      1  \n",
       "1   73.52      62.936476      2  \n",
       "2   69.63      64.010791      3  \n",
       "3   64.68      65.711491      4  \n",
       "4   60.92      65.711491      5  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 17 45  9 52  7 25 38 10 36  6]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rands = np.random.choice(df.Subject_ID.unique(),11, replace=False)\n",
    "print(rands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Subjects into Test and Train Sets (n=44,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Subject_ID'].isin(rands)] \n",
    "train = df[-df['Subject_ID'].isin(rands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this for only Accelerometry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'Magnitude_mean', 'ACC1_std',\n",
    "       'ACC2_std', 'ACC3_std', 'Magnitude_std', 'Subject_ID', 'count', 'Activity']]\n",
    "test = test[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'Magnitude_mean', 'ACC1_std',\n",
    "       'ACC2_std', 'ACC3_std', 'Magnitude_std', 'Subject_ID', 'count', 'Activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train_df is made so we can use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SID = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] =1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'], prefix = 'SID')], axis =1).drop('Subject_ID', axis =1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['count'], prefix = 'count')], axis =1).drop('count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270, 122) (1302, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/N1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop([\"train\"], axis = 1, inplace = True)\n",
    "test.drop([\"train\"], axis = 1, inplace = True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACC1_mean', 'ACC2_mean', 'ACC3_mean', 'Magnitude_mean', 'ACC1_std',\n",
       "       'ACC2_std', 'ACC3_std', 'Magnitude_std', 'Activity', 'SID_0',\n",
       "       ...\n",
       "       'count_49', 'count_50', 'count_51', 'count_52', 'count_53', 'count_54',\n",
       "       'count_55', 'count_56', 'count_57', 'count_58'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train.drop(\"Activity\", axis =1)\n",
    "test_f = test.drop(\"Activity\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_f\n",
    "y_train = train.Activity\n",
    "X_test = test_f\n",
    "y_test = test.Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>SID_0</th>\n",
       "      <th>SID_1</th>\n",
       "      <th>SID_2</th>\n",
       "      <th>SID_3</th>\n",
       "      <th>SID_4</th>\n",
       "      <th>SID_5</th>\n",
       "      <th>SID_6</th>\n",
       "      <th>SID_7</th>\n",
       "      <th>SID_8</th>\n",
       "      <th>SID_9</th>\n",
       "      <th>SID_10</th>\n",
       "      <th>SID_11</th>\n",
       "      <th>SID_12</th>\n",
       "      <th>SID_13</th>\n",
       "      <th>SID_14</th>\n",
       "      <th>SID_15</th>\n",
       "      <th>SID_16</th>\n",
       "      <th>SID_17</th>\n",
       "      <th>SID_18</th>\n",
       "      <th>SID_19</th>\n",
       "      <th>SID_20</th>\n",
       "      <th>SID_21</th>\n",
       "      <th>SID_22</th>\n",
       "      <th>SID_23</th>\n",
       "      <th>SID_24</th>\n",
       "      <th>SID_25</th>\n",
       "      <th>SID_26</th>\n",
       "      <th>SID_27</th>\n",
       "      <th>SID_28</th>\n",
       "      <th>SID_29</th>\n",
       "      <th>SID_30</th>\n",
       "      <th>SID_31</th>\n",
       "      <th>SID_32</th>\n",
       "      <th>SID_33</th>\n",
       "      <th>SID_34</th>\n",
       "      <th>SID_35</th>\n",
       "      <th>SID_36</th>\n",
       "      <th>SID_37</th>\n",
       "      <th>SID_38</th>\n",
       "      <th>SID_39</th>\n",
       "      <th>SID_40</th>\n",
       "      <th>SID_41</th>\n",
       "      <th>SID_42</th>\n",
       "      <th>SID_43</th>\n",
       "      <th>SID_44</th>\n",
       "      <th>SID_45</th>\n",
       "      <th>SID_46</th>\n",
       "      <th>SID_47</th>\n",
       "      <th>SID_48</th>\n",
       "      <th>SID_49</th>\n",
       "      <th>SID_50</th>\n",
       "      <th>SID_51</th>\n",
       "      <th>SID_52</th>\n",
       "      <th>SID_53</th>\n",
       "      <th>SID_54</th>\n",
       "      <th>count_1</th>\n",
       "      <th>count_2</th>\n",
       "      <th>count_3</th>\n",
       "      <th>count_4</th>\n",
       "      <th>count_5</th>\n",
       "      <th>count_6</th>\n",
       "      <th>count_7</th>\n",
       "      <th>count_8</th>\n",
       "      <th>count_9</th>\n",
       "      <th>count_10</th>\n",
       "      <th>count_11</th>\n",
       "      <th>count_12</th>\n",
       "      <th>count_13</th>\n",
       "      <th>count_14</th>\n",
       "      <th>count_15</th>\n",
       "      <th>count_16</th>\n",
       "      <th>count_17</th>\n",
       "      <th>count_18</th>\n",
       "      <th>count_19</th>\n",
       "      <th>count_20</th>\n",
       "      <th>count_21</th>\n",
       "      <th>count_22</th>\n",
       "      <th>count_23</th>\n",
       "      <th>count_24</th>\n",
       "      <th>count_25</th>\n",
       "      <th>count_26</th>\n",
       "      <th>count_27</th>\n",
       "      <th>count_28</th>\n",
       "      <th>count_29</th>\n",
       "      <th>count_30</th>\n",
       "      <th>count_31</th>\n",
       "      <th>count_32</th>\n",
       "      <th>count_33</th>\n",
       "      <th>count_34</th>\n",
       "      <th>count_35</th>\n",
       "      <th>count_36</th>\n",
       "      <th>count_37</th>\n",
       "      <th>count_38</th>\n",
       "      <th>count_39</th>\n",
       "      <th>count_40</th>\n",
       "      <th>count_41</th>\n",
       "      <th>count_42</th>\n",
       "      <th>count_43</th>\n",
       "      <th>count_44</th>\n",
       "      <th>count_45</th>\n",
       "      <th>count_46</th>\n",
       "      <th>count_47</th>\n",
       "      <th>count_48</th>\n",
       "      <th>count_49</th>\n",
       "      <th>count_50</th>\n",
       "      <th>count_51</th>\n",
       "      <th>count_52</th>\n",
       "      <th>count_53</th>\n",
       "      <th>count_54</th>\n",
       "      <th>count_55</th>\n",
       "      <th>count_56</th>\n",
       "      <th>count_57</th>\n",
       "      <th>count_58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>25.927500</td>\n",
       "      <td>-12.236250</td>\n",
       "      <td>46.733750</td>\n",
       "      <td>59.921697</td>\n",
       "      <td>17.017661</td>\n",
       "      <td>5.972112</td>\n",
       "      <td>17.824375</td>\n",
       "      <td>7.640022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>13.338895</td>\n",
       "      <td>-5.276078</td>\n",
       "      <td>65.181670</td>\n",
       "      <td>66.976772</td>\n",
       "      <td>2.920879</td>\n",
       "      <td>4.369355</td>\n",
       "      <td>5.616240</td>\n",
       "      <td>5.262136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>13.452000</td>\n",
       "      <td>-4.651000</td>\n",
       "      <td>56.675250</td>\n",
       "      <td>58.688932</td>\n",
       "      <td>1.503319</td>\n",
       "      <td>3.232566</td>\n",
       "      <td>12.371872</td>\n",
       "      <td>11.664220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>14.930137</td>\n",
       "      <td>-8.046923</td>\n",
       "      <td>47.678158</td>\n",
       "      <td>50.678627</td>\n",
       "      <td>1.350929</td>\n",
       "      <td>1.577617</td>\n",
       "      <td>5.591732</td>\n",
       "      <td>5.303471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>15.528588</td>\n",
       "      <td>-9.314588</td>\n",
       "      <td>57.195412</td>\n",
       "      <td>60.188016</td>\n",
       "      <td>3.555749</td>\n",
       "      <td>1.230064</td>\n",
       "      <td>8.234073</td>\n",
       "      <td>7.653292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5270 rows  121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACC1_mean  ACC2_mean  ACC3_mean  Magnitude_mean   ACC1_std  ACC2_std  \\\n",
       "0     40.248370  28.012880  38.824457       62.553853   0.701573  0.687590   \n",
       "1     40.820000  26.815000  38.192500       62.021872   1.192214  1.149559   \n",
       "2     43.252235  25.312684  37.488043       62.621785   2.109896  0.815025   \n",
       "3     44.905798  24.915984  37.638218       63.734171   1.832017  1.509593   \n",
       "4     43.577055  22.974382  38.971144       62.913435   2.115371  2.585687   \n",
       "...         ...        ...        ...             ...        ...       ...   \n",
       "6567  25.927500 -12.236250  46.733750       59.921697  17.017661  5.972112   \n",
       "6568  13.338895  -5.276078  65.181670       66.976772   2.920879  4.369355   \n",
       "6569  13.452000  -4.651000  56.675250       58.688932   1.503319  3.232566   \n",
       "6570  14.930137  -8.046923  47.678158       50.678627   1.350929  1.577617   \n",
       "6571  15.528588  -9.314588  57.195412       60.188016   3.555749  1.230064   \n",
       "\n",
       "       ACC3_std  Magnitude_std  SID_0  SID_1  SID_2  SID_3  SID_4  SID_5  \\\n",
       "0      0.632616       0.609756      1      0      0      0      0      0   \n",
       "1      0.529382       0.542348      1      0      0      0      0      0   \n",
       "2      0.647914       0.942868      1      0      0      0      0      0   \n",
       "3      1.773398       0.841361      1      0      0      0      0      0   \n",
       "4      1.809092       1.365652      1      0      0      0      0      0   \n",
       "...         ...            ...    ...    ...    ...    ...    ...    ...   \n",
       "6567  17.824375       7.640022      0      0      0      0      0      0   \n",
       "6568   5.616240       5.262136      0      0      0      0      0      0   \n",
       "6569  12.371872      11.664220      0      0      0      0      0      0   \n",
       "6570   5.591732       5.303471      0      0      0      0      0      0   \n",
       "6571   8.234073       7.653292      0      0      0      0      0      0   \n",
       "\n",
       "      SID_6  SID_7  SID_8  SID_9  SID_10  SID_11  SID_12  SID_13  SID_14  \\\n",
       "0         0      0      0      0       0       0       0       0       0   \n",
       "1         0      0      0      0       0       0       0       0       0   \n",
       "2         0      0      0      0       0       0       0       0       0   \n",
       "3         0      0      0      0       0       0       0       0       0   \n",
       "4         0      0      0      0       0       0       0       0       0   \n",
       "...     ...    ...    ...    ...     ...     ...     ...     ...     ...   \n",
       "6567      0      0      0      0       0       0       0       0       0   \n",
       "6568      0      0      0      0       0       0       0       0       0   \n",
       "6569      0      0      0      0       0       0       0       0       0   \n",
       "6570      0      0      0      0       0       0       0       0       0   \n",
       "6571      0      0      0      0       0       0       0       0       0   \n",
       "\n",
       "      SID_15  SID_16  SID_17  SID_18  SID_19  SID_20  SID_21  SID_22  SID_23  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_24  SID_25  SID_26  SID_27  SID_28  SID_29  SID_30  SID_31  SID_32  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_33  SID_34  SID_35  SID_36  SID_37  SID_38  SID_39  SID_40  SID_41  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_42  SID_43  SID_44  SID_45  SID_46  SID_47  SID_48  SID_49  SID_50  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_51  SID_52  SID_53  SID_54  count_1  count_2  count_3  count_4  \\\n",
       "0          0       0       0       0        1        0        0        0   \n",
       "1          0       0       0       0        0        1        0        0   \n",
       "2          0       0       0       0        0        0        1        0   \n",
       "3          0       0       0       0        0        0        0        1   \n",
       "4          0       0       0       0        0        0        0        0   \n",
       "...      ...     ...     ...     ...      ...      ...      ...      ...   \n",
       "6567       0       0       0       1        1        0        0        0   \n",
       "6568       0       0       0       1        0        1        0        0   \n",
       "6569       0       0       0       1        0        0        1        0   \n",
       "6570       0       0       0       1        0        0        0        1   \n",
       "6571       0       0       0       1        0        0        0        0   \n",
       "\n",
       "      count_5  count_6  count_7  count_8  count_9  count_10  count_11  \\\n",
       "0           0        0        0        0        0         0         0   \n",
       "1           0        0        0        0        0         0         0   \n",
       "2           0        0        0        0        0         0         0   \n",
       "3           0        0        0        0        0         0         0   \n",
       "4           1        0        0        0        0         0         0   \n",
       "...       ...      ...      ...      ...      ...       ...       ...   \n",
       "6567        0        0        0        0        0         0         0   \n",
       "6568        0        0        0        0        0         0         0   \n",
       "6569        0        0        0        0        0         0         0   \n",
       "6570        0        0        0        0        0         0         0   \n",
       "6571        1        0        0        0        0         0         0   \n",
       "\n",
       "      count_12  count_13  count_14  count_15  count_16  count_17  count_18  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_19  count_20  count_21  count_22  count_23  count_24  count_25  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_26  count_27  count_28  count_29  count_30  count_31  count_32  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_33  count_34  count_35  count_36  count_37  count_38  count_39  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_40  count_41  count_42  count_43  count_44  count_45  count_46  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_47  count_48  count_49  count_50  count_51  count_52  count_53  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_54  count_55  count_56  count_57  count_58  \n",
       "0            0         0         0         0         0  \n",
       "1            0         0         0         0         0  \n",
       "2            0         0         0         0         0  \n",
       "3            0         0         0         0         0  \n",
       "4            0         0         0         0         0  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "6567         0         0         0         0         0  \n",
       "6568         0         0         0         0         0  \n",
       "6569         0         0         0         0         0  \n",
       "6570         0         0         0         0         0  \n",
       "6571         0         0         0         0         0  \n",
       "\n",
       "[5270 rows x 121 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:16] = sc.fit_transform(X_train.iloc[:,:16])\n",
    "X_test.iloc[:,:16] = sc.transform(X_test.iloc[:,:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Object Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training and validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9037 - accuracy: 0.6535\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5665 - accuracy: 0.7765\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8179\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8692\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.9023\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9199\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9306\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9380\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9479\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9493\n",
      "Score for fold 1: loss of 1.7083476781845093; accuracy of 64.51612710952759%, F1 of 0.596671066743668\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9099 - accuracy: 0.6595\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7909\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8364\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8704\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.8949\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9110\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9219\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9335\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9423\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9442\n",
      "Score for fold 2: loss of 0.6648320555686951; accuracy of 79.83871102333069%, F1 of 0.7649433156251711\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9449 - accuracy: 0.6061\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7719\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4709 - accuracy: 0.8028\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8280\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8729\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.9005\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9223\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9345\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9427\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9475\n",
      "Score for fold 3: loss of 3.2251710891723633; accuracy of 58.870965242385864%, F1 of 0.5056283127811879\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9295 - accuracy: 0.6551\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5941 - accuracy: 0.7691\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8241\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8869\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.9112\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9260\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9341\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9388\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9446\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9489\n",
      "Score for fold 4: loss of 1.6282453536987305; accuracy of 50.806450843811035%, F1 of 0.5327205225295347\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9471 - accuracy: 0.6284\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.7779\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8502\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8960\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9238\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9357\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9390\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9506\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9479\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9549\n",
      "Score for fold 5: loss of 3.328608274459839; accuracy of 43.54838728904724%, F1 of 0.30938261326490923\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9472 - accuracy: 0.6061\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6013 - accuracy: 0.7577\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8348\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.9050\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.9234\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9339\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9378\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9438\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9528\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9543\n",
      "Score for fold 6: loss of 1.630079746246338; accuracy of 71.77419066429138%, F1 of 0.6652359868678656\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9274 - accuracy: 0.6415\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5448 - accuracy: 0.7841\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8502\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9195\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9217\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9396\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9460\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9520\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9588\n",
      "Score for fold 7: loss of 0.5572362542152405; accuracy of 84.67742204666138%, F1 of 0.8548792237792732\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9370 - accuracy: 0.6201\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.7660\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8539\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9192\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9256\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9390\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9438\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9522\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9530\n",
      "Score for fold 8: loss of 0.7643086910247803; accuracy of 78.22580933570862%, F1 of 0.7810016460583225\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9552 - accuracy: 0.6238\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.7717\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8556\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.8982\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.9124\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9230\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9370\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9401\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9468\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9497\n",
      "Score for fold 9: loss of 0.291485995054245; accuracy of 89.51612710952759%, F1 of 0.8975404770732912\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8963 - accuracy: 0.6543\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.7658\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3959 - accuracy: 0.8350\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8877\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9124\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9234\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1829 - accuracy: 0.9310\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9466\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9495\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9469\n",
      "Score for fold 10: loss of 0.5528564453125; accuracy of 83.06451439857483%, F1 of 0.83252021414307\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9412 - accuracy: 0.6446\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.7750: 0s - loss: 0.6067 - accuracy: 0.76\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8562\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8974\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.9127\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9271\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9382\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9477\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9553\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9539\n",
      "Score for fold 11: loss of 2.363436698913574; accuracy of 60.48387289047241%, F1 of 0.5964405535264542\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9302 - accuracy: 0.6479\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5555 - accuracy: 0.7798\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8428\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8958\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.9118\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9285\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9333\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9349\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9427\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9520\n",
      "Score for fold 12: loss of 0.6890842318534851; accuracy of 79.03226017951965%, F1 of 0.8036873370460325\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9510 - accuracy: 0.6121\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5964 - accuracy: 0.7730\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8290\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 986us/step - loss: 0.3119 - accuracy: 0.8840\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.9153\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9318\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9388\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9456\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9473\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9569\n",
      "Score for fold 13: loss of 0.49871793389320374; accuracy of 87.09677457809448%, F1 of 0.873913401722078\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8892 - accuracy: 0.6621\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5895 - accuracy: 0.7723\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.8047\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8811\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.9176\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9380\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9419\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9450\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9574\n",
      "Score for fold 14: loss of 0.36156028509140015; accuracy of 91.93548560142517%, F1 of 0.8826058622692563\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9530 - accuracy: 0.6504\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6191 - accuracy: 0.7610\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8218\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9102\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9230\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9335\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 990us/step - loss: 0.1656 - accuracy: 0.9376\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9468\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9522\n",
      "Score for fold 15: loss of 0.406220406293869; accuracy of 88.70967626571655%, F1 of 0.8925668917229308\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.9665 - accuracy: 0.5924\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5836 - accuracy: 0.7684\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.3863 - accuracy: 0.8393\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.2367 - accuracy: 0.9161\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9272\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9320\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1655 - accuracy: 0.9361\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9453\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1466 - accuracy: 0.9489\n",
      "Score for fold 16: loss of 0.14482338726520538; accuracy of 93.54838728904724%, F1 of 0.9394592391196805\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.6265\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5957 - accuracy: 0.7717\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.8074\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.9001\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9207\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9314\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1723 - accuracy: 0.9392\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1583 - accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9506\n",
      "Score for fold 17: loss of 0.44377627968788147; accuracy of 87.09677457809448%, F1 of 0.8792981897081165\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9746 - accuracy: 0.6207\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.7627\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.9106\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9215\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9333\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9363\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9419\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9489\n",
      "Score for fold 18: loss of 0.3004695475101471; accuracy of 85.48387289047241%, F1 of 0.8604726935878367\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9583 - accuracy: 0.6162\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6061 - accuracy: 0.7676\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8070: 0s - loss: 0.4635 - accuracy: 0.80\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8568\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9098\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9197\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9376\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1622 - accuracy: 0.9440\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.9429\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9561\n",
      "Score for fold 19: loss of 0.3754764199256897; accuracy of 87.90322542190552%, F1 of 0.8791941961393962\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8975 - accuracy: 0.6496\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.7713\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8199\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8780\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.9050\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9188\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1884 - accuracy: 0.9306\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9409\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9506\n",
      "Score for fold 20: loss of 0.061246149241924286; accuracy of 97.5806474685669%, F1 of 0.9757243003450353\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8388 - accuracy: 0.6710\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7837\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8807\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9159\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9213\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9322\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9378\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9460\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9466\n",
      "Score for fold 21: loss of 0.16457407176494598; accuracy of 91.93548560142517%, F1 of 0.919962945281775\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9644 - accuracy: 0.6162\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6225 - accuracy: 0.7623\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.8016\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8682\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8947\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.9166\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2095 - accuracy: 0.9281\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9277\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1709 - accuracy: 0.9372\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9438\n",
      "Score for fold 22: loss of 0.18577370047569275; accuracy of 91.93548560142517%, F1 of 0.923908675346263\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 1.0469 - accuracy: 0.5457\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6129 - accuracy: 0.7759\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8596\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9224\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.9307\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1797 - accuracy: 0.9401\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9468\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9489\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9543\n",
      "Score for fold 23: loss of 1.538427710533142; accuracy of 59.67742204666138%, F1 of 0.6206595588291012\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9175 - accuracy: 0.6432\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7810\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.8465\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9143\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2089 - accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.9380\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9417\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1560 - accuracy: 0.9423\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1349 - accuracy: 0.9506\n",
      "Score for fold 24: loss of 0.24222664535045624; accuracy of 94.35483813285828%, F1 of 0.9444983129946709\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9578 - accuracy: 0.6430\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.7620\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.8030\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8426\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8861\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9143\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9236\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9366\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9419\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.9468\n",
      "Score for fold 25: loss of 0.10157497227191925; accuracy of 96.77419066429138%, F1 of 0.968621101510302\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9336 - accuracy: 0.6609\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.7711\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8078\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8506\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8826\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.9054\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9186\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9287\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9429\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.1450 - accuracy: 0.9446\n",
      "Score for fold 26: loss of 0.09712409973144531; accuracy of 96.77419066429138%, F1 of 0.9669004135823841\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9843 - accuracy: 0.6131\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7771\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8352\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8785\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9098\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9244\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9376\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9398\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1655 - accuracy: 0.9417\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9497\n",
      "Score for fold 27: loss of 0.3019666075706482; accuracy of 92.7419364452362%, F1 of 0.9347467735234599\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9389 - accuracy: 0.6292\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5877 - accuracy: 0.7674\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8434\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8966\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9143\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9248\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9337\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9528\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9516\n",
      "Score for fold 28: loss of 0.0984845906496048; accuracy of 97.5806474685669%, F1 of 0.976194433749612\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9279 - accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.7734\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8327\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8815\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2541 - accuracy: 0.9015\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2116 - accuracy: 0.9211\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1849 - accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9372\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9448\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9485\n",
      "Score for fold 29: loss of 1.01025390625; accuracy of 69.35483813285828%, F1 of 0.6736334649656112\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9328 - accuracy: 0.6178\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7833\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8397\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8785\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.9073\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9227\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9238\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9382\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9415\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9512\n",
      "Score for fold 30: loss of 0.296965628862381; accuracy of 86.29032373428345%, F1 of 0.8463334203996342\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9371 - accuracy: 0.6158\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6036 - accuracy: 0.7728\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8136\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2643 - accuracy: 0.9058\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9230\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9324\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9425\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9458\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9526\n",
      "Score for fold 31: loss of 0.2566963732242584; accuracy of 91.93548560142517%, F1 of 0.9213710929129192\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9538 - accuracy: 0.6345\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5924 - accuracy: 0.7761\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4149 - accuracy: 0.8317\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.9079\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9230\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9343\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9368\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9431\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9510\n",
      "Score for fold 32: loss of 0.21835832297801971; accuracy of 91.93548560142517%, F1 of 0.9257344284746006\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9282 - accuracy: 0.6426\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6148 - accuracy: 0.7596\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4269 - accuracy: 0.8354\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8805\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.9069\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9267\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9353\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9405\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9450\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9541\n",
      "Score for fold 33: loss of 0.28382840752601624; accuracy of 88.70967626571655%, F1 of 0.8890600584907391\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9116 - accuracy: 0.6529\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5850 - accuracy: 0.7721\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8325\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8782\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.9013\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9176\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9232\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9357\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9370\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.9403\n",
      "Score for fold 34: loss of 0.5139033794403076; accuracy of 83.87096524238586%, F1 of 0.8311574809507872\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9305 - accuracy: 0.6391\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6219 - accuracy: 0.7773\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4842 - accuracy: 0.8074\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8459\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8861\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2393 - accuracy: 0.9102\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9248\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9322\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9421\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9499\n",
      "Score for fold 35: loss of 0.2745361626148224; accuracy of 92.7419364452362%, F1 of 0.9194582299421009\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9769 - accuracy: 0.6226\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7765\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3929 - accuracy: 0.8486\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8906\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9139\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9285\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9361\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9384\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9435\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9526\n",
      "Score for fold 36: loss of 0.13883478939533234; accuracy of 95.96773982048035%, F1 of 0.959351835679068\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9583 - accuracy: 0.6191: 0s - loss: 0.9707 - accuracy: 0.61\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.7654\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8426\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8888\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9110\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9256\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9333\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9372\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9440\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9528\n",
      "Score for fold 37: loss of 0.2326887845993042; accuracy of 92.7419364452362%, F1 of 0.9315351991216742\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9126 - accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5887 - accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8096\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8640\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.9005\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9168\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9223\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9326\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9440\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9485\n",
      "Score for fold 38: loss of 0.25774112343788147; accuracy of 91.12903475761414%, F1 of 0.9125632130343984\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9394 - accuracy: 0.6448\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7734\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.8424\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8733\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.9083\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2276 - accuracy: 0.9188\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9312\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9347\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9448\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9504\n",
      "Score for fold 39: loss of 0.06437306851148605; accuracy of 97.5806474685669%, F1 of 0.9740537301508039\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8729 - accuracy: 0.6658\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7864\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8362\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8721\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9151\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9256\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9351\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9431\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9522\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9586\n",
      "Score for fold 40: loss of 0.9987678527832031; accuracy of 68.54838728904724%, F1 of 0.7134954943841127\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9722 - accuracy: 0.6438\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.7709\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4203 - accuracy: 0.8344\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8801\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9149\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9207\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9326\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9419\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9473\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9495\n",
      "Score for fold 41: loss of 0.37613970041275024; accuracy of 92.7419364452362%, F1 of 0.926251946373312\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9244 - accuracy: 0.6185\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7791\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8234\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8704\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.9083\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9137\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9285\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9343\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9401\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9433\n",
      "Score for fold 42: loss of 0.5727632641792297; accuracy of 81.45161271095276%, F1 of 0.8097806308285274\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9551 - accuracy: 0.6115\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.7522\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7938\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8640\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2571 - accuracy: 0.9059\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9242\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9295\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.9380\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9448\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9547\n",
      "Score for fold 43: loss of 0.47248372435569763; accuracy of 87.09677457809448%, F1 of 0.8687710284877869\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.9245 - accuracy: 0.6430\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5865 - accuracy: 0.7652\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8305\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.8902\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.9082\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9180\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9286\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9374\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9449\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9476\n",
      "Score for fold 44: loss of 0.42291954159736633; accuracy of 82.2580635547638%, F1 of 0.8006429844665138\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.7083476781845093 - Accuracy: 64.51612710952759% - F1:0.596671066743668%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6648320555686951 - Accuracy: 79.83871102333069% - F1:0.7649433156251711%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 3.2251710891723633 - Accuracy: 58.870965242385864% - F1:0.5056283127811879%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.6282453536987305 - Accuracy: 50.806450843811035% - F1:0.5327205225295347%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 3.328608274459839 - Accuracy: 43.54838728904724% - F1:0.30938261326490923%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.630079746246338 - Accuracy: 71.77419066429138% - F1:0.6652359868678656%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.5572362542152405 - Accuracy: 84.67742204666138% - F1:0.8548792237792732%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.7643086910247803 - Accuracy: 78.22580933570862% - F1:0.7810016460583225%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.291485995054245 - Accuracy: 89.51612710952759% - F1:0.8975404770732912%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.5528564453125 - Accuracy: 83.06451439857483% - F1:0.83252021414307%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 2.363436698913574 - Accuracy: 60.48387289047241% - F1:0.5964405535264542%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 0.6890842318534851 - Accuracy: 79.03226017951965% - F1:0.8036873370460325%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 0.49871793389320374 - Accuracy: 87.09677457809448% - F1:0.873913401722078%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 0.36156028509140015 - Accuracy: 91.93548560142517% - F1:0.8826058622692563%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 0.406220406293869 - Accuracy: 88.70967626571655% - F1:0.8925668917229308%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 0.14482338726520538 - Accuracy: 93.54838728904724% - F1:0.9394592391196805%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.44377627968788147 - Accuracy: 87.09677457809448% - F1:0.8792981897081165%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 0.3004695475101471 - Accuracy: 85.48387289047241% - F1:0.8604726935878367%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.3754764199256897 - Accuracy: 87.90322542190552% - F1:0.8791941961393962%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.061246149241924286 - Accuracy: 97.5806474685669% - F1:0.9757243003450353%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 0.16457407176494598 - Accuracy: 91.93548560142517% - F1:0.919962945281775%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 0.18577370047569275 - Accuracy: 91.93548560142517% - F1:0.923908675346263%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.538427710533142 - Accuracy: 59.67742204666138% - F1:0.6206595588291012%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.24222664535045624 - Accuracy: 94.35483813285828% - F1:0.9444983129946709%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.10157497227191925 - Accuracy: 96.77419066429138% - F1:0.968621101510302%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.09712409973144531 - Accuracy: 96.77419066429138% - F1:0.9669004135823841%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.3019666075706482 - Accuracy: 92.7419364452362% - F1:0.9347467735234599%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 0.0984845906496048 - Accuracy: 97.5806474685669% - F1:0.976194433749612%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.01025390625 - Accuracy: 69.35483813285828% - F1:0.6736334649656112%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 0.296965628862381 - Accuracy: 86.29032373428345% - F1:0.8463334203996342%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.2566963732242584 - Accuracy: 91.93548560142517% - F1:0.9213710929129192%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.21835832297801971 - Accuracy: 91.93548560142517% - F1:0.9257344284746006%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.28382840752601624 - Accuracy: 88.70967626571655% - F1:0.8890600584907391%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 0.5139033794403076 - Accuracy: 83.87096524238586% - F1:0.8311574809507872%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.2745361626148224 - Accuracy: 92.7419364452362% - F1:0.9194582299421009%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.13883478939533234 - Accuracy: 95.96773982048035% - F1:0.959351835679068%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.2326887845993042 - Accuracy: 92.7419364452362% - F1:0.9315351991216742%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 0.25774112343788147 - Accuracy: 91.12903475761414% - F1:0.9125632130343984%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 0.06437306851148605 - Accuracy: 97.5806474685669% - F1:0.9740537301508039%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 0.9987678527832031 - Accuracy: 68.54838728904724% - F1:0.7134954943841127%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.37613970041275024 - Accuracy: 92.7419364452362% - F1:0.926251946373312%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.5727632641792297 - Accuracy: 81.45161271095276% - F1:0.8097806308285274%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.47248372435569763 - Accuracy: 87.09677457809448% - F1:0.8687710284877869%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 0.42291954159736633 - Accuracy: 82.2580635547638% - F1:0.8006429844665138%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 83.63269824873318 (+- 13.138862593784358)\n",
      "> F1: 0.8291493749439378 (+- 0.14608163267020968)\n",
      "> Loss: 0.6617588488893076\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = train_SID\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/30_TF_FE_ACC_Only/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/30_TF_FE_ACC_Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/30_TF_FE_ACC_Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 779us/step - loss: 0.8035 - accuracy: 0.8402\n",
      "Test loss, Test acc: [0.803471565246582, 0.84024578332901]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=32)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[530,  78,   1,   0],\n",
       "       [ 20, 416,  10,  37],\n",
       "       [  1,  15,  84,   5],\n",
       "       [  0,  35,   6,  64]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87027915, 0.12807882, 0.00164204, 0.        ],\n",
       "       [0.04140787, 0.86128364, 0.02070393, 0.07660455],\n",
       "       [0.00952381, 0.14285714, 0.8       , 0.04761905],\n",
       "       [0.        , 0.33333333, 0.05714286, 0.60952381]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Only Phys Feature Engineered STEP ANN - Confusion Matrix')\n",
    "plt.savefig('30_figures/30_feat_engin_window_ANN_ACC_only.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.840 \n",
      "F1 Score: 0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

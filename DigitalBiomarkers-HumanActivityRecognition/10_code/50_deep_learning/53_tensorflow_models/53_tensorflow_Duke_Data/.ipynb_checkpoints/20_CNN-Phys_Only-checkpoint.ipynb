{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **plain_data.csv**\n",
    "\n",
    "This notebook converts our individual timepoint data into windows to be input into our model. It then label encodes Subject_ID and Activity (our y variable).  The Subject_ID is then one-hot encoded to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.275354</td>\n",
       "      <td>15.25</td>\n",
       "      <td>78.9800</td>\n",
       "      <td>63.410094</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>78.8350</td>\n",
       "      <td>63.453054</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>78.6900</td>\n",
       "      <td>63.496142</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>18.39</td>\n",
       "      <td>78.5450</td>\n",
       "      <td>63.539358</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.34</td>\n",
       "      <td>0.268950</td>\n",
       "      <td>13.61</td>\n",
       "      <td>78.4000</td>\n",
       "      <td>63.582702</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279835</th>\n",
       "      <td>21.176471</td>\n",
       "      <td>-11.176471</td>\n",
       "      <td>64.823529</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.708502</td>\n",
       "      <td>0.85</td>\n",
       "      <td>92.8275</td>\n",
       "      <td>69.104605</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279836</th>\n",
       "      <td>24.235294</td>\n",
       "      <td>-12.235294</td>\n",
       "      <td>62.764706</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.694414</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>92.8800</td>\n",
       "      <td>68.384649</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279837</th>\n",
       "      <td>27.294118</td>\n",
       "      <td>-13.294118</td>\n",
       "      <td>60.705882</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>5.22</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>67.874197</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279838</th>\n",
       "      <td>30.352941</td>\n",
       "      <td>-14.352941</td>\n",
       "      <td>58.647059</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.664957</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>93.0000</td>\n",
       "      <td>67.577995</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279839</th>\n",
       "      <td>33.411765</td>\n",
       "      <td>-15.411765</td>\n",
       "      <td>56.588235</td>\n",
       "      <td>32.09</td>\n",
       "      <td>0.648308</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.0600</td>\n",
       "      <td>67.498866</td>\n",
       "      <td>Type</td>\n",
       "      <td>19-056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279840 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ACC1       ACC2       ACC3   TEMP       EDA    BVP       HR  \\\n",
       "0       41.000000  27.200000  40.000000  32.39  0.275354  15.25  78.9800   \n",
       "1       41.000000  27.300000  40.000000  32.39  0.276634 -12.75  78.8350   \n",
       "2       41.000000  27.400000  40.000000  32.39  0.270231 -42.99  78.6900   \n",
       "3       41.000000  27.500000  40.000000  32.39  0.270231  18.39  78.5450   \n",
       "4       41.000000  27.600000  40.000000  32.34  0.268950  13.61  78.4000   \n",
       "...           ...        ...        ...    ...       ...    ...      ...   \n",
       "279835  21.176471 -11.176471  64.823529  32.09  0.708502   0.85  92.8275   \n",
       "279836  24.235294 -12.235294  62.764706  32.09  0.694414  -1.00  92.8800   \n",
       "279837  27.294118 -13.294118  60.705882  32.09  0.672642   5.22  92.9400   \n",
       "279838  30.352941 -14.352941  58.647059  32.09  0.664957  -1.47  93.0000   \n",
       "279839  33.411765 -15.411765  56.588235  32.09  0.648308   1.37  93.0600   \n",
       "\n",
       "        Magnitude  Activity Subject_ID  Round  \n",
       "0       63.410094  Baseline     19-001      1  \n",
       "1       63.453054  Baseline     19-001      1  \n",
       "2       63.496142  Baseline     19-001      1  \n",
       "3       63.539358  Baseline     19-001      1  \n",
       "4       63.582702  Baseline     19-001      1  \n",
       "...           ...       ...        ...    ...  \n",
       "279835  69.104605      Type     19-056      1  \n",
       "279836  68.384649      Type     19-056      1  \n",
       "279837  67.874197      Type     19-056      1  \n",
       "279838  67.577995      Type     19-056      1  \n",
       "279839  67.498866      Type     19-056      1  \n",
       "\n",
       "[279840 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('plain_data.csv') #read in the csv file\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_windows function below, subsets the data by Subject_ID, then by Activity, and finally by Activity Round. It then subsets for only columns that contain features that we will use in our model. This is done to only make windows of rows with the same Subject_ID, Activity, and Round. A window is a selection of readings from consecutive timepoints; the length of each window can be defined in the function below. Since our timepoints occur every 0.25 seconds, a window length of 40 timepoints would be equivalent to 10 second windows. \n",
    "\n",
    "The reason why we are making windows is to allow us to both engineer and extract features from our data, so that our model is able to quantify changes or patterns over time for each sensor reading as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_list = list(df['Activity'].unique()) \n",
    "sub_id_list = list(df['Subject_ID'].unique())\n",
    "round_list = list(df['Round'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_slider import Slider\n",
    "\n",
    "def make_windows(df, bucket_size, overlap_count):\n",
    "    \"\"\"\n",
    "    bucket_size = the window length (in rows) the data should be aggregated by\n",
    "    overlap_count = the number of overlapping timepoints from one window to the next\n",
    "    \"\"\"\n",
    "    window_list = []\n",
    "    final = pd.DataFrame()\n",
    "    activity_list = list(df['Activity'].unique()) #list of the four activities\n",
    "    sub_id_list = list(df['Subject_ID'].unique()) #list of the subject ids\n",
    "    round_list = list(df['Round'].unique())\n",
    "    df_list = []\n",
    "\n",
    "    for i in sub_id_list:\n",
    "        df_subject = df[df['Subject_ID'] == i] #isolate a single subject id\n",
    "        for j in activity_list:\n",
    "            df_subject_activity = df_subject[df_subject['Activity'] == j] #isolate by activity\n",
    "            for k in round_list:\n",
    "                df_subject_activity_round = df_subject_activity[df_subject_activity['Round'] == k]\n",
    "                final_df = pd.DataFrame()\n",
    "                if df_subject_activity_round.empty:\n",
    "                      pass\n",
    "                else:\n",
    "                    df_flat = df_subject_activity_round[['ACC1', 'ACC2','ACC3','TEMP','EDA','BVP','HR','Magnitude', 'Subject_ID']].T.values #array of arrays, each row is every single reading in an array for a sensor in that isolation \n",
    "\n",
    "                    slider = Slider(bucket_size,overlap_count)\n",
    "                    slider.fit(df_flat)\n",
    "                    while True:\n",
    "                        window_data = slider.slide()\n",
    "\n",
    "                        if slider.reached_end_of_list(): break\n",
    "                        window_list.append(list(window_data))\n",
    "                    final_df = final.append(window_list)\n",
    "                    final_df.columns = [['ACC1', 'ACC2','ACC3','TEMP','EDA','BVP','HR','Magnitude', 'SID']]\n",
    "                    final_df.insert(9, \"Subject_ID\", [i]*len(final_df), True)\n",
    "                    final_df.insert(10, \"Activity\", [j]*len(final_df), True)\n",
    "                    final_df.insert(11, \"Round\", [k]*len(final_df), True)\n",
    "                    df_list.append(final_df)\n",
    "                    window_list = []\n",
    "\n",
    "    final = pd.DataFrame(columns = df_list[0].columns)\n",
    "\n",
    "    for l in df_list:\n",
    "        final = final.append(l)\n",
    "\n",
    "    final\n",
    "    final.columns = final.columns.map(''.join)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the window length and the window overlap below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 40\n",
    "window_overlap = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed = make_windows(df, window_size, window_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a window number to each window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a window number to each window gives us another feature to use in the model. As a participant performs an activity, it their physiological sensor readings change overtime. In order to allow our model to better understand where in time a window is relative to the activity being performed, we have added window numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed=windowed.assign(count=windowed.groupby(windowed.Activity.ne(windowed.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_arr = []\n",
    "for i in range(len(windowed)):\n",
    "    rep_arr.append(np.repeat(np.array(windowed['count'].iloc[i]),window_size))\n",
    "windowed['count'] = rep_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11008, 13) (2560, 13)\n"
     ]
    }
   ],
   "source": [
    "ID_list = list(windowed['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "#change size of train/test split\n",
    "train = windowed[windowed['Subject_ID'].isin(ID_list[:45])]\n",
    "test = windowed[windowed['Subject_ID'].isin(ID_list[45:])]\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output below displays arrays of size 40 which are our window size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>SID</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-3.0, -13.0, -17.0, -6.0, -8.0, -14.0, 0.0, -...</td>\n",
       "      <td>[10.0, 10.0, 16.0, 14.0, 8.0, 15.0, 12.0, 15.0...</td>\n",
       "      <td>[-52.0, -64.0, -77.0, -62.0, -52.0, -65.0, -63...</td>\n",
       "      <td>[26.65, 26.65, 26.65, 26.65, 29.01, 29.01, 29....</td>\n",
       "      <td>[0.007684999999999998, 0.005123, 0.005123, 0.0...</td>\n",
       "      <td>[-46.27, -2.81, -81.67, -415.4, 442.23, 233.86...</td>\n",
       "      <td>[106.85, 106.75, 106.65, 106.55, 106.45, 106.3...</td>\n",
       "      <td>[53.03772242470448, 66.06814663663572, 80.4611...</td>\n",
       "      <td>[17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17....</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [-3.0, -13.0, -17.0, -6.0, -8.0, -14.0, 0.0, -...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [10.0, 10.0, 16.0, 14.0, 8.0, 15.0, 12.0, 15.0...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [-52.0, -64.0, -77.0, -62.0, -52.0, -65.0, -63...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [26.65, 26.65, 26.65, 26.65, 29.01, 29.01, 29....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.007684999999999998, 0.005123, 0.005123, 0.0...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [-46.27, -2.81, -81.67, -415.4, 442.23, 233.86...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [106.85, 106.75, 106.65, 106.55, 106.45, 106.3...   \n",
       "\n",
       "                                           Magnitude  \\\n",
       "0  [53.03772242470448, 66.06814663663572, 80.4611...   \n",
       "\n",
       "                                                 SID Subject_ID Activity  \\\n",
       "0  [17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17....         17        1   \n",
       "\n",
       "  Round                                              count  \n",
       "0     1  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create SID_list below so that we are able to use the Subject_ID values, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SID_list = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not able to feed arrays from a dataframe into our model. Therefore, we first select the columns of interest (those containing features), and then make each value in the array, a single row corresponding to other sensors at the same window timepoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['TEMP', 'EDA', 'HR', 'BVP', 'Magnitude', 'ACC1', 'ACC2', 'ACC3', 'SID', 'count']]\n",
    "X_test = test[['TEMP', 'EDA', 'HR', 'BVP', 'Magnitude', 'ACC1', 'ACC2', 'ACC3', 'SID', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440320, 10) (102400, 10) 542720\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(pd.Series.explode).reset_index().drop('index', axis =1)\n",
    "X_test = X_test.apply(pd.Series.explode).reset_index().drop('index', axis =1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train and y_test values are selected here as they will be our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11008,) (2560,) 13568\n"
     ]
    }
   ],
   "source": [
    "y_train = train['Activity'].values\n",
    "y_test = test['Activity'].values\n",
    "print(y_train.shape, y_test.shape, y_train.shape[0] + y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Subject_ID and Window Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID and window count, so that they may be used as variables in our model. Adding Subject_ID allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>HR</th>\n",
       "      <th>BVP</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>SID</th>\n",
       "      <th>count</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.65</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>106.85</td>\n",
       "      <td>-46.27</td>\n",
       "      <td>53.0377</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>-52</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.65</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>106.75</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>66.0681</td>\n",
       "      <td>-13</td>\n",
       "      <td>10</td>\n",
       "      <td>-64</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.65</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>106.65</td>\n",
       "      <td>-81.67</td>\n",
       "      <td>80.4612</td>\n",
       "      <td>-17</td>\n",
       "      <td>16</td>\n",
       "      <td>-77</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.65</td>\n",
       "      <td>0.029458</td>\n",
       "      <td>106.55</td>\n",
       "      <td>-415.4</td>\n",
       "      <td>63.8436</td>\n",
       "      <td>-6</td>\n",
       "      <td>14</td>\n",
       "      <td>-62</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.01</td>\n",
       "      <td>0.142168</td>\n",
       "      <td>106.45</td>\n",
       "      <td>442.23</td>\n",
       "      <td>53.2165</td>\n",
       "      <td>-8</td>\n",
       "      <td>8</td>\n",
       "      <td>-52</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440315</th>\n",
       "      <td>33.66</td>\n",
       "      <td>12.2743</td>\n",
       "      <td>82.855</td>\n",
       "      <td>10.66</td>\n",
       "      <td>61.7419</td>\n",
       "      <td>21.4717</td>\n",
       "      <td>55.9811</td>\n",
       "      <td>14.7358</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440316</th>\n",
       "      <td>33.68</td>\n",
       "      <td>12.2705</td>\n",
       "      <td>82.95</td>\n",
       "      <td>8.28</td>\n",
       "      <td>61.7299</td>\n",
       "      <td>22.1321</td>\n",
       "      <td>55.7547</td>\n",
       "      <td>14.566</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440317</th>\n",
       "      <td>33.68</td>\n",
       "      <td>12.2423</td>\n",
       "      <td>83.0375</td>\n",
       "      <td>6.56</td>\n",
       "      <td>61.7263</td>\n",
       "      <td>22.7925</td>\n",
       "      <td>55.5283</td>\n",
       "      <td>14.3962</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440318</th>\n",
       "      <td>33.68</td>\n",
       "      <td>12.3025</td>\n",
       "      <td>83.125</td>\n",
       "      <td>52.58</td>\n",
       "      <td>61.7311</td>\n",
       "      <td>23.4528</td>\n",
       "      <td>55.3019</td>\n",
       "      <td>14.2264</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440319</th>\n",
       "      <td>33.68</td>\n",
       "      <td>12.2743</td>\n",
       "      <td>83.2125</td>\n",
       "      <td>-7.57</td>\n",
       "      <td>61.7442</td>\n",
       "      <td>24.1132</td>\n",
       "      <td>55.0755</td>\n",
       "      <td>14.0566</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440320 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEMP       EDA       HR     BVP Magnitude     ACC1     ACC2     ACC3  \\\n",
       "0       26.65  0.007685   106.85  -46.27   53.0377       -3       10      -52   \n",
       "1       26.65  0.005123   106.75   -2.81   66.0681      -13       10      -64   \n",
       "2       26.65  0.005123   106.65  -81.67   80.4612      -17       16      -77   \n",
       "3       26.65  0.029458   106.55  -415.4   63.8436       -6       14      -62   \n",
       "4       29.01  0.142168   106.45  442.23   53.2165       -8        8      -52   \n",
       "...       ...       ...      ...     ...       ...      ...      ...      ...   \n",
       "440315  33.66   12.2743   82.855   10.66   61.7419  21.4717  55.9811  14.7358   \n",
       "440316  33.68   12.2705    82.95    8.28   61.7299  22.1321  55.7547   14.566   \n",
       "440317  33.68   12.2423  83.0375    6.56   61.7263  22.7925  55.5283  14.3962   \n",
       "440318  33.68   12.3025   83.125   52.58   61.7311  23.4528  55.3019  14.2264   \n",
       "440319  33.68   12.2743  83.2125   -7.57   61.7442  24.1132  55.0755  14.0566   \n",
       "\n",
       "       SID count  train  \n",
       "0       17     2      1  \n",
       "1       17     2      1  \n",
       "2       17     2      1  \n",
       "3       17     2      1  \n",
       "4       17     2      1  \n",
       "...     ..   ...    ...  \n",
       "440315  25    60      1  \n",
       "440316  25    60      1  \n",
       "440317  25    60      1  \n",
       "440318  25    60      1  \n",
       "440319  25    60      1  \n",
       "\n",
       "[440320 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined = pd.concat([combined, pd.get_dummies(combined['count'])], axis =1).drop('count', axis = 1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['SID'])], axis =1).drop('SID', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440320, 181) (102400, 181) 542720\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\"], axis = 1, inplace = True)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>HR</th>\n",
       "      <th>BVP</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>45.0</th>\n",
       "      <th>46.0</th>\n",
       "      <th>47.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>49.0</th>\n",
       "      <th>50.0</th>\n",
       "      <th>51.0</th>\n",
       "      <th>52.0</th>\n",
       "      <th>53.0</th>\n",
       "      <th>54.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.65</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>106.85</td>\n",
       "      <td>-46.27</td>\n",
       "      <td>53.0377</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>-52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TEMP       EDA      HR    BVP Magnitude ACC1 ACC2 ACC3  1  2  ...  45.0  \\\n",
       "0  26.65  0.007685  106.85 -46.27   53.0377   -3   10  -52  0  1  ...     0   \n",
       "\n",
       "   46.0  47.0  48.0  49.0  50.0  51.0  52.0  53.0  54.0  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[1 rows x 181 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set. The scaling only applied to sensor features and not to values which are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:8] = ss.fit_transform(X_train.iloc[:,:8])\n",
    "X_test.iloc[:,:8] = ss.transform(X_test.iloc[:,:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping windows as arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have our CNN model, accept the windows, they need to be reshaped correctly. Here we take the data from the dataframe and reshape it into sizes of (-1 (inferred from other dimensions), window_size (40), # of features (181)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to transposed arrays\n",
    "X_test = X_test.T.values\n",
    "X_train = X_train.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11008, 40, 181) (11008,) (2560, 40, 181) (2560,)\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.astype('float64')\n",
    "X_train = X_train.astype('float64')\n",
    "\n",
    "# Reshape to -1, window_size, # features\n",
    "X_train = X_train.reshape((-1, window_size, X_train.shape[0]))\n",
    "X_test = X_test.reshape((-1, window_size, X_test.shape[0])) \n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode y (activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed, Conv1D, AveragePooling1D, Conv2D, MaxPooling2D, MaxPool2D, Lambda, GlobalAveragePooling1D, Reshape, MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import tensorflow.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2044 - accuracy: 0.4368\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1735 - accuracy: 0.4531\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1602 - accuracy: 0.4586\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1557 - accuracy: 0.4602\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1504 - accuracy: 0.4622\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1462 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1433 - accuracy: 0.4628\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1414 - accuracy: 0.4637\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1369 - accuracy: 0.4665\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1337 - accuracy: 0.4691\n",
      "Score for fold 1: loss of 1.1491481065750122; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.2298 - accuracy: 0.4343\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1783 - accuracy: 0.4501\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1690 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1559 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1514 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1468 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1457 - accuracy: 0.4607\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1435 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1425 - accuracy: 0.4609\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1426 - accuracy: 0.4608\n",
      "Score for fold 2: loss of 1.151496410369873; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.2124 - accuracy: 0.4304\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1728 - accuracy: 0.4518\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1604 - accuracy: 0.4572\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1564 - accuracy: 0.4586\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1508 - accuracy: 0.4605\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 3s 8ms/step - loss: 1.1474 - accuracy: 0.4607\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1440 - accuracy: 0.4607\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1430 - accuracy: 0.4620\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1391 - accuracy: 0.4631\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1401 - accuracy: 0.4627\n",
      "Score for fold 3: loss of 1.1530581712722778; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.2160 - accuracy: 0.4328\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 3s 7ms/step - loss: 1.1702 - accuracy: 0.4500\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1606 - accuracy: 0.4495\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1543 - accuracy: 0.4581\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1501 - accuracy: 0.4612\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1466 - accuracy: 0.4615\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1408 - accuracy: 0.4633\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1405 - accuracy: 0.4632\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1352 - accuracy: 0.4676\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1320 - accuracy: 0.4688\n",
      "Score for fold 4: loss of 1.167190432548523; accuracy of 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2645 - accuracy: 0.4394\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1844 - accuracy: 0.4426\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1630 - accuracy: 0.4531\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1605 - accuracy: 0.4602\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1517 - accuracy: 0.4601\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1460 - accuracy: 0.4622\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1428 - accuracy: 0.4616\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1441 - accuracy: 0.4618\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1411 - accuracy: 0.4641\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1393 - accuracy: 0.4658\n",
      "Score for fold 5: loss of 1.1469871997833252; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2196 - accuracy: 0.4402\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1768 - accuracy: 0.4507\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1593 - accuracy: 0.4580\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1531 - accuracy: 0.4607\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1512 - accuracy: 0.4594\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1428 - accuracy: 0.4650\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1415 - accuracy: 0.4677\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1375 - accuracy: 0.4674\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1353 - accuracy: 0.4689\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1328 - accuracy: 0.4695\n",
      "Score for fold 6: loss of 1.145033836364746; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2228 - accuracy: 0.4401\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1778 - accuracy: 0.4468\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1651 - accuracy: 0.4584\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1550 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1519 - accuracy: 0.4615\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1480 - accuracy: 0.4628\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1464 - accuracy: 0.4638\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1427 - accuracy: 0.4631\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1406 - accuracy: 0.4680\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1378 - accuracy: 0.4690\n",
      "Score for fold 7: loss of 1.1610182523727417; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2274 - accuracy: 0.4275\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1781 - accuracy: 0.4418\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1614 - accuracy: 0.4572\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1542 - accuracy: 0.4620\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1518 - accuracy: 0.4627\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1450 - accuracy: 0.4650\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1421 - accuracy: 0.4660\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1399 - accuracy: 0.4639\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1395 - accuracy: 0.4666\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1350 - accuracy: 0.4695\n",
      "Score for fold 8: loss of 1.156912922859192; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2223 - accuracy: 0.4395\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1807 - accuracy: 0.4443\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1626 - accuracy: 0.4593\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1522 - accuracy: 0.4613\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1503 - accuracy: 0.4597\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1435 - accuracy: 0.4621\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1394 - accuracy: 0.4655\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1347 - accuracy: 0.4690\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1344 - accuracy: 0.4676\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1283 - accuracy: 0.4703\n",
      "Score for fold 9: loss of 1.1953377723693848; accuracy of 42.96875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2189 - accuracy: 0.4227\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1766 - accuracy: 0.4590\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1656 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1565 - accuracy: 0.4612\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1521 - accuracy: 0.4601\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1467 - accuracy: 0.4625\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1450 - accuracy: 0.4612\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.1432 - accuracy: 0.4653\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1428 - accuracy: 0.4632\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1386 - accuracy: 0.4663\n",
      "Score for fold 10: loss of 1.1673150062561035; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2612 - accuracy: 0.4332\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1859 - accuracy: 0.4381\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1670 - accuracy: 0.4552\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1532 - accuracy: 0.4621\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1502 - accuracy: 0.4601\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1449 - accuracy: 0.4615\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1434 - accuracy: 0.4648\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1410 - accuracy: 0.4637\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1390 - accuracy: 0.4643\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1383 - accuracy: 0.4684\n",
      "Score for fold 11: loss of 1.165282130241394; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2219 - accuracy: 0.4305\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1755 - accuracy: 0.4408\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1586 - accuracy: 0.4634\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1533 - accuracy: 0.4583\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1473 - accuracy: 0.4622\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1433 - accuracy: 0.4673\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1371 - accuracy: 0.4671\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1344 - accuracy: 0.4678\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1302 - accuracy: 0.4656\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1283 - accuracy: 0.4725\n",
      "Score for fold 12: loss of 1.1813502311706543; accuracy of 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2304 - accuracy: 0.4448\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1847 - accuracy: 0.4493\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1655 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1542 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1486 - accuracy: 0.4608\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1479 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1416 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1425 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1390 - accuracy: 0.4608\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1385 - accuracy: 0.4615\n",
      "Score for fold 13: loss of 1.152753233909607; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2165 - accuracy: 0.4386\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1790 - accuracy: 0.4498\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1645 - accuracy: 0.4581\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1566 - accuracy: 0.4610\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1490 - accuracy: 0.4605\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1457 - accuracy: 0.4624\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 3s 9ms/step - loss: 1.1441 - accuracy: 0.4623\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1409 - accuracy: 0.4631\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1381 - accuracy: 0.4641\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1373 - accuracy: 0.4635\n",
      "Score for fold 14: loss of 1.1705995798110962; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 2s 6ms/step - loss: 1.2036 - accuracy: 0.4287\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1724 - accuracy: 0.4573\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1613 - accuracy: 0.4611\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1549 - accuracy: 0.4614\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1489 - accuracy: 0.4622\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1464 - accuracy: 0.4637\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1435 - accuracy: 0.4640\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1394 - accuracy: 0.4671\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1380 - accuracy: 0.4665\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1328 - accuracy: 0.4671\n",
      "Score for fold 15: loss of 1.1491059064865112; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2198 - accuracy: 0.4549\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1805 - accuracy: 0.4609\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1650 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1546 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1498 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1490 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1456 - accuracy: 0.4609\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1431 - accuracy: 0.4608\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1402 - accuracy: 0.4599\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1400 - accuracy: 0.4608\n",
      "Score for fold 16: loss of 1.1528191566467285; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2317 - accuracy: 0.4347\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.1732 - accuracy: 0.4554\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1605 - accuracy: 0.4598\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1548 - accuracy: 0.4613\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1514 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1474 - accuracy: 0.4615\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1437 - accuracy: 0.4624\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1413 - accuracy: 0.4623\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1421 - accuracy: 0.4609\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1371 - accuracy: 0.4664\n",
      "Score for fold 17: loss of 1.163263201713562; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2091 - accuracy: 0.4347\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1730 - accuracy: 0.4539\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1626 - accuracy: 0.4611\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1550 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1516 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1439 - accuracy: 0.4621\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1406 - accuracy: 0.4628\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1393 - accuracy: 0.4649\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 3s 8ms/step - loss: 1.1364 - accuracy: 0.4653\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1315 - accuracy: 0.4687\n",
      "Score for fold 18: loss of 1.1724793910980225; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2339 - accuracy: 0.4356\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1832 - accuracy: 0.4541\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1646 - accuracy: 0.4613\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1574 - accuracy: 0.4604\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1509 - accuracy: 0.4602\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1471 - accuracy: 0.4604\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1444 - accuracy: 0.4618\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1432 - accuracy: 0.4613\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1413 - accuracy: 0.4604\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1391 - accuracy: 0.4611\n",
      "Score for fold 19: loss of 1.1458837985992432; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2177 - accuracy: 0.4412\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1778 - accuracy: 0.4610\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1616 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1540 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1521 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1465 - accuracy: 0.4604\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1435 - accuracy: 0.4613\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1428 - accuracy: 0.4613\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1411 - accuracy: 0.4633\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1395 - accuracy: 0.4624\n",
      "Score for fold 20: loss of 1.1461944580078125; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2462 - accuracy: 0.4321\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1821 - accuracy: 0.4398\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1657 - accuracy: 0.4594\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1580 - accuracy: 0.4613\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1517 - accuracy: 0.4626\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1460 - accuracy: 0.4621\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1448 - accuracy: 0.4636\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1415 - accuracy: 0.4652\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1430 - accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1403 - accuracy: 0.4665\n",
      "Score for fold 21: loss of 1.1428658962249756; accuracy of 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2505 - accuracy: 0.4162\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1869 - accuracy: 0.4530\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1706 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1588 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1528 - accuracy: 0.4614\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1477 - accuracy: 0.4614\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1449 - accuracy: 0.4607\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1442 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1422 - accuracy: 0.4628\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1418 - accuracy: 0.4604\n",
      "Score for fold 22: loss of 1.142596960067749; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.2260 - accuracy: 0.4222\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1752 - accuracy: 0.4509\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 1.1626 - accuracy: 0.4568\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1538 - accuracy: 0.4612\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1504 - accuracy: 0.4618\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1473 - accuracy: 0.4630\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1420 - accuracy: 0.4626\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1397 - accuracy: 0.4634\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1383 - accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 1.1381 - accuracy: 0.4662\n",
      "Score for fold 23: loss of 1.144789218902588; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2239 - accuracy: 0.4169\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 1.1739 - accuracy: 0.4552\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1640 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1586 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1495 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1463 - accuracy: 0.4611\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1446 - accuracy: 0.4621\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1424 - accuracy: 0.4626: 0s - loss:\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1376 - accuracy: 0.4633\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1349 - accuracy: 0.4659\n",
      "Score for fold 24: loss of 1.206791639328003; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2204 - accuracy: 0.4289\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1730 - accuracy: 0.4478\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1588 - accuracy: 0.4546\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1552 - accuracy: 0.4621\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1491 - accuracy: 0.4626\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1460 - accuracy: 0.4620\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1422 - accuracy: 0.4635\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1384 - accuracy: 0.4654\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1362 - accuracy: 0.4704\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1320 - accuracy: 0.4727\n",
      "Score for fold 25: loss of 1.162665843963623; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2154 - accuracy: 0.4361\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1766 - accuracy: 0.4488\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1609 - accuracy: 0.4607\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1555 - accuracy: 0.4601\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1490 - accuracy: 0.4618\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1467 - accuracy: 0.4599\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1452 - accuracy: 0.4613\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1416 - accuracy: 0.4611\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1408 - accuracy: 0.4623\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1366 - accuracy: 0.4647\n",
      "Score for fold 26: loss of 1.1579594612121582; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2390 - accuracy: 0.4261\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1794 - accuracy: 0.4483\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1646 - accuracy: 0.4597\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1566 - accuracy: 0.4613\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1518 - accuracy: 0.4612\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1465 - accuracy: 0.4622\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1424 - accuracy: 0.4617\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1396 - accuracy: 0.4617\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1363 - accuracy: 0.4625\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1358 - accuracy: 0.4639\n",
      "Score for fold 27: loss of 1.1498103141784668; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1953 - accuracy: 0.4440\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1688 - accuracy: 0.4522\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1609 - accuracy: 0.4578\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1537 - accuracy: 0.4586\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1492 - accuracy: 0.4613\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1473 - accuracy: 0.4641\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1418 - accuracy: 0.4650\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1413 - accuracy: 0.4658\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1353 - accuracy: 0.4647\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1366 - accuracy: 0.4640\n",
      "Score for fold 28: loss of 1.1785094738006592; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2364 - accuracy: 0.4284\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 7ms/step - loss: 1.1856 - accuracy: 0.4461\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1677 - accuracy: 0.4621\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1575 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1523 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1478 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1446 - accuracy: 0.4611\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1438 - accuracy: 0.4613\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1433 - accuracy: 0.4614\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1409 - accuracy: 0.4621\n",
      "Score for fold 29: loss of 1.1617958545684814; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2262 - accuracy: 0.4522\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1792 - accuracy: 0.4609\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1667 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 4s 11ms/step - loss: 1.1565 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1528 - accuracy: 0.4601\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1479 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1450 - accuracy: 0.4618\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1426 - accuracy: 0.4610\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1422 - accuracy: 0.4627\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1401 - accuracy: 0.4621\n",
      "Score for fold 30: loss of 1.1678962707519531; accuracy of 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2444 - accuracy: 0.4457\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1832 - accuracy: 0.4609\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1637 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1579 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1517 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1495 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1467 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1454 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1434 - accuracy: 0.4610\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1434 - accuracy: 0.4608\n",
      "Score for fold 31: loss of 1.1472129821777344; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2348 - accuracy: 0.4273\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1856 - accuracy: 0.4534\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1669 - accuracy: 0.4609\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1565 - accuracy: 0.4608\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1482 - accuracy: 0.4608\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1468 - accuracy: 0.4615\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1452 - accuracy: 0.4616\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1419 - accuracy: 0.4625\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1426 - accuracy: 0.4631\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1402 - accuracy: 0.4630\n",
      "Score for fold 32: loss of 1.148178219795227; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2657 - accuracy: 0.4294\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1915 - accuracy: 0.4448\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1663 - accuracy: 0.4602\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1582 - accuracy: 0.4610\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1496 - accuracy: 0.4616\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1473 - accuracy: 0.4597\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1463 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1433 - accuracy: 0.4618\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1414 - accuracy: 0.4626\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1421 - accuracy: 0.4618\n",
      "Score for fold 33: loss of 1.1432980298995972; accuracy of 48.828125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2183 - accuracy: 0.4337\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1752 - accuracy: 0.4525\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1616 - accuracy: 0.4585\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1509 - accuracy: 0.4599\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1495 - accuracy: 0.4619\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1452 - accuracy: 0.4632\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1425 - accuracy: 0.4625\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1401 - accuracy: 0.4644\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1365 - accuracy: 0.4673\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1337 - accuracy: 0.4662\n",
      "Score for fold 34: loss of 1.1611981391906738; accuracy of 46.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2138 - accuracy: 0.4469\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1762 - accuracy: 0.4597\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1646 - accuracy: 0.4608\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1533 - accuracy: 0.4611\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1541 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1465 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1441 - accuracy: 0.4600\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1405 - accuracy: 0.4615\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1395 - accuracy: 0.4616\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1365 - accuracy: 0.4634\n",
      "Score for fold 35: loss of 1.1549524068832397; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2508 - accuracy: 0.4292\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1850 - accuracy: 0.4347: 0s - loss:\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1656 - accuracy: 0.4491\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1532 - accuracy: 0.4618\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 4s 12ms/step - loss: 1.1479 - accuracy: 0.4661\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1447 - accuracy: 0.4643\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1439 - accuracy: 0.4628\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1413 - accuracy: 0.4682\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1375 - accuracy: 0.4657\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1351 - accuracy: 0.4677\n",
      "Score for fold 36: loss of 1.1661086082458496; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.2194 - accuracy: 0.4558\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1719 - accuracy: 0.4484\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1625 - accuracy: 0.4575\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1591 - accuracy: 0.4577\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1511 - accuracy: 0.4612\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1474 - accuracy: 0.4607\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1450 - accuracy: 0.4623\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1459 - accuracy: 0.4628\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1400 - accuracy: 0.4626\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1383 - accuracy: 0.4637\n",
      "Score for fold 37: loss of 1.1432725191116333; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.2078 - accuracy: 0.4395\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.1683 - accuracy: 0.4462\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1585 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1559 - accuracy: 0.4611: 0s - loss: 1.1544 - ac\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1495 - accuracy: 0.4644\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1459 - accuracy: 0.4651\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1418 - accuracy: 0.4670\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 4ms/step - loss: 1.1383 - accuracy: 0.4666\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1346 - accuracy: 0.4693\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 2s 4ms/step - loss: 1.1325 - accuracy: 0.4684\n",
      "Score for fold 38: loss of 1.208611011505127; accuracy of 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.2047 - accuracy: 0.4378\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1722 - accuracy: 0.4478\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1606 - accuracy: 0.4535\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1534 - accuracy: 0.4606\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1461 - accuracy: 0.4629\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1444 - accuracy: 0.4652\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1415 - accuracy: 0.4644\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1382 - accuracy: 0.4675\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1362 - accuracy: 0.4644\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1310 - accuracy: 0.4699\n",
      "Score for fold 39: loss of 1.1994680166244507; accuracy of 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2364 - accuracy: 0.4332\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1781 - accuracy: 0.4507\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 6ms/step - loss: 1.1636 - accuracy: 0.4543\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1545 - accuracy: 0.4599\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1525 - accuracy: 0.4607\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1474 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1460 - accuracy: 0.4612\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1423 - accuracy: 0.4611\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1362 - accuracy: 0.4627\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1374 - accuracy: 0.4613\n",
      "Score for fold 40: loss of 1.1505465507507324; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2350 - accuracy: 0.4288\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1804 - accuracy: 0.4512\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1670 - accuracy: 0.4576\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1535 - accuracy: 0.4629\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1506 - accuracy: 0.4617\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1464 - accuracy: 0.4620\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1441 - accuracy: 0.4627\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1422 - accuracy: 0.4623\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1386 - accuracy: 0.4630\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1396 - accuracy: 0.4671\n",
      "Score for fold 41: loss of 1.1701780557632446; accuracy of 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2066 - accuracy: 0.4458\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1706 - accuracy: 0.4584\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1626 - accuracy: 0.4596\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 4ms/step - loss: 1.1581 - accuracy: 0.4623\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1516 - accuracy: 0.4622\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1460 - accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1438 - accuracy: 0.4628\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1422 - accuracy: 0.4634\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1407 - accuracy: 0.4635\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1355 - accuracy: 0.4659\n",
      "Score for fold 42: loss of 1.1670944690704346; accuracy of 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.2468 - accuracy: 0.4322\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1867 - accuracy: 0.4282\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1649 - accuracy: 0.4538\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1557 - accuracy: 0.4609\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1502 - accuracy: 0.4610\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 3s 10ms/step - loss: 1.1464 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1432 - accuracy: 0.4610\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1443 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1430 - accuracy: 0.4612\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1416 - accuracy: 0.4609\n",
      "Score for fold 43: loss of 1.1592127084732056; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.2340 - accuracy: 0.4343\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1834 - accuracy: 0.4490\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1672 - accuracy: 0.4610\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1566 - accuracy: 0.4613\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 2s 5ms/step - loss: 1.1505 - accuracy: 0.4609\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1471 - accuracy: 0.4609\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1443 - accuracy: 0.4614\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1419 - accuracy: 0.4612\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1412 - accuracy: 0.4619\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 1.1413 - accuracy: 0.4611\n",
      "Score for fold 44: loss of 1.154476523399353; accuracy of 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1988 - accuracy: 0.4460\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1719 - accuracy: 0.4541\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1622 - accuracy: 0.4605\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1531 - accuracy: 0.4616\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1505 - accuracy: 0.4616\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1454 - accuracy: 0.4614\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1438 - accuracy: 0.4600\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1397 - accuracy: 0.4628\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 1.1370 - accuracy: 0.4619\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.1364 - accuracy: 0.4631\n",
      "Score for fold 45: loss of 1.1829683780670166; accuracy of 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.1491481065750122 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.151496410369873 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.1530581712722778 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.167190432548523 - Accuracy: 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.1469871997833252 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.145033836364746 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.1610182523727417 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.156912922859192 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.1953377723693848 - Accuracy: 42.96875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.1673150062561035 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 1.165282130241394 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.1813502311706543 - Accuracy: 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 1.152753233909607 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 1.1705995798110962 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 1.1491059064865112 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 1.1528191566467285 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 1.163263201713562 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 1.1724793910980225 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 1.1458837985992432 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 1.1461944580078125 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 1.1428658962249756 - Accuracy: 47.265625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 1.142596960067749 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.144789218902588 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 1.206791639328003 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 1.162665843963623 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 1.1579594612121582 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 1.1498103141784668 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 1.1785094738006592 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.1617958545684814 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 1.1678962707519531 - Accuracy: 45.703125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 1.1472129821777344 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 1.148178219795227 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 1.1432980298995972 - Accuracy: 48.828125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.1611981391906738 - Accuracy: 46.875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 1.1549524068832397 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 1.1661086082458496 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 1.1432725191116333 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.208611011505127 - Accuracy: 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.1994680166244507 - Accuracy: 44.921875%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 1.1505465507507324 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 1.1701780557632446 - Accuracy: 45.3125%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 1.1670944690704346 - Accuracy: 46.484375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 1.1592127084732056 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.154476523399353 - Accuracy: 46.09375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 1.1829683780670166 - Accuracy: 44.53125%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 45.94618055555556 (+- 0.8994329060408762)\n",
      "> Loss: 1.161459705564711\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = SID_list\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    #model.add(Lambda(lambda v: tf.cast(tf.signal.fft(tf.cast(v,dtype=tf.complex64)),tf.float32)))\n",
    "    #model.add(Conv1D(160, 2, activation='relu'))\n",
    "    #model.add(Conv1D(160, 3, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/N1/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/10_TF_CNN/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/10_TF_CNN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 1.1500 - accuracy: 0.4656\n",
      "Test loss, Test acc: [1.150010585784912, 0.46562498807907104]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1171,  919,  216,  217],\n",
       "       [   9,   21,    4,    3],\n",
       "       [   0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred.astype(int), y_test.astype(int))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm/cm.astype(np.float).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data CNN - Confusion Matrix')\n",
    "plt.savefig('Duke_Data_CNN_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.466 \n",
      "F1 Score: 0.624\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_pred.astype(int), y_test.astype(int))\n",
    "f1_s = f1_score(y_pred.astype(int), y_test.astype(int), average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

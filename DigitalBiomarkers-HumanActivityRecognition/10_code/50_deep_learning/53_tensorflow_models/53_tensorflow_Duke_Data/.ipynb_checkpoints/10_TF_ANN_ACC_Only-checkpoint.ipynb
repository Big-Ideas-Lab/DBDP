{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 1: Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **rolled_data.csv**\n",
    "\n",
    "This notebook label encodes Subject_ID and Activity (our y variable). It then one-hot encodes Subject_ID to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('rolled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.reindex(columns = ['ACC1', 'ACC2', 'ACC3', 'TEMP', 'EDA', 'BVP', 'HR', 'Magnitude', 'Subject_ID', 'Round', 'Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.22500</td>\n",
       "      <td>28.29750</td>\n",
       "      <td>39.225000</td>\n",
       "      <td>32.35600</td>\n",
       "      <td>0.265940</td>\n",
       "      <td>-0.21875</td>\n",
       "      <td>76.10525</td>\n",
       "      <td>62.913102</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.17500</td>\n",
       "      <td>28.34250</td>\n",
       "      <td>39.175000</td>\n",
       "      <td>32.35475</td>\n",
       "      <td>0.265556</td>\n",
       "      <td>-1.07075</td>\n",
       "      <td>75.96875</td>\n",
       "      <td>62.870169</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.12663</td>\n",
       "      <td>28.38337</td>\n",
       "      <td>39.125543</td>\n",
       "      <td>32.35350</td>\n",
       "      <td>0.265140</td>\n",
       "      <td>-0.75950</td>\n",
       "      <td>75.83375</td>\n",
       "      <td>62.826763</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACC1      ACC2       ACC3      TEMP       EDA      BVP        HR  \\\n",
       "0  40.22500  28.29750  39.225000  32.35600  0.265940 -0.21875  76.10525   \n",
       "1  40.17500  28.34250  39.175000  32.35475  0.265556 -1.07075  75.96875   \n",
       "2  40.12663  28.38337  39.125543  32.35350  0.265140 -0.75950  75.83375   \n",
       "\n",
       "   Magnitude Subject_ID  Round  Activity  \n",
       "0  62.913102     19-001      1  Baseline  \n",
       "1  62.870169     19-001      1  Baseline  \n",
       "2  62.826763     19-001      1  Baseline  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Activity'] = le.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "df['Subject_ID'] = le1.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train sets (by Subject ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = list(df['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 11) (47196, 11)\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Subject_ID'].isin(ID_list[:45])]\n",
    "test = df[df['Subject_ID'].isin(ID_list[45:])]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the test train split can be changed by changing the index below. For our purposes, n = 45 for train and n = 10 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 9) (216108,) (47196, 9) (47196,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:,0:9]\n",
    "X_test = test.iloc[:,0:9]\n",
    "\n",
    "y_train = train.iloc[:,-1].values\n",
    "y_test = test.iloc[:,-1].values\n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create X_train_df below so that we are able to use the Subject_ID column later on, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID, so that it may be used as a variable in our model. This allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0\n",
    "\n",
    "combined = pd.concat([X_train, X_test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 63) (47196, 63) 263304\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:8] = ss.fit_transform(X_train.iloc[:,:8])\n",
    "X_test.iloc[:,:8] = ss.transform(X_test.iloc[:,:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.4982 - accuracy: 0.8039\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3418 - accuracy: 0.8678\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3103 - accuracy: 0.8790: 0s - loss: 0.3104 - accuracy: 0.\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2966 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2879 - accuracy: 0.8880\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2820 - accuracy: 0.8901\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2771 - accuracy: 0.8924\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2739 - accuracy: 0.8933\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2688 - accuracy: 0.8957\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2665 - accuracy: 0.8959\n",
      "Score for fold 1: loss of 0.9486937057700712; accuracy of 68.33735909822866%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.4716 - accuracy: 0.8142\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.3298 - accuracy: 0.8713\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.88 - 33s 5ms/step - loss: 0.3016 - accuracy: 0.8829\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 48s 7ms/step - loss: 0.2854 - accuracy: 0.8884\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 37s 6ms/step - loss: 0.2733 - accuracy: 0.8923\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2671 - accuracy: 0.8952 0s - loss: 0.2675 \n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2629 - accuracy: 0.8969\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2599 - accuracy: 0.8975\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2573 - accuracy: 0.8985: \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2530 - accuracy: 0.9002\n",
      "Score for fold 2: loss of 2.8126126849145034; accuracy of 36.73510466988728%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4887 - accuracy: 0.8072\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3394 - accuracy: 0.8689\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3104 - accuracy: 0.8795\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2952 - accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2881 - accuracy: 0.8875\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2810 - accuracy: 0.8903\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2760 - accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2719 - accuracy: 0.8945\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2687 - accuracy: 0.8958\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2654 - accuracy: 0.8969\n",
      "Score for fold 3: loss of 1.223574114410627; accuracy of 79.46859903381642%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.4755 - accuracy: 0.8157\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3279 - accuracy: 0.8749\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3015 - accuracy: 0.8826\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2897 - accuracy: 0.8872 0s - loss: 0.2896 - accu - ETA: 0s - loss: 0.2892 \n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2797 - accuracy: 0.8901\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2756 - accuracy: 0.8909\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2703 - accuracy: 0.8924\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2667 - accuracy: 0.8937\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2652 - accuracy: 0.8946\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2617 - accuracy: 0.8961\n",
      "Score for fold 4: loss of 2.5671860547751755; accuracy of 62.5402576489533%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.4841 - accuracy: 0.8102\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3318 - accuracy: 0.8715\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3009 - accuracy: 0.8820\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2890 - accuracy: 0.8869\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2801 - accuracy: 0.8908 1s - loss: 0.2\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2772 - accuracy: 0.8909\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2691 - accuracy: 0.8943\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2681 - accuracy: 0.8949\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2644 - accuracy: 0.8964\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2623 - accuracy: 0.8973\n",
      "Score for fold 5: loss of 2.4223317456118245; accuracy of 38.00322061191627%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4944 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3445 - accuracy: 0.8688\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3122 - accuracy: 0.8805\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2963 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2876 - accuracy: 0.8878\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2802 - accuracy: 0.8911\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2748 - accuracy: 0.8935\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2712 - accuracy: 0.8953\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2675 - accuracy: 0.8963\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2646 - accuracy: 0.8974: 0s - loss: 0.2648 - accuracy: \n",
      "Score for fold 6: loss of 0.7415656224832831; accuracy of 83.15217391304348%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 974us/step - loss: 0.4925 - accuracy: 0.8080\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3511 - accuracy: 0.8674\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3208 - accuracy: 0.8783\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3035 - accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2964 - accuracy: 0.8868\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2891 - accuracy: 0.8889\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2813 - accuracy: 0.8918\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2766 - accuracy: 0.8941\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2727 - accuracy: 0.8948\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 961us/step - loss: 0.2700 - accuracy: 0.8956\n",
      "Score for fold 7: loss of 0.9901504874296062; accuracy of 79.02576489533011%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4771 - accuracy: 0.8159\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3305 - accuracy: 0.8753 \n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3038 - accuracy: 0.8837 0s - loss: 0.3038 - accuracy: 0.\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2880 - accuracy: 0.8899\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2798 - accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 958us/step - loss: 0.2717 - accuracy: 0.8971\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2679 - accuracy: 0.8987\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2638 - accuracy: 0.9004\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2594 - accuracy: 0.9014\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2566 - accuracy: 0.9029\n",
      "Score for fold 8: loss of 1.123469437011395; accuracy of 48.188405797101446%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4844 - accuracy: 0.8087\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 976us/step - loss: 0.3376 - accuracy: 0.8669\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3093 - accuracy: 0.8767\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2941 - accuracy: 0.8832\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2843 - accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2798 - accuracy: 0.8890\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2758 - accuracy: 0.8902\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 977us/step - loss: 0.2718 - accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2698 - accuracy: 0.8927\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2666 - accuracy: 0.8934\n",
      "Score for fold 9: loss of 0.6892428142569761; accuracy of 82.26650563607085%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4700 - accuracy: 0.8168: 0s - loss: 0.4759 - \n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3270 - accuracy: 0.8746\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2984 - accuracy: 0.8842: 0s\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2867 - accuracy: 0.8874\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2785 - accuracy: 0.8907\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2713 - accuracy: 0.8929\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2671 - accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2631 - accuracy: 0.8952\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2599 - accuracy: 0.8974\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2569 - accuracy: 0.8985\n",
      "Score for fold 10: loss of 1.5078004893381152; accuracy of 67.31078904991948%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.4784 - accuracy: 0.8133\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3318 - accuracy: 0.8719\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3039 - accuracy: 0.8808\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2886 - accuracy: 0.8865\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2819 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2750 - accuracy: 0.8917\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2702 - accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2673 - accuracy: 0.8941\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2617 - accuracy: 0.8971\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2578 - accuracy: 0.8970\n",
      "Score for fold 11: loss of 3.4543331225122205; accuracy of 40.21739130434783%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.4895 - accuracy: 0.8084\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3318 - accuracy: 0.8715\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2988 - accuracy: 0.8841\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2835 - accuracy: 0.8893\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2734 - accuracy: 0.8927\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2680 - accuracy: 0.8944 0s - los\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2634 - accuracy: 0.8961\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2596 - accuracy: 0.8976\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2577 - accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2529 - accuracy: 0.8993\n",
      "Score for fold 12: loss of 1.0880349998680694; accuracy of 74.09420289855072%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4827 - accuracy: 0.8111\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3302 - accuracy: 0.8722\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3004 - accuracy: 0.8821\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2855 - accuracy: 0.8865\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2775 - accuracy: 0.8897\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2706 - accuracy: 0.8922\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 962us/step - loss: 0.2663 - accuracy: 0.8944\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2624 - accuracy: 0.8956: 0s\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2579 - accuracy: 0.8972\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2543 - accuracy: 0.8991\n",
      "Score for fold 13: loss of 1.008694793591034; accuracy of 74.37600644122384%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4807 - accuracy: 0.8123\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3373 - accuracy: 0.8693\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3099 - accuracy: 0.8787\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 986us/step - loss: 0.2975 - accuracy: 0.8821\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 994us/step - loss: 0.2865 - accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2796 - accuracy: 0.8893\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2765 - accuracy: 0.8907\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2708 - accuracy: 0.8928\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2683 - accuracy: 0.8941\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2624 - accuracy: 0.8963\n",
      "Score for fold 14: loss of 2.7648294968374043; accuracy of 36.312399355877616%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4920 - accuracy: 0.8044\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 993us/step - loss: 0.3360 - accuracy: 0.8677\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3082 - accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2928 - accuracy: 0.8845: 0s - loss: 0.2930 - accuracy: 0.\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 988us/step - loss: 0.2850 - accuracy: 0.8869\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2818 - accuracy: 0.8884\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2750 - accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2725 - accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2683 - accuracy: 0.8945\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2635 - accuracy: 0.8961\n",
      "Score for fold 15: loss of 1.199640593150884; accuracy of 63.54669887278583%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4915 - accuracy: 0.8082\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3444 - accuracy: 0.8658\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 993us/step - loss: 0.3123 - accuracy: 0.8773\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2973 - accuracy: 0.8828\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2883 - accuracy: 0.8865\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2795 - accuracy: 0.8901\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2747 - accuracy: 0.8923\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2699 - accuracy: 0.8939\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2680 - accuracy: 0.8946\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2642 - accuracy: 0.8964: 0s - loss: 0.2643 - accuracy\n",
      "Score for fold 16: loss of 6.7420588009008995; accuracy of 46.39694041867955%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4937 - accuracy: 0.8074\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3401 - accuracy: 0.8692\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 985us/step - loss: 0.3131 - accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2999 - accuracy: 0.8810\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2904 - accuracy: 0.8849\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2830 - accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2791 - accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2765 - accuracy: 0.8899\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2728 - accuracy: 0.8912\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2698 - accuracy: 0.8924\n",
      "Score for fold 17: loss of 0.46069123823630004; accuracy of 62.86231884057971%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.4870 - accuracy: 0.8105\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.3421 - accuracy: 0.8687\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.3110 - accuracy: 0.8802\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2962 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2857 - accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2788 - accuracy: 0.8918\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2741 - accuracy: 0.8929\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2695 - accuracy: 0.8948\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 10s 2ms/step - loss: 0.2665 - accuracy: 0.8959 2s - loss: 0.2651 \n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.2635 - accuracy: 0.8967\n",
      "Score for fold 18: loss of 2.0289119804538966; accuracy of 26.570048309178745%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.4793 - accuracy: 0.8153\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3372 - accuracy: 0.8727\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3073 - accuracy: 0.8822\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2945 - accuracy: 0.8861\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2868 - accuracy: 0.8888\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2789 - accuracy: 0.8914\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2727 - accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2681 - accuracy: 0.8948\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2646 - accuracy: 0.8963\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 997us/step - loss: 0.2601 - accuracy: 0.8979\n",
      "Score for fold 19: loss of 1.0662668162301459; accuracy of 77.13365539452496%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.5075 - accuracy: 0.8001\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3489 - accuracy: 0.8665\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3205 - accuracy: 0.8766\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3024 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2896 - accuracy: 0.8887\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 989us/step - loss: 0.2857 - accuracy: 0.8894\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2796 - accuracy: 0.8926\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 981us/step - loss: 0.2768 - accuracy: 0.8920\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 977us/step - loss: 0.2740 - accuracy: 0.8932\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 995us/step - loss: 0.2717 - accuracy: 0.8930\n",
      "Score for fold 20: loss of 0.5879754875186365; accuracy of 79.66988727858293%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4947 - accuracy: 0.8064\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3467 - accuracy: 0.8683\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3166 - accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3034 - accuracy: 0.8831\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2911 - accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2826 - accuracy: 0.8914: 0s - loss: 0.2824 \n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2784 - accuracy: 0.8929\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 981us/step - loss: 0.2734 - accuracy: 0.8946\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 974us/step - loss: 0.2694 - accuracy: 0.8965\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 944us/step - loss: 0.2676 - accuracy: 0.8967\n",
      "Score for fold 21: loss of 0.9767616586144804; accuracy of 61.55394524959742%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 949us/step - loss: 0.5065 - accuracy: 0.8014\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3427 - accuracy: 0.8680\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 963us/step - loss: 0.3093 - accuracy: 0.8797\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2940 - accuracy: 0.8854\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 938us/step - loss: 0.2851 - accuracy: 0.8890\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 964us/step - loss: 0.2787 - accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 953us/step - loss: 0.2736 - accuracy: 0.8932\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 966us/step - loss: 0.2711 - accuracy: 0.8941\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 996us/step - loss: 0.2681 - accuracy: 0.8954\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 987us/step - loss: 0.2624 - accuracy: 0.8971\n",
      "Score for fold 22: loss of 0.6046836729617634; accuracy of 81.56199677938808%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4925 - accuracy: 0.8091\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3342 - accuracy: 0.8713\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3061 - accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2890 - accuracy: 0.8875\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2827 - accuracy: 0.8893\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2761 - accuracy: 0.8922\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2731 - accuracy: 0.8931\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2674 - accuracy: 0.8955\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2648 - accuracy: 0.8967\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2630 - accuracy: 0.8971\n",
      "Score for fold 23: loss of 0.9593763125065818; accuracy of 75.50322061191626%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4985 - accuracy: 0.8064\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3436 - accuracy: 0.8684\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3104 - accuracy: 0.8803\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2960 - accuracy: 0.8855\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2876 - accuracy: 0.8880\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2795 - accuracy: 0.8915\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2741 - accuracy: 0.8927\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2706 - accuracy: 0.8940\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2664 - accuracy: 0.8963\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2640 - accuracy: 0.8960\n",
      "Score for fold 24: loss of 0.36938894448433174; accuracy of 90.37842190016103%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4993 - accuracy: 0.8042\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3438 - accuracy: 0.8657\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3123 - accuracy: 0.8757\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2962 - accuracy: 0.8827\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2869 - accuracy: 0.8856\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2803 - accuracy: 0.8875\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2759 - accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2716 - accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2707 - accuracy: 0.8922\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2665 - accuracy: 0.8934\n",
      "Score for fold 25: loss of 0.2471855504882144; accuracy of 90.68035426731079%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.5029 - accuracy: 0.8029\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3489 - accuracy: 0.8655\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3191 - accuracy: 0.8766\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3046 - accuracy: 0.8822\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2940 - accuracy: 0.8857\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2866 - accuracy: 0.8882\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2824 - accuracy: 0.8899\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2774 - accuracy: 0.8925\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2744 - accuracy: 0.8936\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2702 - accuracy: 0.8945\n",
      "Score for fold 26: loss of 2.528577310118853; accuracy of 55.65619967793881%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4962 - accuracy: 0.8048\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3365 - accuracy: 0.8704\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3092 - accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2941 - accuracy: 0.8859\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2879 - accuracy: 0.8873\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2818 - accuracy: 0.8893\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2765 - accuracy: 0.8914\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2727 - accuracy: 0.8924\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2687 - accuracy: 0.8939\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2679 - accuracy: 0.8939\n",
      "Score for fold 27: loss of 0.7126033262298778; accuracy of 83.43397745571659%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4765 - accuracy: 0.8151\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3272 - accuracy: 0.8739\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3013 - accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2873 - accuracy: 0.8870\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2807 - accuracy: 0.8883\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2764 - accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2711 - accuracy: 0.8920\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2660 - accuracy: 0.8941\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2614 - accuracy: 0.8962\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2580 - accuracy: 0.8968\n",
      "Score for fold 28: loss of 2.4090518139268235; accuracy of 28.07971014492754%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4942 - accuracy: 0.8079\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3395 - accuracy: 0.8709\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3109 - accuracy: 0.8818\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2945 - accuracy: 0.8876\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2837 - accuracy: 0.8914\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2768 - accuracy: 0.8938\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2731 - accuracy: 0.8952\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2675 - accuracy: 0.8984\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2669 - accuracy: 0.8982\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2638 - accuracy: 0.8996\n",
      "Score for fold 29: loss of 0.8925345347364615; accuracy of 88.62721417069244%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 993us/step - loss: 0.4821 - accuracy: 0.8127\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 991us/step - loss: 0.3375 - accuracy: 0.8717\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3078 - accuracy: 0.8811: 0s\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2964 - accuracy: 0.8850: 0s - loss: 0.2965 - accuracy\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2877 - accuracy: 0.8875\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2816 - accuracy: 0.8898\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2783 - accuracy: 0.8915: 0s - loss: 0\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2743 - accuracy: 0.8938\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2716 - accuracy: 0.8939\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2686 - accuracy: 0.8946\n",
      "Score for fold 30: loss of 2.316445784048027; accuracy of 67.27053140096618%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4963 - accuracy: 0.8101\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3434 - accuracy: 0.8693\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3153 - accuracy: 0.8782\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3034 - accuracy: 0.8820\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2926 - accuracy: 0.8862\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 988us/step - loss: 0.2844 - accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 988us/step - loss: 0.2813 - accuracy: 0.89070s - l\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2765 - accuracy: 0.8918\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2736 - accuracy: 0.8935\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2707 - accuracy: 0.8944\n",
      "Score for fold 31: loss of 0.2807664558751285; accuracy of 83.11191626409018%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4938 - accuracy: 0.8068: 0s - loss: 0.4998 - \n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3388 - accuracy: 0.8718\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3105 - accuracy: 0.8802\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2953 - accuracy: 0.8855\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2866 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2811 - accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2757 - accuracy: 0.8915\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2703 - accuracy: 0.8944: 0s - loss: 0.2706 - ac\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2662 - accuracy: 0.8960\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2620 - accuracy: 0.8973\n",
      "Score for fold 32: loss of 1.9895585029789449; accuracy of 39.65378421900161%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5059 - accuracy: 0.8033\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3460 - accuracy: 0.8655\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3162 - accuracy: 0.8759\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2980 - accuracy: 0.8834\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2870 - accuracy: 0.8880\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2782 - accuracy: 0.8905\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2727 - accuracy: 0.8930\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2680 - accuracy: 0.8946\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2640 - accuracy: 0.8957\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2624 - accuracy: 0.8972\n",
      "Score for fold 33: loss of 0.3655247457463539; accuracy of 86.37278582930756%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4920 - accuracy: 0.8080\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3369 - accuracy: 0.8722\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3085 - accuracy: 0.8801\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2932 - accuracy: 0.8851\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2822 - accuracy: 0.8891\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2776 - accuracy: 0.8911\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2706 - accuracy: 0.8942\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2678 - accuracy: 0.8951: 0s - l\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2627 - accuracy: 0.8971\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2587 - accuracy: 0.8986\n",
      "Score for fold 34: loss of 1.2721465506800909; accuracy of 61.83574879227053%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4999 - accuracy: 0.8041\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3441 - accuracy: 0.8671\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 996us/step - loss: 0.3108 - accuracy: 0.8796\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2945 - accuracy: 0.8847\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2854 - accuracy: 0.8876: 0s - loss: 0.2857 - accuracy: \n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2771 - accuracy: 0.8911\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2739 - accuracy: 0.8918\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2687 - accuracy: 0.8938\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2668 - accuracy: 0.8936\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2633 - accuracy: 0.8961\n",
      "Score for fold 35: loss of 0.8891492590126563; accuracy of 76.40901771336553%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 969us/step - loss: 0.4942 - accuracy: 0.8066\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 986us/step - loss: 0.3446 - accuracy: 0.8684\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3130 - accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2990 - accuracy: 0.8845\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2891 - accuracy: 0.8873\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2818 - accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2775 - accuracy: 0.8921\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2719 - accuracy: 0.8941\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2675 - accuracy: 0.8956\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2635 - accuracy: 0.8970\n",
      "Score for fold 36: loss of 0.6163108474672516; accuracy of 72.30273752012883%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.4810 - accuracy: 0.8142: 0s - l\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3391 - accuracy: 0.8708\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 10s 1ms/step - loss: 0.3100 - accuracy: 0.8814\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 10s 1ms/step - loss: 0.2951 - accuracy: 0.8874\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2874 - accuracy: 0.8898\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 10s 1ms/step - loss: 0.2804 - accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2740 - accuracy: 0.8947\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.2719 - accuracy: 0.8954\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2683 - accuracy: 0.8972\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2663 - accuracy: 0.8975: 0s - loss: 0.2661 - accu\n",
      "Score for fold 37: loss of 0.45902508303368633; accuracy of 88.76811594202898%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.4965 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3421 - accuracy: 0.8681\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3117 - accuracy: 0.8793\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2967 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2886 - accuracy: 0.8886\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2825 - accuracy: 0.8911\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2781 - accuracy: 0.8929\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2725 - accuracy: 0.8946\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2679 - accuracy: 0.8967\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2667 - accuracy: 0.8974\n",
      "Score for fold 38: loss of 0.393496482641954; accuracy of 85.32608695652173%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4868 - accuracy: 0.8098\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 975us/step - loss: 0.3301 - accuracy: 0.8725\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2996 - accuracy: 0.8834\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2853 - accuracy: 0.8885\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2753 - accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2679 - accuracy: 0.8955: 0s - loss: 0.267\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2640 - accuracy: 0.8963\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2581 - accuracy: 0.8980\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2560 - accuracy: 0.8990\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2556 - accuracy: 0.8993\n",
      "Score for fold 39: loss of 1.2729863016919223; accuracy of 66.74718196457327%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4802 - accuracy: 0.8149\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3306 - accuracy: 0.8742\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3008 - accuracy: 0.8833\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2861 - accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 985us/step - loss: 0.2788 - accuracy: 0.8913\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2733 - accuracy: 0.8933\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2669 - accuracy: 0.8947\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2652 - accuracy: 0.8970: 0s - loss: 0.2656 - accura\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2620 - accuracy: 0.8976\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2594 - accuracy: 0.8982\n",
      "Score for fold 40: loss of 2.857881701444898; accuracy of 10.929951690821255%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4908 - accuracy: 0.8105\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3372 - accuracy: 0.8717\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3105 - accuracy: 0.8809\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2978 - accuracy: 0.8859\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2885 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2806 - accuracy: 0.8922\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2767 - accuracy: 0.8944\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2715 - accuracy: 0.8955\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2670 - accuracy: 0.8973\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2650 - accuracy: 0.8978\n",
      "Score for fold 41: loss of 0.731502108056932; accuracy of 81.17954911433173%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4816 - accuracy: 0.8115\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3335 - accuracy: 0.8728\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3088 - accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2946 - accuracy: 0.8855\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2844 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2784 - accuracy: 0.8916\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2727 - accuracy: 0.8930\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2689 - accuracy: 0.8949\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2655 - accuracy: 0.8962\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2638 - accuracy: 0.8970\n",
      "Score for fold 42: loss of 0.8402365528357526; accuracy of 73.26892109500805%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4822 - accuracy: 0.8161\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3385 - accuracy: 0.8719\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 995us/step - loss: 0.3088 - accuracy: 0.8813\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2946 - accuracy: 0.8862\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2846 - accuracy: 0.8896\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2782 - accuracy: 0.8930\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2724 - accuracy: 0.8951\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2686 - accuracy: 0.8960\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2671 - accuracy: 0.8962\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2638 - accuracy: 0.8974\n",
      "Score for fold 43: loss of 0.9124410718808182; accuracy of 72.14170692431561%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4814 - accuracy: 0.8141\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3366 - accuracy: 0.8724\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 985us/step - loss: 0.3063 - accuracy: 0.8817\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 962us/step - loss: 0.2936 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 975us/step - loss: 0.2848 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2790 - accuracy: 0.8911\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2741 - accuracy: 0.8930: 0s - loss: 0.2741 - accuracy: \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2698 - accuracy: 0.8936\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2671 - accuracy: 0.8949\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2641 - accuracy: 0.8963\n",
      "Score for fold 44: loss of 1.0443900150947614; accuracy of 58.454106280193244%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.4806 - accuracy: 0.8141\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.3389 - accuracy: 0.8691: 1s -\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3129 - accuracy: 0.8781\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.2990 - accuracy: 0.8830\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.2897 - accuracy: 0.8871\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2835 - accuracy: 0.8883\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.2790 - accuracy: 0.8907\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.2727 - accuracy: 0.8937\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2702 - accuracy: 0.8944\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.2660 - accuracy: 0.8950\n",
      "Score for fold 45: loss of 0.9365068063928765; accuracy of 69.96779388083736%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.9486937057700712 - Accuracy: 68.33735909822866%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.8126126849145034 - Accuracy: 36.73510466988728%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.223574114410627 - Accuracy: 79.46859903381642%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 2.5671860547751755 - Accuracy: 62.5402576489533%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.4223317456118245 - Accuracy: 38.00322061191627%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.7415656224832831 - Accuracy: 83.15217391304348%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.9901504874296062 - Accuracy: 79.02576489533011%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.123469437011395 - Accuracy: 48.188405797101446%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.6892428142569761 - Accuracy: 82.26650563607085%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.5078004893381152 - Accuracy: 67.31078904991948%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 3.4543331225122205 - Accuracy: 40.21739130434783%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.0880349998680694 - Accuracy: 74.09420289855072%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 1.008694793591034 - Accuracy: 74.37600644122384%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.7648294968374043 - Accuracy: 36.312399355877616%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 1.199640593150884 - Accuracy: 63.54669887278583%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 6.7420588009008995 - Accuracy: 46.39694041867955%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.46069123823630004 - Accuracy: 62.86231884057971%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 2.0289119804538966 - Accuracy: 26.570048309178745%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 1.0662668162301459 - Accuracy: 77.13365539452496%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.5879754875186365 - Accuracy: 79.66988727858293%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 0.9767616586144804 - Accuracy: 61.55394524959742%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 0.6046836729617634 - Accuracy: 81.56199677938808%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 0.9593763125065818 - Accuracy: 75.50322061191626%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.36938894448433174 - Accuracy: 90.37842190016103%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.2471855504882144 - Accuracy: 90.68035426731079%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 2.528577310118853 - Accuracy: 55.65619967793881%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.7126033262298778 - Accuracy: 83.43397745571659%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 2.4090518139268235 - Accuracy: 28.07971014492754%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 0.8925345347364615 - Accuracy: 88.62721417069244%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 2.316445784048027 - Accuracy: 67.27053140096618%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.2807664558751285 - Accuracy: 83.11191626409018%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 1.9895585029789449 - Accuracy: 39.65378421900161%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.3655247457463539 - Accuracy: 86.37278582930756%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.2721465506800909 - Accuracy: 61.83574879227053%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.8891492590126563 - Accuracy: 76.40901771336553%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.6163108474672516 - Accuracy: 72.30273752012883%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.45902508303368633 - Accuracy: 88.76811594202898%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 0.393496482641954 - Accuracy: 85.32608695652173%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.2729863016919223 - Accuracy: 66.74718196457327%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 2.857881701444898 - Accuracy: 10.929951690821255%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.731502108056932 - Accuracy: 81.17954911433173%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.8402365528357526 - Accuracy: 73.26892109500805%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.9124410718808182 - Accuracy: 72.14170692431561%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.0443900150947614 - Accuracy: 58.454106280193244%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 0.9365068063928765 - Accuracy: 69.96779388083736%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 66.12050456253354 (+- 19.228291811496614)\n",
      "> Loss: 1.384591019516678\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = X_train_df['Subject_ID'].values \n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, './10_TF_ANN.h5', overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/10_TF_ANN/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/10_TF_ANN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/N1/Data6/Data-2020/10_code/50_deep_learning/53_tensorflow_models/53_tensorflow_Duke_Data\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 [==============================] - 0s 897us/step - loss: 1.4011 - accuracy: 0.7065\n",
      "Test loss, Test acc: [1.4011396852426565, 0.7065005508941435]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18865,  1411,    92,    49],\n",
       "       [ 1796, 10668,   971,  2155],\n",
       "       [ 1327,  3760,  2590,   394],\n",
       "       [   71,  1660,   166,  1221]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm/cm.astype(np.float).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 362
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le.classes_, yticklabels = le.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data Rolled - Confusion Matrix')\n",
    "plt.savefig('Duke_Data_ANN_Rolled_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.707 \n",
      "F1 Score: 0.718\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

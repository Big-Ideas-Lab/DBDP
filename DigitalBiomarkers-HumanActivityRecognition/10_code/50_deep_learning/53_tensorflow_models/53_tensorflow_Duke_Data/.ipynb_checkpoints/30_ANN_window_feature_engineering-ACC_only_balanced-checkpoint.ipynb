{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Feature Classification Model: ANN with Feature Engineering - ACC Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is composed of an artifical neural network classification model to evaluate if using features from windows of time (20 seconds with 10 second overlap), would generate a better model than our simple timepoint classifier. Leave-One-Group-Out (LOGO) Cross-Validation is used to validate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INPUT: .csv files containing the rolled sensor data with feature engineering (engineered_features.csv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT: Neural Network Multi-Classification Window Featuer Model (F1 Score = 0.829)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded dataset contains windows of data that are 20 seconds long with a 10 second overlap. These are stored as arrays in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('/Users/N1/Data7/Data-2020/10_code/40_usable_data_for_models/41_Duke_Data/engineered_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a window number that changes everytime there is a new activity present, as we wish to use this as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(count=df.groupby(df.Activity.ne(df.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>ACC1_skew</th>\n",
       "      <th>ACC2_skew</th>\n",
       "      <th>ACC3_skew</th>\n",
       "      <th>TEMP_skew</th>\n",
       "      <th>EDA_skew</th>\n",
       "      <th>BVP_skew</th>\n",
       "      <th>HR_skew</th>\n",
       "      <th>Magnitude_skew</th>\n",
       "      <th>ACC1_min</th>\n",
       "      <th>ACC2_min</th>\n",
       "      <th>ACC3_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>Magnitude_min</th>\n",
       "      <th>ACC1_max</th>\n",
       "      <th>ACC2_max</th>\n",
       "      <th>ACC3_max</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>Magnitude_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...</td>\n",
       "      <td>[27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...</td>\n",
       "      <td>[40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...</td>\n",
       "      <td>[32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.275354 0.276634 0.270231 0.270231 0.26895 0...</td>\n",
       "      <td>[15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...</td>\n",
       "      <td>[78.98 78.83500000000002 78.69 78.545 78.4 78....</td>\n",
       "      <td>[63.410093833710725 63.453053512025726 63.4961...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>32.350</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>-0.082592</td>\n",
       "      <td>-0.558848</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>-0.392823</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.531557</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.456522</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>69.7650</td>\n",
       "      <td>61.692787</td>\n",
       "      <td>41.543478</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>34.83</td>\n",
       "      <td>78.98</td>\n",
       "      <td>63.757353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0 39.06521739130435 39.130434782608695 39....</td>\n",
       "      <td>[29.0 28.93478260869565 28.869565217391305 28....</td>\n",
       "      <td>[38.0 38.02173913043478 38.04347826086956 38.0...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.25998499999999997 0.25998499999999997 0.258...</td>\n",
       "      <td>[-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...</td>\n",
       "      <td>[73.52 73.435 73.35 73.265 73.18 73.0925 73.00...</td>\n",
       "      <td>[61.69278726074872 61.7168170027034 61.7409828...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>32.339</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.170923</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>64.8025</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>37.72</td>\n",
       "      <td>73.52</td>\n",
       "      <td>62.936476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392 41.67391304347826 41.739130...</td>\n",
       "      <td>[26.39130434782609 26.32608695652174 26.260869...</td>\n",
       "      <td>[38.869565217391305 38.89130434782609 38.91304...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.265108 0.263827 0.266389 0.265108 0.266389 ...</td>\n",
       "      <td>[-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...</td>\n",
       "      <td>[69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...</td>\n",
       "      <td>[62.758486272725364 62.78782873035957 62.81730...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>32.337</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>0.227966</td>\n",
       "      <td>1.329886</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>-0.274279</td>\n",
       "      <td>0.185657</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.252301</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>60.9950</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>45.532258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>47.14</td>\n",
       "      <td>69.63</td>\n",
       "      <td>64.010791</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575 44.14285714285715 44.31428...</td>\n",
       "      <td>[24.514285714285712 24.42857142857143 24.34285...</td>\n",
       "      <td>[37.17142857142857 37.142857142857146 37.11428...</td>\n",
       "      <td>[32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.258704 0.258704 0.258704 0.257424 0.257424 ...</td>\n",
       "      <td>[17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....</td>\n",
       "      <td>[64.68 64.555 64.43 64.305 64.18 64.0475 63.91...</td>\n",
       "      <td>[62.579164557660036 62.649331804179724 62.7200...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>-4.941414</td>\n",
       "      <td>-1.040349</td>\n",
       "      <td>3.435987</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>-0.828441</td>\n",
       "      <td>0.406263</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.985816</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.8025</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>47.14</td>\n",
       "      <td>64.68</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418 45.564516129032256 45.58064...</td>\n",
       "      <td>[25.64516129032258 25.69354838709677 25.741935...</td>\n",
       "      <td>[37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.253581 0.253581 0.253581 0.252301 0.252301 ...</td>\n",
       "      <td>[-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...</td>\n",
       "      <td>[60.92 60.8475 60.77500000000001 60.7025 60.63...</td>\n",
       "      <td>[64.04162603123257 64.07248675362091 64.103373...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>32.389</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-1.857224</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>1.380509</td>\n",
       "      <td>-0.686481</td>\n",
       "      <td>1.239716</td>\n",
       "      <td>-0.833856</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.408229</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.702128</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.249739</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.5300</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.43</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>39.00</td>\n",
       "      <td>60.92</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...   \n",
       "1  [39.0 39.06521739130435 39.130434782608695 39....   \n",
       "2  [41.60869565217392 41.67391304347826 41.739130...   \n",
       "3  [43.971428571428575 44.14285714285715 44.31428...   \n",
       "4  [45.54838709677418 45.564516129032256 45.58064...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...   \n",
       "1  [29.0 28.93478260869565 28.869565217391305 28....   \n",
       "2  [26.39130434782609 26.32608695652174 26.260869...   \n",
       "3  [24.514285714285712 24.42857142857143 24.34285...   \n",
       "4  [25.64516129032258 25.69354838709677 25.741935...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...   \n",
       "1  [38.0 38.02173913043478 38.04347826086956 38.0...   \n",
       "2  [38.869565217391305 38.89130434782609 38.91304...   \n",
       "3  [37.17142857142857 37.142857142857146 37.11428...   \n",
       "4  [37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....   \n",
       "1  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "2  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "3  [32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....   \n",
       "4  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.275354 0.276634 0.270231 0.270231 0.26895 0...   \n",
       "1  [0.25998499999999997 0.25998499999999997 0.258...   \n",
       "2  [0.265108 0.263827 0.266389 0.265108 0.266389 ...   \n",
       "3  [0.258704 0.258704 0.258704 0.257424 0.257424 ...   \n",
       "4  [0.253581 0.253581 0.253581 0.252301 0.252301 ...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...   \n",
       "1  [-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...   \n",
       "2  [-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...   \n",
       "3  [17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....   \n",
       "4  [-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [78.98 78.83500000000002 78.69 78.545 78.4 78....   \n",
       "1  [73.52 73.435 73.35 73.265 73.18 73.0925 73.00...   \n",
       "2  [69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...   \n",
       "3  [64.68 64.555 64.43 64.305 64.18 64.0475 63.91...   \n",
       "4  [60.92 60.8475 60.77500000000001 60.7025 60.63...   \n",
       "\n",
       "                                           Magnitude Subject_ID  Activity  \\\n",
       "0  [63.410093833710725 63.453053512025726 63.4961...     19-001  Baseline   \n",
       "1  [61.69278726074872 61.7168170027034 61.7409828...     19-001  Baseline   \n",
       "2  [62.758486272725364 62.78782873035957 62.81730...     19-001  Baseline   \n",
       "3  [62.579164557660036 62.649331804179724 62.7200...     19-001  Baseline   \n",
       "4  [64.04162603123257 64.07248675362091 64.103373...     19-001  Baseline   \n",
       "\n",
       "   Round  ACC1_mean  ACC2_mean  ACC3_mean  TEMP_mean  EDA_mean  BVP_mean  \\\n",
       "0      1  40.248370  28.012880  38.824457     32.350  0.262354 -0.109875   \n",
       "1      1  40.820000  26.815000  38.192500     32.339  0.261058  0.321375   \n",
       "2      1  43.252235  25.312684  37.488043     32.337  0.259585  0.684000   \n",
       "3      1  44.905798  24.915984  37.638218     32.356  0.254510 -0.180875   \n",
       "4      1  43.577055  22.974382  38.971144     32.389  0.252733 -0.209750   \n",
       "\n",
       "     HR_mean  Magnitude_mean  ACC1_std  ACC2_std  ACC3_std  TEMP_std  \\\n",
       "0  73.931187       62.553853  0.701573  0.687590  0.632616  0.017607   \n",
       "1  69.481750       62.021872  1.192214  1.149559  0.529382  0.012610   \n",
       "2  64.893188       62.621785  2.109896  0.815025  0.647914  0.010536   \n",
       "3  61.157687       63.734171  1.832017  1.509593  1.773398  0.025377   \n",
       "4  59.226438       62.913435  2.115371  2.585687  1.809092  0.027000   \n",
       "\n",
       "    EDA_std    BVP_std    HR_std  Magnitude_std  ACC1_skew  ACC2_skew  \\\n",
       "0  0.004877  18.439453  2.574676       0.609756  -0.082592  -0.558848   \n",
       "1  0.003007  20.104717  2.608254       0.542348   0.515544   0.109446   \n",
       "2  0.004337  23.756276  2.639037       0.942868  -0.473020   0.227966   \n",
       "3  0.002396  25.635645  1.674001       0.841361  -4.941414  -1.040349   \n",
       "4  0.002055  25.593597  0.684349       1.365652  -1.857224   0.511935   \n",
       "\n",
       "   ACC3_skew  TEMP_skew  EDA_skew  BVP_skew   HR_skew  Magnitude_skew  \\\n",
       "0   0.705668   0.714533  0.896382 -0.392823  0.296262        0.531557   \n",
       "1  -0.188071   0.787066  0.212943 -0.322900 -0.170923       -0.438037   \n",
       "2   1.329886   0.620801  0.072564 -0.274279  0.185657       -0.382833   \n",
       "3   3.435987   0.672586  0.734482 -0.828441  0.406263       -0.532117   \n",
       "4   1.380509  -0.686481  1.239716 -0.833856  0.996232        0.408229   \n",
       "\n",
       "   ACC1_min   ACC2_min  ACC3_min  TEMP_min   EDA_min  BVP_min   HR_min  \\\n",
       "0      39.0  26.456522      38.0     32.33  0.254862   -42.99  69.7650   \n",
       "1      39.0  24.600000      37.2     32.31  0.254862   -48.52  64.8025   \n",
       "2      39.0  24.000000      37.0     32.31  0.252301   -48.52  60.9950   \n",
       "3      32.0  20.985816      37.0     32.33  0.251020  -101.74  58.8025   \n",
       "4      32.0  20.702128      37.0     32.33  0.249739  -101.74  58.5300   \n",
       "\n",
       "   Magnitude_min   ACC1_max  ACC2_max  ACC3_max  TEMP_max   EDA_max  BVP_max  \\\n",
       "0      61.692787  41.543478      29.0      40.0     32.39  0.276634    34.83   \n",
       "1      60.778286  43.800000      29.0      39.0     32.37  0.266389    37.72   \n",
       "2      60.778286  45.532258      27.0      39.0     32.37  0.266389    47.14   \n",
       "3      61.392182  46.000000      27.0      48.0     32.41  0.262546    47.14   \n",
       "4      61.392182  46.000000      27.0      48.0     32.43  0.258704    39.00   \n",
       "\n",
       "   HR_max  Magnitude_max  count  \n",
       "0   78.98      63.757353      1  \n",
       "1   73.52      62.936476      2  \n",
       "2   69.63      64.010791      3  \n",
       "3   64.68      65.711491      4  \n",
       "4   60.92      65.711491      5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These will be our Subjects in our test set: [39 17 45]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rands = np.random.choice(df.Subject_ID.unique(),3, replace=False)\n",
    "print(f' These will be our Subjects in our test set: {rands}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Subjects into Test and Train Sets (n=44,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Subject_ID'].isin(rands)] \n",
    "train = df[-df['Subject_ID'].isin(rands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', \n",
    "        'Magnitude_mean', 'ACC1_std', 'ACC2_std', 'ACC3_std', 'Subject_ID', 'count','Activity']]\n",
    "test = test[['ACC1_mean', 'ACC2_mean', 'ACC3_mean', \n",
    "        'Magnitude_mean', 'ACC1_std', 'ACC2_std', 'ACC3_std', 'Subject_ID', 'count','Activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train_SID is made so we can use the Subject_ID values to perform LOGO later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2929\n",
       "1    2323\n",
       "3     505\n",
       "2     505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n",
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = train[train['Activity'] == 0]\n",
    "one = train[train['Activity'] == 1]\n",
    "two = train[train['Activity'] == 2]\n",
    "three =train[train['Activity'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = zero.sample(505)\n",
    "one = one.sample(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([zero, one, two, three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    505\n",
       "2    505\n",
       "1    505\n",
       "0    505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SID = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID and window count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject_ID and window count must be one-hot encoded to be used as features in our model. Test and train dataframes must be concatenated before we one-hot encode, so that we do not get different encodings for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] =1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'], prefix = 'SID')], axis =1).drop('Subject_ID', axis =1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['count'], prefix = 'count')], axis =1).drop('count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 121) (310, 121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/N1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop([\"train\"], axis = 1, inplace = True)\n",
    "test.drop([\"train\"], axis = 1, inplace = True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove activity from our train and test datasets as this is the y variable (target variable) and we are only interested in keeping the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train.drop(\"Activity\", axis =1)\n",
    "test_f = test.drop(\"Activity\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X (features) and y (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_f\n",
    "y_train = train.Activity\n",
    "X_test = test_f\n",
    "y_test = test.Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>SID_0</th>\n",
       "      <th>SID_1</th>\n",
       "      <th>SID_2</th>\n",
       "      <th>SID_3</th>\n",
       "      <th>SID_4</th>\n",
       "      <th>SID_5</th>\n",
       "      <th>SID_6</th>\n",
       "      <th>SID_7</th>\n",
       "      <th>SID_8</th>\n",
       "      <th>SID_9</th>\n",
       "      <th>SID_10</th>\n",
       "      <th>SID_11</th>\n",
       "      <th>SID_12</th>\n",
       "      <th>SID_13</th>\n",
       "      <th>SID_14</th>\n",
       "      <th>SID_15</th>\n",
       "      <th>SID_16</th>\n",
       "      <th>SID_17</th>\n",
       "      <th>SID_18</th>\n",
       "      <th>SID_19</th>\n",
       "      <th>SID_20</th>\n",
       "      <th>SID_21</th>\n",
       "      <th>SID_22</th>\n",
       "      <th>SID_23</th>\n",
       "      <th>SID_24</th>\n",
       "      <th>SID_25</th>\n",
       "      <th>SID_26</th>\n",
       "      <th>SID_27</th>\n",
       "      <th>SID_28</th>\n",
       "      <th>SID_29</th>\n",
       "      <th>SID_30</th>\n",
       "      <th>SID_31</th>\n",
       "      <th>SID_32</th>\n",
       "      <th>SID_33</th>\n",
       "      <th>SID_34</th>\n",
       "      <th>SID_35</th>\n",
       "      <th>SID_36</th>\n",
       "      <th>SID_37</th>\n",
       "      <th>SID_38</th>\n",
       "      <th>SID_39</th>\n",
       "      <th>SID_40</th>\n",
       "      <th>SID_41</th>\n",
       "      <th>SID_42</th>\n",
       "      <th>SID_43</th>\n",
       "      <th>SID_44</th>\n",
       "      <th>SID_45</th>\n",
       "      <th>SID_46</th>\n",
       "      <th>SID_47</th>\n",
       "      <th>SID_48</th>\n",
       "      <th>SID_49</th>\n",
       "      <th>SID_50</th>\n",
       "      <th>SID_51</th>\n",
       "      <th>SID_52</th>\n",
       "      <th>SID_53</th>\n",
       "      <th>SID_54</th>\n",
       "      <th>count_1</th>\n",
       "      <th>count_2</th>\n",
       "      <th>count_3</th>\n",
       "      <th>count_4</th>\n",
       "      <th>count_5</th>\n",
       "      <th>count_6</th>\n",
       "      <th>count_7</th>\n",
       "      <th>count_8</th>\n",
       "      <th>count_9</th>\n",
       "      <th>count_10</th>\n",
       "      <th>count_11</th>\n",
       "      <th>count_12</th>\n",
       "      <th>count_13</th>\n",
       "      <th>count_14</th>\n",
       "      <th>count_15</th>\n",
       "      <th>count_16</th>\n",
       "      <th>count_17</th>\n",
       "      <th>count_18</th>\n",
       "      <th>count_19</th>\n",
       "      <th>count_20</th>\n",
       "      <th>count_21</th>\n",
       "      <th>count_22</th>\n",
       "      <th>count_23</th>\n",
       "      <th>count_24</th>\n",
       "      <th>count_25</th>\n",
       "      <th>count_26</th>\n",
       "      <th>count_27</th>\n",
       "      <th>count_28</th>\n",
       "      <th>count_29</th>\n",
       "      <th>count_30</th>\n",
       "      <th>count_31</th>\n",
       "      <th>count_32</th>\n",
       "      <th>count_33</th>\n",
       "      <th>count_34</th>\n",
       "      <th>count_35</th>\n",
       "      <th>count_36</th>\n",
       "      <th>count_37</th>\n",
       "      <th>count_38</th>\n",
       "      <th>count_39</th>\n",
       "      <th>count_40</th>\n",
       "      <th>count_41</th>\n",
       "      <th>count_42</th>\n",
       "      <th>count_43</th>\n",
       "      <th>count_44</th>\n",
       "      <th>count_45</th>\n",
       "      <th>count_46</th>\n",
       "      <th>count_47</th>\n",
       "      <th>count_48</th>\n",
       "      <th>count_49</th>\n",
       "      <th>count_50</th>\n",
       "      <th>count_51</th>\n",
       "      <th>count_52</th>\n",
       "      <th>count_53</th>\n",
       "      <th>count_54</th>\n",
       "      <th>count_55</th>\n",
       "      <th>count_56</th>\n",
       "      <th>count_57</th>\n",
       "      <th>count_58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.435833</td>\n",
       "      <td>1.262854</td>\n",
       "      <td>-0.300627</td>\n",
       "      <td>-0.081639</td>\n",
       "      <td>1.952124</td>\n",
       "      <td>2.091931</td>\n",
       "      <td>1.943832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>-0.677686</td>\n",
       "      <td>-1.129973</td>\n",
       "      <td>0.970477</td>\n",
       "      <td>-0.309359</td>\n",
       "      <td>-0.426971</td>\n",
       "      <td>-0.221658</td>\n",
       "      <td>-0.398060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>0.345336</td>\n",
       "      <td>2.056002</td>\n",
       "      <td>-0.798363</td>\n",
       "      <td>0.589234</td>\n",
       "      <td>0.227483</td>\n",
       "      <td>0.493635</td>\n",
       "      <td>-0.155969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>0.772761</td>\n",
       "      <td>1.406226</td>\n",
       "      <td>-0.622967</td>\n",
       "      <td>0.033461</td>\n",
       "      <td>1.349438</td>\n",
       "      <td>1.574961</td>\n",
       "      <td>0.155253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>1.206269</td>\n",
       "      <td>0.230053</td>\n",
       "      <td>-0.717958</td>\n",
       "      <td>-0.111885</td>\n",
       "      <td>0.792336</td>\n",
       "      <td>1.804125</td>\n",
       "      <td>0.655970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>0.413261</td>\n",
       "      <td>-0.610243</td>\n",
       "      <td>0.711764</td>\n",
       "      <td>-0.850371</td>\n",
       "      <td>1.522210</td>\n",
       "      <td>-0.094951</td>\n",
       "      <td>1.332030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>0.102606</td>\n",
       "      <td>-0.371634</td>\n",
       "      <td>1.354930</td>\n",
       "      <td>0.138474</td>\n",
       "      <td>-0.567114</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.206574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>0.105398</td>\n",
       "      <td>-0.350205</td>\n",
       "      <td>1.058363</td>\n",
       "      <td>-1.023157</td>\n",
       "      <td>-0.777214</td>\n",
       "      <td>-0.396973</td>\n",
       "      <td>0.644845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>0.141874</td>\n",
       "      <td>-0.466625</td>\n",
       "      <td>0.744690</td>\n",
       "      <td>-2.145888</td>\n",
       "      <td>-0.799801</td>\n",
       "      <td>-0.579423</td>\n",
       "      <td>-0.209663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>0.156642</td>\n",
       "      <td>-0.510083</td>\n",
       "      <td>1.076498</td>\n",
       "      <td>-0.813044</td>\n",
       "      <td>-0.473018</td>\n",
       "      <td>-0.617739</td>\n",
       "      <td>0.123354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2020 rows  120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACC1_mean  ACC2_mean  ACC3_mean  Magnitude_mean  ACC1_std  ACC2_std  \\\n",
       "800    0.435833   1.262854  -0.300627       -0.081639  1.952124  2.091931   \n",
       "606   -0.677686  -1.129973   0.970477       -0.309359 -0.426971 -0.221658   \n",
       "823    0.345336   2.056002  -0.798363        0.589234  0.227483  0.493635   \n",
       "4291   0.772761   1.406226  -0.622967        0.033461  1.349438  1.574961   \n",
       "4326   1.206269   0.230053  -0.717958       -0.111885  0.792336  1.804125   \n",
       "...         ...        ...        ...             ...       ...       ...   \n",
       "6567   0.413261  -0.610243   0.711764       -0.850371  1.522210 -0.094951   \n",
       "6568   0.102606  -0.371634   1.354930        0.138474 -0.567114 -0.271648   \n",
       "6569   0.105398  -0.350205   1.058363       -1.023157 -0.777214 -0.396973   \n",
       "6570   0.141874  -0.466625   0.744690       -2.145888 -0.799801 -0.579423   \n",
       "6571   0.156642  -0.510083   1.076498       -0.813044 -0.473018 -0.617739   \n",
       "\n",
       "      ACC3_std  SID_0  SID_1  SID_2  SID_3  SID_4  SID_5  SID_6  SID_7  SID_8  \\\n",
       "800   1.943832      0      0      0      0      0      0      1      0      0   \n",
       "606  -0.398060      0      0      0      0      1      0      0      0      0   \n",
       "823  -0.155969      0      0      0      0      0      0      1      0      0   \n",
       "4291  0.155253      0      0      0      0      0      0      0      0      0   \n",
       "4326  0.655970      0      0      0      0      0      0      0      0      0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "6567  1.332030      0      0      0      0      0      0      0      0      0   \n",
       "6568 -0.206574      0      0      0      0      0      0      0      0      0   \n",
       "6569  0.644845      0      0      0      0      0      0      0      0      0   \n",
       "6570 -0.209663      0      0      0      0      0      0      0      0      0   \n",
       "6571  0.123354      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      SID_9  SID_10  SID_11  SID_12  SID_13  SID_14  SID_15  SID_16  SID_17  \\\n",
       "800       0       0       0       0       0       0       0       0       0   \n",
       "606       0       0       0       0       0       0       0       0       0   \n",
       "823       0       0       0       0       0       0       0       0       0   \n",
       "4291      0       0       0       0       0       0       0       0       0   \n",
       "4326      0       0       0       0       0       0       0       0       0   \n",
       "...     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567      0       0       0       0       0       0       0       0       0   \n",
       "6568      0       0       0       0       0       0       0       0       0   \n",
       "6569      0       0       0       0       0       0       0       0       0   \n",
       "6570      0       0       0       0       0       0       0       0       0   \n",
       "6571      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_18  SID_19  SID_20  SID_21  SID_22  SID_23  SID_24  SID_25  SID_26  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_27  SID_28  SID_29  SID_30  SID_31  SID_32  SID_33  SID_34  SID_35  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       1   \n",
       "4326       0       0       0       0       0       0       0       0       1   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_36  SID_37  SID_38  SID_39  SID_40  SID_41  SID_42  SID_43  SID_44  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_45  SID_46  SID_47  SID_48  SID_49  SID_50  SID_51  SID_52  SID_53  \\\n",
       "800        0       0       0       0       0       0       0       0       0   \n",
       "606        0       0       0       0       0       0       0       0       0   \n",
       "823        0       0       0       0       0       0       0       0       0   \n",
       "4291       0       0       0       0       0       0       0       0       0   \n",
       "4326       0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_54  count_1  count_2  count_3  count_4  count_5  count_6  count_7  \\\n",
       "800        0        1        0        0        0        0        0        0   \n",
       "606        0        0        0        0        0        0        0        0   \n",
       "823        0        0        0        0        0        0        0        0   \n",
       "4291       0        0        0        0        0        0        0        0   \n",
       "4326       0        0        0        0        0        0        0        0   \n",
       "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "6567       1        1        0        0        0        0        0        0   \n",
       "6568       1        0        1        0        0        0        0        0   \n",
       "6569       1        0        0        1        0        0        0        0   \n",
       "6570       1        0        0        0        1        0        0        0   \n",
       "6571       1        0        0        0        0        1        0        0   \n",
       "\n",
       "      count_8  count_9  count_10  count_11  count_12  count_13  count_14  \\\n",
       "800         0        0         0         0         0         0         0   \n",
       "606         0        0         0         0         0         0         0   \n",
       "823         0        0         0         0         0         0         0   \n",
       "4291        0        0         0         0         0         0         0   \n",
       "4326        0        0         0         0         0         0         0   \n",
       "...       ...      ...       ...       ...       ...       ...       ...   \n",
       "6567        0        0         0         0         0         0         0   \n",
       "6568        0        0         0         0         0         0         0   \n",
       "6569        0        0         0         0         0         0         0   \n",
       "6570        0        0         0         0         0         0         0   \n",
       "6571        0        0         0         0         0         0         0   \n",
       "\n",
       "      count_15  count_16  count_17  count_18  count_19  count_20  count_21  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         1         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_22  count_23  count_24  count_25  count_26  count_27  count_28  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         1         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_29  count_30  count_31  count_32  count_33  count_34  count_35  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_36  count_37  count_38  count_39  count_40  count_41  count_42  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_43  count_44  count_45  count_46  count_47  count_48  count_49  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         0         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_50  count_51  count_52  count_53  count_54  count_55  count_56  \\\n",
       "800          0         0         0         0         0         0         0   \n",
       "606          0         0         0         0         0         1         0   \n",
       "823          0         0         0         0         0         0         0   \n",
       "4291         0         0         0         0         0         0         0   \n",
       "4326         0         0         0         0         0         1         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_57  count_58  \n",
       "800          0         0  \n",
       "606          0         0  \n",
       "823          0         0  \n",
       "4291         0         0  \n",
       "4326         0         0  \n",
       "...        ...       ...  \n",
       "6567         0         0  \n",
       "6568         0         0  \n",
       "6569         0         0  \n",
       "6570         0         0  \n",
       "6571         0         0  \n",
       "\n",
       "[2020 rows x 120 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:7] = sc.fit_transform(X_train.iloc[:,:7])\n",
    "X_test.iloc[:,:7] = sc.transform(X_test.iloc[:,:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "- 6 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training and validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3389 - accuracy: 0.3567\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0890 - accuracy: 0.5376\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8374 - accuracy: 0.6786\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6995 - accuracy: 0.7473\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.7817\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.8120\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8464\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8762\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8914\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.9080\n",
      "Score for fold 1: loss of 0.546458899974823; accuracy of 82.92682766914368%, F1 of 0.8361241858032616\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3106 - accuracy: 0.4186\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 0.9867 - accuracy: 0.6234\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7149 - accuracy: 0.7189\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.7725\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5124 - accuracy: 0.8028\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8453\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8741\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8837\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2986 - accuracy: 0.9080\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.9171\n",
      "Score for fold 2: loss of 0.6648795008659363; accuracy of 80.95238208770752%, F1 of 0.807544070259683\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2904 - accuracy: 0.4354\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9603 - accuracy: 0.6201\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7766 - accuracy: 0.6978\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6444 - accuracy: 0.7593\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7856\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.8027\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4595 - accuracy: 0.8325\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8623\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8819\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8875\n",
      "Score for fold 3: loss of 0.7662348747253418; accuracy of 63.15789222717285%, F1 of 0.5944862155388472\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2813 - accuracy: 0.3988\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9750 - accuracy: 0.6321\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7302 - accuracy: 0.7293\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.7910\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.8376\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.3876 - accuracy: 0.8659\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8887\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.9054\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2317 - accuracy: 0.9241\n",
      "Score for fold 4: loss of 0.8013269305229187; accuracy of 81.81818127632141%, F1 of 0.8169985385458531\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3171 - accuracy: 0.3875\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0180 - accuracy: 0.5952\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7975 - accuracy: 0.6986\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6585 - accuracy: 0.7492\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7908\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.8176\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4262 - accuracy: 0.8480\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8647\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8810\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.8977\n",
      "Score for fold 5: loss of 1.5432920455932617; accuracy of 58.69565010070801%, F1 of 0.5233216724970847\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3201 - accuracy: 0.3505\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0705 - accuracy: 0.5379\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7523 - accuracy: 0.7096\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.7480\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7848\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8146\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4610 - accuracy: 0.8273\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8424\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8692\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8753\n",
      "Score for fold 6: loss of 0.6483484506607056; accuracy of 72.50000238418579%, F1 of 0.7063988095238095\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2972 - accuracy: 0.3382\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0003 - accuracy: 0.6128\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7475 - accuracy: 0.6890\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6332 - accuracy: 0.7532\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5534 - accuracy: 0.7769\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8011\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.8395\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8425\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3626 - accuracy: 0.8647\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8824\n",
      "Score for fold 7: loss of 0.19577109813690186; accuracy of 94.87179517745972%, F1 of 0.9486536854957908\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3276 - accuracy: 0.3766\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0094 - accuracy: 0.5931\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7924 - accuracy: 0.6870\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.7340\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5412 - accuracy: 0.7900\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.8344\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3853 - accuracy: 0.8637\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.9011\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.9091\n",
      "Score for fold 8: loss of 1.014273762702942; accuracy of 71.79487347602844%, F1 of 0.7182817182817184\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3363 - accuracy: 0.3498\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0365 - accuracy: 0.5878\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7967 - accuracy: 0.6990\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.7700: 0s - loss: 0.6676 - accuracy\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5462 - accuracy: 0.8098\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.8359\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8626\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8853\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.9054\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.9099\n",
      "Score for fold 9: loss of 0.819299042224884; accuracy of 75.75757503509521%, F1 of 0.7406395790155993\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3204 - accuracy: 0.3034\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1320 - accuracy: 0.5121\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9064 - accuracy: 0.6542\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7117 - accuracy: 0.7465\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5881 - accuracy: 0.7858\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8261\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8639\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8821\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.8957\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2858 - accuracy: 0.9037\n",
      "Score for fold 10: loss of 0.2607787549495697; accuracy of 86.11111044883728%, F1 of 0.8622807017543859\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3274 - accuracy: 0.3516\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0362 - accuracy: 0.5708\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7956 - accuracy: 0.6841\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6496 - accuracy: 0.7486\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5798 - accuracy: 0.7844\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.8418\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8564\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8811\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8861\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.9058\n",
      "Score for fold 11: loss of 0.642505943775177; accuracy of 77.14285850524902%, F1 of 0.7718253968253967\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3130 - accuracy: 0.3620\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.0002 - accuracy: 0.6137\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7698 - accuracy: 0.7190\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.7544\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.8035\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.8289\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8516\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8709\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8927\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8987\n",
      "Score for fold 12: loss of 0.8291533589363098; accuracy of 75.55555701255798%, F1 of 0.7498765432098766\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3299 - accuracy: 0.3542\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0793 - accuracy: 0.5979\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7664 - accuracy: 0.7200\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6241 - accuracy: 0.7639\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5161 - accuracy: 0.8083\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.8557\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.8794\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3284 - accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2999 - accuracy: 0.9016\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2573 - accuracy: 0.9112\n",
      "Score for fold 13: loss of 0.40878039598464966; accuracy of 86.84210777282715%, F1 of 0.8698060941828255\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3079 - accuracy: 0.3855\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9803 - accuracy: 0.6231\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7921 - accuracy: 0.7043\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.7366\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5744 - accuracy: 0.7866\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5160 - accuracy: 0.7997\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.8259\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4028 - accuracy: 0.8572\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8713\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8860\n",
      "Score for fold 14: loss of 1.0958583354949951; accuracy of 76.31579041481018%, F1 of 0.7313509919621972\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2928 - accuracy: 0.4002\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9959 - accuracy: 0.6109\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7916 - accuracy: 0.6905\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6532 - accuracy: 0.7589\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5597 - accuracy: 0.7928\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4826 - accuracy: 0.8207\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.8531\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3794 - accuracy: 0.8647\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8850\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8997\n",
      "Score for fold 15: loss of 0.9047871232032776; accuracy of 63.04348111152649%, F1 of 0.6447550034506557\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2780 - accuracy: 0.4242\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9305 - accuracy: 0.6413\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7690 - accuracy: 0.7027\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.7555\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7920\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.8168\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.8351\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8671\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3171 - accuracy: 0.8853\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2834 - accuracy: 0.9097\n",
      "Score for fold 16: loss of 0.666027307510376; accuracy of 71.42857313156128%, F1 of 0.7098021026592455\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2920 - accuracy: 0.3802\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0473 - accuracy: 0.5774\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8009 - accuracy: 0.7078\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.7599\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7947\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.8311\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4000 - accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8777\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8999\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2709 - accuracy: 0.9075\n",
      "Score for fold 17: loss of 0.3582523763179779; accuracy of 92.85714030265808%, F1 of 0.9283857792629723\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2857 - accuracy: 0.4105\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9513 - accuracy: 0.6268\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7317 - accuracy: 0.7168\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.7445\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.7832\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8063\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.8410\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8702\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8763\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8969\n",
      "Score for fold 18: loss of 1.3283700942993164; accuracy of 68.75%, F1 of 0.5667735042735043\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3087 - accuracy: 0.3926\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9931 - accuracy: 0.6179\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8029 - accuracy: 0.6941\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.7445\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.7752\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5085 - accuracy: 0.8059\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.8342\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4205 - accuracy: 0.8528\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8856\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.9007\n",
      "Score for fold 19: loss of 0.42183998227119446; accuracy of 88.88888955116272%, F1 of 0.8796296296296295\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3194 - accuracy: 0.3788\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0281 - accuracy: 0.5792\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7748 - accuracy: 0.7084\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6353 - accuracy: 0.7570\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.8066\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4386 - accuracy: 0.8432\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4132 - accuracy: 0.8537\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8968\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.9048\n",
      "Score for fold 20: loss of 0.7689235806465149; accuracy of 66.66666865348816%, F1 of 0.5793650793650793\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3373 - accuracy: 0.3673\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9905 - accuracy: 0.6120\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7708 - accuracy: 0.7074\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6291 - accuracy: 0.7699\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.8032\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4696 - accuracy: 0.8335\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3999 - accuracy: 0.8658\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.8759\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8966\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.9142\n",
      "Score for fold 21: loss of 0.6986738443374634; accuracy of 71.05262875556946%, F1 of 0.6525404420141263\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3201 - accuracy: 0.3467\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0559 - accuracy: 0.5319\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8491 - accuracy: 0.6827\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.7490\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5686 - accuracy: 0.7829\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4938 - accuracy: 0.8163\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8639\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8998\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2787 - accuracy: 0.9114\n",
      "Score for fold 22: loss of 0.5621051788330078; accuracy of 81.81818127632141%, F1 of 0.8169103623649079\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3070 - accuracy: 0.3619\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9994 - accuracy: 0.5651\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7717 - accuracy: 0.7187\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.7739\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8191\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8545\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8723\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3149 - accuracy: 0.8936\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.9037\n",
      "Score for fold 23: loss of 0.33562806248664856; accuracy of 87.2340440750122%, F1 of 0.8696410917687513\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.2723 - accuracy: 0.3824\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9982 - accuracy: 0.6055\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7804 - accuracy: 0.6993\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.7424\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7723\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.8007\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4053 - accuracy: 0.8580\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8834\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.9042\n",
      "Score for fold 24: loss of 0.1646478921175003; accuracy of 93.75%, F1 of 0.9369879672299027\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3628 - accuracy: 0.3129\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1182 - accuracy: 0.5389\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.8265 - accuracy: 0.6805\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.7391\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5768 - accuracy: 0.7882\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.7973\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.8225\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8392\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8691\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8822\n",
      "Score for fold 25: loss of 0.5480790138244629; accuracy of 78.57142686843872%, F1 of 0.7313031795790417\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3362 - accuracy: 0.3241\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.0687 - accuracy: 0.5541\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.7882 - accuracy: 0.6982\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.7725\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5397 - accuracy: 0.8013\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8367\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8630\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.9004\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.9014\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.9095\n",
      "Score for fold 26: loss of 0.6943110823631287; accuracy of 66.66666865348816%, F1 of 0.6529456654456655\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3295 - accuracy: 0.3212\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0941 - accuracy: 0.5237\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7951 - accuracy: 0.7040\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6132 - accuracy: 0.7833\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.8066\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8556\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8753\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8960\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.9035\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.9101\n",
      "Score for fold 27: loss of 0.0872361809015274; accuracy of 100.0%, F1 of 1.0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3199 - accuracy: 0.3252\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0951 - accuracy: 0.5564\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8151 - accuracy: 0.6823\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6775 - accuracy: 0.7473\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7852\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5114 - accuracy: 0.8097\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8202\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4175 - accuracy: 0.8556\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3560 - accuracy: 0.8751\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8961\n",
      "Score for fold 28: loss of 0.3929510712623596; accuracy of 77.77777910232544%, F1 of 0.7723665223665224\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3077 - accuracy: 0.3581\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0279 - accuracy: 0.5823\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8442 - accuracy: 0.6924\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6728 - accuracy: 0.7268\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5751 - accuracy: 0.7712\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4965 - accuracy: 0.8141\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 975us/step - loss: 0.4416 - accuracy: 0.8318\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8641\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8758\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8919\n",
      "Score for fold 29: loss of 0.26424315571784973; accuracy of 92.5000011920929%, F1 of 0.9238289337474119\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3292 - accuracy: 0.3226\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0053 - accuracy: 0.6280\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7437 - accuracy: 0.7102\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.7688\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7875\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5007 - accuracy: 0.8087\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8319\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4167 - accuracy: 0.8481\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8637\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8859\n",
      "Score for fold 30: loss of 0.10591819882392883; accuracy of 94.87179517745972%, F1 of 0.9455677655677656\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3040 - accuracy: 0.3756\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9996 - accuracy: 0.6046\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7699 - accuracy: 0.7147\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6347 - accuracy: 0.7457\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.7888\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.8152\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8335\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8655\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.8766\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8939\n",
      "Score for fold 31: loss of 0.20941299200057983; accuracy of 93.99999976158142%, F1 of 0.9398632946001367\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3271 - accuracy: 0.3623\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9878 - accuracy: 0.6321\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7431 - accuracy: 0.7135\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.7721\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.8322\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8585\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8833\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.9106\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.9085\n",
      "Score for fold 32: loss of 0.13559667766094208; accuracy of 97.56097793579102%, F1 of 0.975660253496945\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3531 - accuracy: 0.3837\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0683 - accuracy: 0.5685\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8216 - accuracy: 0.6697\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.7231\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6046 - accuracy: 0.7523\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.8011\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.8419\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.8595\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.3666 - accuracy: 0.8776\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8922\n",
      "Score for fold 33: loss of 0.17344428598880768; accuracy of 91.17646813392639%, F1 of 0.9141779788838612\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3477 - accuracy: 0.3236\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1366 - accuracy: 0.4876\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9056 - accuracy: 0.6588\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6641 - accuracy: 0.7718\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.8410\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8703\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8788\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8920\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.9076\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9207\n",
      "Score for fold 34: loss of 1.6698862314224243; accuracy of 51.28205418586731%, F1 of 0.428204587597377\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2922 - accuracy: 0.3529\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0123 - accuracy: 0.6153\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8166 - accuracy: 0.7042\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6646 - accuracy: 0.7406\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.7750\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4858 - accuracy: 0.8286\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8493\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8655\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8948\n",
      "Score for fold 35: loss of 0.36998113989830017; accuracy of 83.33333134651184%, F1 of 0.8336986968700524\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3130 - accuracy: 0.4166\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0227 - accuracy: 0.5693\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8039 - accuracy: 0.6882\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6580 - accuracy: 0.7592\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.7809\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4955 - accuracy: 0.8141\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8413\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8625\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8947\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.9023\n",
      "Score for fold 36: loss of 0.6913919448852539; accuracy of 77.14285850524902%, F1 of 0.7736214485794318\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3422 - accuracy: 0.3528\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0028 - accuracy: 0.5976\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7459 - accuracy: 0.7086\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6623 - accuracy: 0.7486\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7826\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5365 - accuracy: 0.7968\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.8262\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.8348\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8677\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8819\n",
      "Score for fold 37: loss of 0.3006351590156555; accuracy of 91.4893627166748%, F1 of 0.9135340624702328\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3201 - accuracy: 0.3648\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.6080\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7896 - accuracy: 0.6852\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6459 - accuracy: 0.7563\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.8012\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.8285\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8613\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8784\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.8986\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.9127\n",
      "Score for fold 38: loss of 0.8325619697570801; accuracy of 78.94737124443054%, F1 of 0.7571379040486592\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3144 - accuracy: 0.3794\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9755 - accuracy: 0.6276\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7539 - accuracy: 0.7023\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.7523\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.7679\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.8118\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8360\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8461\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8739\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8971\n",
      "Score for fold 39: loss of 0.6904008984565735; accuracy of 78.94737124443054%, F1 of 0.7593984962406015\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3224 - accuracy: 0.3303\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0986 - accuracy: 0.5401\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8229 - accuracy: 0.6762\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6668 - accuracy: 0.7332\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5735 - accuracy: 0.7766\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.8074\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4156 - accuracy: 0.8583\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8775\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8951\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.9057\n",
      "Score for fold 40: loss of 0.15321296453475952; accuracy of 91.89189076423645%, F1 of 0.9158209592992202\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3213 - accuracy: 0.3873\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0929 - accuracy: 0.5935\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 988us/step - loss: 0.8045 - accuracy: 0.6967\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6430 - accuracy: 0.7609\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.8129\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.8473\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8736\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8974\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.9221\n",
      "Score for fold 41: loss of 0.24610383808612823; accuracy of 83.33333134651184%, F1 of 0.8021978021978022\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3174 - accuracy: 0.3069\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1114 - accuracy: 0.5866\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8411 - accuracy: 0.6810\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.7542\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.8011\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.8380\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8748\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8814\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.3104 - accuracy: 0.8950\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 996us/step - loss: 0.2757 - accuracy: 0.9152\n",
      "Score for fold 42: loss of 0.17888247966766357; accuracy of 92.30769276618958%, F1 of 0.9192310957016838\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3364 - accuracy: 0.3496\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0186 - accuracy: 0.5860\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7802 - accuracy: 0.6866\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6609 - accuracy: 0.7430\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5444 - accuracy: 0.8043\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.8431\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8647\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3195 - accuracy: 0.8919\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.9034\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.9145\n",
      "Score for fold 43: loss of 0.06428536772727966; accuracy of 96.875%, F1 of 0.968671679197995\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3322 - accuracy: 0.3426\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0093 - accuracy: 0.6428\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7440 - accuracy: 0.7124\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5955 - accuracy: 0.7730\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.8103\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.8330\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.8673\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8855\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2971 - accuracy: 0.8951\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.9041\n",
      "Score for fold 44: loss of 0.25123050808906555; accuracy of 89.47368264198303%, F1 of 0.8923444976076554\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2966 - accuracy: 0.4200\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9140 - accuracy: 0.6290\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7427 - accuracy: 0.7052\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.7511\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.7693\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.8036\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 997us/step - loss: 0.4857 - accuracy: 0.8077\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.8157\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8274\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8516\n",
      "Score for fold 45: loss of 0.7706422209739685; accuracy of 69.2307710647583%, F1 of 0.673968253968254\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 46 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3557 - accuracy: 0.3419\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1052 - accuracy: 0.5058\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8183 - accuracy: 0.6909\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.7511\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7982\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.8432\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8649\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8867\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8943\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.9110\n",
      "Score for fold 46: loss of 0.46501624584198; accuracy of 81.39534592628479%, F1 of 0.7946940151186865\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 47 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3090 - accuracy: 0.3521\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.5411\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8081 - accuracy: 0.7254\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.7627\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5588 - accuracy: 0.8050\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8227\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4086 - accuracy: 0.8544\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8635\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3476 - accuracy: 0.8746\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8937\n",
      "Score for fold 47: loss of 0.8734635710716248; accuracy of 68.57143044471741%, F1 of 0.7091036414565827\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 48 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3144 - accuracy: 0.3647\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0873 - accuracy: 0.5065\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9115 - accuracy: 0.6358\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7358 - accuracy: 0.7113\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7867\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.8154\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8446\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8617\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.8763\n",
      "Score for fold 48: loss of 0.21899673342704773; accuracy of 93.75%, F1 of 0.9340909090909091\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 49 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3259 - accuracy: 0.3937\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0056 - accuracy: 0.6179\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7491 - accuracy: 0.7264\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6260 - accuracy: 0.7678\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.8077\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.8415\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8738\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8955\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.9026\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.2662 - accuracy: 0.9122\n",
      "Score for fold 49: loss of 0.47353020310401917; accuracy of 76.92307829856873%, F1 of 0.7679556400784022\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 50 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3799 - accuracy: 0.2520\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2189 - accuracy: 0.4602\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8964 - accuracy: 0.6683\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7100 - accuracy: 0.7349\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.7797\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.8140\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.8438\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8987\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.9002\n",
      "Score for fold 50: loss of 0.27374735474586487; accuracy of 91.66666865348816%, F1 of 0.9168615984405457\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 51 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2812 - accuracy: 0.4110\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9826 - accuracy: 0.6259\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7653 - accuracy: 0.6911\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.7482\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.7796\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.8134\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.8342\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8706\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8812\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8923\n",
      "Score for fold 51: loss of 0.8739291429519653; accuracy of 73.8095223903656%, F1 of 0.7061478100011934\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 52 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3139 - accuracy: 0.4037\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9863 - accuracy: 0.6203\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7470 - accuracy: 0.7229\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.7724\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.8014\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8614\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8859\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9125\n",
      "Score for fold 52: loss of 0.09523254632949829; accuracy of 100.0%, F1 of 1.0\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.546458899974823 - Accuracy: 82.92682766914368% - F1:0.8361241858032616%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6648795008659363 - Accuracy: 80.95238208770752% - F1:0.807544070259683%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.7662348747253418 - Accuracy: 63.15789222717285% - F1:0.5944862155388472%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.8013269305229187 - Accuracy: 81.81818127632141% - F1:0.8169985385458531%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.5432920455932617 - Accuracy: 58.69565010070801% - F1:0.5233216724970847%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.6483484506607056 - Accuracy: 72.50000238418579% - F1:0.7063988095238095%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.19577109813690186 - Accuracy: 94.87179517745972% - F1:0.9486536854957908%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.014273762702942 - Accuracy: 71.79487347602844% - F1:0.7182817182817184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.819299042224884 - Accuracy: 75.75757503509521% - F1:0.7406395790155993%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.2607787549495697 - Accuracy: 86.11111044883728% - F1:0.8622807017543859%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 0.642505943775177 - Accuracy: 77.14285850524902% - F1:0.7718253968253967%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 0.8291533589363098 - Accuracy: 75.55555701255798% - F1:0.7498765432098766%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 0.40878039598464966 - Accuracy: 86.84210777282715% - F1:0.8698060941828255%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 1.0958583354949951 - Accuracy: 76.31579041481018% - F1:0.7313509919621972%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 0.9047871232032776 - Accuracy: 63.04348111152649% - F1:0.6447550034506557%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 0.666027307510376 - Accuracy: 71.42857313156128% - F1:0.7098021026592455%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.3582523763179779 - Accuracy: 92.85714030265808% - F1:0.9283857792629723%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 1.3283700942993164 - Accuracy: 68.75% - F1:0.5667735042735043%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.42183998227119446 - Accuracy: 88.88888955116272% - F1:0.8796296296296295%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.7689235806465149 - Accuracy: 66.66666865348816% - F1:0.5793650793650793%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 0.6986738443374634 - Accuracy: 71.05262875556946% - F1:0.6525404420141263%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 0.5621051788330078 - Accuracy: 81.81818127632141% - F1:0.8169103623649079%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 0.33562806248664856 - Accuracy: 87.2340440750122% - F1:0.8696410917687513%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.1646478921175003 - Accuracy: 93.75% - F1:0.9369879672299027%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.5480790138244629 - Accuracy: 78.57142686843872% - F1:0.7313031795790417%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.6943110823631287 - Accuracy: 66.66666865348816% - F1:0.6529456654456655%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.0872361809015274 - Accuracy: 100.0% - F1:1.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 0.3929510712623596 - Accuracy: 77.77777910232544% - F1:0.7723665223665224%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 0.26424315571784973 - Accuracy: 92.5000011920929% - F1:0.9238289337474119%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 0.10591819882392883 - Accuracy: 94.87179517745972% - F1:0.9455677655677656%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.20941299200057983 - Accuracy: 93.99999976158142% - F1:0.9398632946001367%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.13559667766094208 - Accuracy: 97.56097793579102% - F1:0.975660253496945%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.17344428598880768 - Accuracy: 91.17646813392639% - F1:0.9141779788838612%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.6698862314224243 - Accuracy: 51.28205418586731% - F1:0.428204587597377%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.36998113989830017 - Accuracy: 83.33333134651184% - F1:0.8336986968700524%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.6913919448852539 - Accuracy: 77.14285850524902% - F1:0.7736214485794318%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.3006351590156555 - Accuracy: 91.4893627166748% - F1:0.9135340624702328%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 0.8325619697570801 - Accuracy: 78.94737124443054% - F1:0.7571379040486592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 0.6904008984565735 - Accuracy: 78.94737124443054% - F1:0.7593984962406015%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 0.15321296453475952 - Accuracy: 91.89189076423645% - F1:0.9158209592992202%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.24610383808612823 - Accuracy: 83.33333134651184% - F1:0.8021978021978022%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.17888247966766357 - Accuracy: 92.30769276618958% - F1:0.9192310957016838%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.06428536772727966 - Accuracy: 96.875% - F1:0.968671679197995%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 0.25123050808906555 - Accuracy: 89.47368264198303% - F1:0.8923444976076554%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 0.7706422209739685 - Accuracy: 69.2307710647583% - F1:0.673968253968254%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 46 - Loss: 0.46501624584198 - Accuracy: 81.39534592628479% - F1:0.7946940151186865%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 47 - Loss: 0.8734635710716248 - Accuracy: 68.57143044471741% - F1:0.7091036414565827%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 48 - Loss: 0.21899673342704773 - Accuracy: 93.75% - F1:0.9340909090909091%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 49 - Loss: 0.47353020310401917 - Accuracy: 76.92307829856873% - F1:0.7679556400784022%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 50 - Loss: 0.27374735474586487 - Accuracy: 91.66666865348816% - F1:0.9168615984405457%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 51 - Loss: 0.8739291429519653 - Accuracy: 73.8095223903656% - F1:0.7061478100011934%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 52 - Loss: 0.09523254632949829 - Accuracy: 100.0% - F1:1.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 81.41207866943799 (+- 11.299371140672143)\n",
      "> F1: 0.7997072280109182 (+- 0.1297801185129814)\n",
      "> Loss: 0.5490488464442583\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = train_SID\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/30_TF_FE_ACC_Only_balanced/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/30_TF_FE_ACC_Only_balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/30_TF_FE_ACC_Only_balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 889us/step - loss: 0.4662 - accuracy: 0.8452\n",
      "Test loss, Test acc: [0.46624189615249634, 0.8451613187789917]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=32)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135,   8,   2,   0],\n",
       "       [ 12,  82,  12,   9],\n",
       "       [  0,   0,  25,   0],\n",
       "       [  1,   1,   3,  20]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93103448, 0.05517241, 0.0137931 , 0.        ],\n",
       "       [0.10434783, 0.71304348, 0.10434783, 0.07826087],\n",
       "       [0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.04      , 0.04      , 0.12      , 0.8       ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Feature Engineered STEP balanced ACC Only - Confusion Matrix')\n",
    "plt.savefig('30_figures/30_feat_engin_window_ANN_ACC_Only_balanced.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.845 \n",
      "F1 Score: 0.846\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Feature Classification Model: ANN with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is composed of an artifical neural network classification model to evaluate if using features from windows of time (20 seconds with 10 second overlap), would generate a better model than our simple timepoint classifier. Leave-One-Group-Out (LOGO) Cross-Validation is used to validate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INPUT: .csv files containing the rolled sensor data with feature engineering (engineered_features.csv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT: Neural Network Multi-Classification Window Featuer Model (F1 Score = 0.829)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded dataset contains windows of data that are 20 seconds long with a 10 second overlap. These are stored as arrays in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('/Users/N1/Data7/Data-2020/10_code/40_usable_data_for_models/41_Duke_Data/engineered_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a window number that changes everytime there is a new activity present, as we wish to use this as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(count=df.groupby(df.Activity.ne(df.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>ACC1_skew</th>\n",
       "      <th>ACC2_skew</th>\n",
       "      <th>ACC3_skew</th>\n",
       "      <th>TEMP_skew</th>\n",
       "      <th>EDA_skew</th>\n",
       "      <th>BVP_skew</th>\n",
       "      <th>HR_skew</th>\n",
       "      <th>Magnitude_skew</th>\n",
       "      <th>ACC1_min</th>\n",
       "      <th>ACC2_min</th>\n",
       "      <th>ACC3_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>Magnitude_min</th>\n",
       "      <th>ACC1_max</th>\n",
       "      <th>ACC2_max</th>\n",
       "      <th>ACC3_max</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>Magnitude_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...</td>\n",
       "      <td>[27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...</td>\n",
       "      <td>[40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...</td>\n",
       "      <td>[32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.275354 0.276634 0.270231 0.270231 0.26895 0...</td>\n",
       "      <td>[15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...</td>\n",
       "      <td>[78.98 78.83500000000002 78.69 78.545 78.4 78....</td>\n",
       "      <td>[63.410093833710725 63.453053512025726 63.4961...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>32.350</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>-0.082592</td>\n",
       "      <td>-0.558848</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>-0.392823</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.531557</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.456522</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>69.7650</td>\n",
       "      <td>61.692787</td>\n",
       "      <td>41.543478</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>34.83</td>\n",
       "      <td>78.98</td>\n",
       "      <td>63.757353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0 39.06521739130435 39.130434782608695 39....</td>\n",
       "      <td>[29.0 28.93478260869565 28.869565217391305 28....</td>\n",
       "      <td>[38.0 38.02173913043478 38.04347826086956 38.0...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.25998499999999997 0.25998499999999997 0.258...</td>\n",
       "      <td>[-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...</td>\n",
       "      <td>[73.52 73.435 73.35 73.265 73.18 73.0925 73.00...</td>\n",
       "      <td>[61.69278726074872 61.7168170027034 61.7409828...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>32.339</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.170923</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>64.8025</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>37.72</td>\n",
       "      <td>73.52</td>\n",
       "      <td>62.936476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392 41.67391304347826 41.739130...</td>\n",
       "      <td>[26.39130434782609 26.32608695652174 26.260869...</td>\n",
       "      <td>[38.869565217391305 38.89130434782609 38.91304...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.265108 0.263827 0.266389 0.265108 0.266389 ...</td>\n",
       "      <td>[-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...</td>\n",
       "      <td>[69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...</td>\n",
       "      <td>[62.758486272725364 62.78782873035957 62.81730...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>32.337</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>0.227966</td>\n",
       "      <td>1.329886</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>-0.274279</td>\n",
       "      <td>0.185657</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.252301</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>60.9950</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>45.532258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>47.14</td>\n",
       "      <td>69.63</td>\n",
       "      <td>64.010791</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575 44.14285714285715 44.31428...</td>\n",
       "      <td>[24.514285714285712 24.42857142857143 24.34285...</td>\n",
       "      <td>[37.17142857142857 37.142857142857146 37.11428...</td>\n",
       "      <td>[32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.258704 0.258704 0.258704 0.257424 0.257424 ...</td>\n",
       "      <td>[17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....</td>\n",
       "      <td>[64.68 64.555 64.43 64.305 64.18 64.0475 63.91...</td>\n",
       "      <td>[62.579164557660036 62.649331804179724 62.7200...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>-4.941414</td>\n",
       "      <td>-1.040349</td>\n",
       "      <td>3.435987</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>-0.828441</td>\n",
       "      <td>0.406263</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.985816</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.8025</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>47.14</td>\n",
       "      <td>64.68</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418 45.564516129032256 45.58064...</td>\n",
       "      <td>[25.64516129032258 25.69354838709677 25.741935...</td>\n",
       "      <td>[37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.253581 0.253581 0.253581 0.252301 0.252301 ...</td>\n",
       "      <td>[-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...</td>\n",
       "      <td>[60.92 60.8475 60.77500000000001 60.7025 60.63...</td>\n",
       "      <td>[64.04162603123257 64.07248675362091 64.103373...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>32.389</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-1.857224</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>1.380509</td>\n",
       "      <td>-0.686481</td>\n",
       "      <td>1.239716</td>\n",
       "      <td>-0.833856</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.408229</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.702128</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.249739</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.5300</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.43</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>39.00</td>\n",
       "      <td>60.92</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...   \n",
       "1  [39.0 39.06521739130435 39.130434782608695 39....   \n",
       "2  [41.60869565217392 41.67391304347826 41.739130...   \n",
       "3  [43.971428571428575 44.14285714285715 44.31428...   \n",
       "4  [45.54838709677418 45.564516129032256 45.58064...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...   \n",
       "1  [29.0 28.93478260869565 28.869565217391305 28....   \n",
       "2  [26.39130434782609 26.32608695652174 26.260869...   \n",
       "3  [24.514285714285712 24.42857142857143 24.34285...   \n",
       "4  [25.64516129032258 25.69354838709677 25.741935...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...   \n",
       "1  [38.0 38.02173913043478 38.04347826086956 38.0...   \n",
       "2  [38.869565217391305 38.89130434782609 38.91304...   \n",
       "3  [37.17142857142857 37.142857142857146 37.11428...   \n",
       "4  [37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....   \n",
       "1  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "2  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "3  [32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....   \n",
       "4  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.275354 0.276634 0.270231 0.270231 0.26895 0...   \n",
       "1  [0.25998499999999997 0.25998499999999997 0.258...   \n",
       "2  [0.265108 0.263827 0.266389 0.265108 0.266389 ...   \n",
       "3  [0.258704 0.258704 0.258704 0.257424 0.257424 ...   \n",
       "4  [0.253581 0.253581 0.253581 0.252301 0.252301 ...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...   \n",
       "1  [-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...   \n",
       "2  [-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...   \n",
       "3  [17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....   \n",
       "4  [-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [78.98 78.83500000000002 78.69 78.545 78.4 78....   \n",
       "1  [73.52 73.435 73.35 73.265 73.18 73.0925 73.00...   \n",
       "2  [69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...   \n",
       "3  [64.68 64.555 64.43 64.305 64.18 64.0475 63.91...   \n",
       "4  [60.92 60.8475 60.77500000000001 60.7025 60.63...   \n",
       "\n",
       "                                           Magnitude Subject_ID  Activity  \\\n",
       "0  [63.410093833710725 63.453053512025726 63.4961...     19-001  Baseline   \n",
       "1  [61.69278726074872 61.7168170027034 61.7409828...     19-001  Baseline   \n",
       "2  [62.758486272725364 62.78782873035957 62.81730...     19-001  Baseline   \n",
       "3  [62.579164557660036 62.649331804179724 62.7200...     19-001  Baseline   \n",
       "4  [64.04162603123257 64.07248675362091 64.103373...     19-001  Baseline   \n",
       "\n",
       "   Round  ACC1_mean  ACC2_mean  ACC3_mean  TEMP_mean  EDA_mean  BVP_mean  \\\n",
       "0      1  40.248370  28.012880  38.824457     32.350  0.262354 -0.109875   \n",
       "1      1  40.820000  26.815000  38.192500     32.339  0.261058  0.321375   \n",
       "2      1  43.252235  25.312684  37.488043     32.337  0.259585  0.684000   \n",
       "3      1  44.905798  24.915984  37.638218     32.356  0.254510 -0.180875   \n",
       "4      1  43.577055  22.974382  38.971144     32.389  0.252733 -0.209750   \n",
       "\n",
       "     HR_mean  Magnitude_mean  ACC1_std  ACC2_std  ACC3_std  TEMP_std  \\\n",
       "0  73.931187       62.553853  0.701573  0.687590  0.632616  0.017607   \n",
       "1  69.481750       62.021872  1.192214  1.149559  0.529382  0.012610   \n",
       "2  64.893188       62.621785  2.109896  0.815025  0.647914  0.010536   \n",
       "3  61.157687       63.734171  1.832017  1.509593  1.773398  0.025377   \n",
       "4  59.226438       62.913435  2.115371  2.585687  1.809092  0.027000   \n",
       "\n",
       "    EDA_std    BVP_std    HR_std  Magnitude_std  ACC1_skew  ACC2_skew  \\\n",
       "0  0.004877  18.439453  2.574676       0.609756  -0.082592  -0.558848   \n",
       "1  0.003007  20.104717  2.608254       0.542348   0.515544   0.109446   \n",
       "2  0.004337  23.756276  2.639037       0.942868  -0.473020   0.227966   \n",
       "3  0.002396  25.635645  1.674001       0.841361  -4.941414  -1.040349   \n",
       "4  0.002055  25.593597  0.684349       1.365652  -1.857224   0.511935   \n",
       "\n",
       "   ACC3_skew  TEMP_skew  EDA_skew  BVP_skew   HR_skew  Magnitude_skew  \\\n",
       "0   0.705668   0.714533  0.896382 -0.392823  0.296262        0.531557   \n",
       "1  -0.188071   0.787066  0.212943 -0.322900 -0.170923       -0.438037   \n",
       "2   1.329886   0.620801  0.072564 -0.274279  0.185657       -0.382833   \n",
       "3   3.435987   0.672586  0.734482 -0.828441  0.406263       -0.532117   \n",
       "4   1.380509  -0.686481  1.239716 -0.833856  0.996232        0.408229   \n",
       "\n",
       "   ACC1_min   ACC2_min  ACC3_min  TEMP_min   EDA_min  BVP_min   HR_min  \\\n",
       "0      39.0  26.456522      38.0     32.33  0.254862   -42.99  69.7650   \n",
       "1      39.0  24.600000      37.2     32.31  0.254862   -48.52  64.8025   \n",
       "2      39.0  24.000000      37.0     32.31  0.252301   -48.52  60.9950   \n",
       "3      32.0  20.985816      37.0     32.33  0.251020  -101.74  58.8025   \n",
       "4      32.0  20.702128      37.0     32.33  0.249739  -101.74  58.5300   \n",
       "\n",
       "   Magnitude_min   ACC1_max  ACC2_max  ACC3_max  TEMP_max   EDA_max  BVP_max  \\\n",
       "0      61.692787  41.543478      29.0      40.0     32.39  0.276634    34.83   \n",
       "1      60.778286  43.800000      29.0      39.0     32.37  0.266389    37.72   \n",
       "2      60.778286  45.532258      27.0      39.0     32.37  0.266389    47.14   \n",
       "3      61.392182  46.000000      27.0      48.0     32.41  0.262546    47.14   \n",
       "4      61.392182  46.000000      27.0      48.0     32.43  0.258704    39.00   \n",
       "\n",
       "   HR_max  Magnitude_max  count  \n",
       "0   78.98      63.757353      1  \n",
       "1   73.52      62.936476      2  \n",
       "2   69.63      64.010791      3  \n",
       "3   64.68      65.711491      4  \n",
       "4   60.92      65.711491      5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These will be our Subjects in our test set: [39 17 45]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rands = np.random.choice(df.Subject_ID.unique(),3, replace=False)\n",
    "print(f' These will be our Subjects in our test set: {rands}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Subjects into Test and Train Sets (n=44,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Subject_ID'].isin(rands)] \n",
    "train = df[-df['Subject_ID'].isin(rands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std',\n",
    "       'EDA_std', 'BVP_std', 'HR_std', 'Subject_ID', 'count', 'Activity']]   \n",
    "test = test[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std',\n",
    "       'EDA_std', 'BVP_std', 'HR_std', 'Subject_ID', 'count', 'Activity']]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train_SID is made so we can use the Subject_ID values to perform LOGO later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2929\n",
       "1    2323\n",
       "3     505\n",
       "2     505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n",
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = train[train['Activity'] == 0]\n",
    "one = train[train['Activity'] == 1]\n",
    "two = train[train['Activity'] == 2]\n",
    "three =train[train['Activity'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = zero.sample(505)\n",
    "one = one.sample(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([zero, one, two, three])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    505\n",
       "2    505\n",
       "1    505\n",
       "0    505\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SID = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID and window count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject_ID and window count must be one-hot encoded to be used as features in our model. Test and train dataframes must be concatenated before we one-hot encode, so that we do not get different encodings for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] =1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'], prefix = 'SID')], axis =1).drop('Subject_ID', axis =1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['count'], prefix = 'count')], axis =1).drop('count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 122) (310, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/N1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop([\"train\"], axis = 1, inplace = True)\n",
    "test.drop([\"train\"], axis = 1, inplace = True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove activity from our train and test datasets as this is the y variable (target variable) and we are only interested in keeping the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train.drop(\"Activity\", axis =1)\n",
    "test_f = test.drop(\"Activity\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X (features) and y (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_f\n",
    "y_train = train.Activity\n",
    "X_test = test_f\n",
    "y_test = test.Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:7] = sc.fit_transform(X_train.iloc[:,:7])\n",
    "X_test.iloc[:,:7] = sc.transform(X_test.iloc[:,:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "- 6 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training aâ€ºnd validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3697 - accuracy: 0.3022\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3130 - accuracy: 0.3845\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2598 - accuracy: 0.4250\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1639 - accuracy: 0.5043\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0002 - accuracy: 0.5937: 0s - loss: 1.0175 - accuracy: \n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8109 - accuracy: 0.6817\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7372 - accuracy: 0.7130\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.7448\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6347 - accuracy: 0.7585\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7696\n",
      "Score for fold 1: loss of 0.8619233965873718; accuracy of 75.60975551605225%, F1 of 0.7471931862175765\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3876 - accuracy: 0.2679\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3610 - accuracy: 0.3463\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2914 - accuracy: 0.4171\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1414 - accuracy: 0.5233\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9717 - accuracy: 0.6173\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8600 - accuracy: 0.6537\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7981 - accuracy: 0.6906\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7459 - accuracy: 0.7073\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7195 - accuracy: 0.7300\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6769 - accuracy: 0.7533\n",
      "Score for fold 2: loss of 0.8652870655059814; accuracy of 61.90476417541504%, F1 of 0.5715216682958618\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3829 - accuracy: 0.2735\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3561 - accuracy: 0.3502\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2775 - accuracy: 0.4168\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1577 - accuracy: 0.5076\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9776 - accuracy: 0.6231\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8343 - accuracy: 0.6771\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7566 - accuracy: 0.7175\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.7447\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.7588\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.7725\n",
      "Score for fold 3: loss of 1.2575099468231201; accuracy of 47.36842215061188%, F1 of 0.48728602883727534\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3834 - accuracy: 0.2804\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3439 - accuracy: 0.3517\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2792 - accuracy: 0.4175\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.1998 - accuracy: 0.4706\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0980 - accuracy: 0.5440\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.9249 - accuracy: 0.6417\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7994 - accuracy: 0.6959\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7309 - accuracy: 0.7191\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6768 - accuracy: 0.7530\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6501 - accuracy: 0.7662\n",
      "Score for fold 4: loss of 0.9983530640602112; accuracy of 65.90909361839294%, F1 of 0.6603970741901776\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3742 - accuracy: 0.2857\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3257 - accuracy: 0.3744\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2603 - accuracy: 0.4250\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1970 - accuracy: 0.4807\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0989 - accuracy: 0.5395\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9770 - accuracy: 0.5892\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8493 - accuracy: 0.6591\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7655 - accuracy: 0.6960\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7174 - accuracy: 0.7270\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.7579\n",
      "Score for fold 5: loss of 0.9873672127723694; accuracy of 58.69565010070801%, F1 of 0.576923076923077\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3809 - accuracy: 0.2944\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3394 - accuracy: 0.3626\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2639 - accuracy: 0.4328\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1575 - accuracy: 0.5146\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.9973 - accuracy: 0.6101\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8479 - accuracy: 0.6778\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7568 - accuracy: 0.7045\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7042 - accuracy: 0.7278\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.7636\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.7747\n",
      "Score for fold 6: loss of 1.127271056175232; accuracy of 52.49999761581421%, F1 of 0.5019157088122606\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3729 - accuracy: 0.3180\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3112 - accuracy: 0.3806\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2282 - accuracy: 0.4730\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1075 - accuracy: 0.5381\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9249 - accuracy: 0.6431\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8006 - accuracy: 0.6890\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7315 - accuracy: 0.7294\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6757 - accuracy: 0.7350\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.7506\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.7779\n",
      "Score for fold 7: loss of 1.7299988269805908; accuracy of 41.025641560554504%, F1 of 0.26832426832426837\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3777 - accuracy: 0.2958\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3170 - accuracy: 0.3867\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2212 - accuracy: 0.4654\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1177 - accuracy: 0.5386\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9527 - accuracy: 0.6189\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8139 - accuracy: 0.6749\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.7072\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6821 - accuracy: 0.7314\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.7582\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.7521\n",
      "Score for fold 8: loss of 1.0068931579589844; accuracy of 51.28205418586731%, F1 of 0.4627084614677666\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3735 - accuracy: 0.2979\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3240 - accuracy: 0.4041\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2686 - accuracy: 0.4434\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1738 - accuracy: 0.4952\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0767 - accuracy: 0.5692\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9286 - accuracy: 0.6412\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8034 - accuracy: 0.6905\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7418 - accuracy: 0.7146\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7049 - accuracy: 0.7368\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6597 - accuracy: 0.7524\n",
      "Score for fold 9: loss of 1.0127556324005127; accuracy of 51.51515007019043%, F1 of 0.5184448725118582\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3723 - accuracy: 0.3110\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3407 - accuracy: 0.3921\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2703 - accuracy: 0.4189\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1658 - accuracy: 0.4909\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0261 - accuracy: 0.5741\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8574 - accuracy: 0.6643\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7841 - accuracy: 0.6905\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7312 - accuracy: 0.7182\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.7389\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.7560\n",
      "Score for fold 10: loss of 0.5831376314163208; accuracy of 72.22222089767456%, F1 of 0.7226170568561874\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3799 - accuracy: 0.2700\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3447 - accuracy: 0.3773\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2970 - accuracy: 0.3929\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2200 - accuracy: 0.4574\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1232 - accuracy: 0.5134\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9734 - accuracy: 0.5965\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8315 - accuracy: 0.6776\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7460 - accuracy: 0.7093\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.7340\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.7567\n",
      "Score for fold 11: loss of 1.2808054685592651; accuracy of 51.428574323654175%, F1 of 0.46231826063758835\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3789 - accuracy: 0.2815\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3452 - accuracy: 0.3691\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2781 - accuracy: 0.4035\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1939 - accuracy: 0.4820\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0805 - accuracy: 0.5489\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9276 - accuracy: 0.6486\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8327 - accuracy: 0.6830\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7480 - accuracy: 0.7139\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.7539\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.7494\n",
      "Score for fold 12: loss of 1.129085898399353; accuracy of 51.11111402511597%, F1 of 0.47190566635011083\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3802 - accuracy: 0.2689\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3313 - accuracy: 0.3986\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2267 - accuracy: 0.4753\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0488 - accuracy: 0.5691\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8819 - accuracy: 0.6342\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8143 - accuracy: 0.6705\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7691 - accuracy: 0.6922\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7026 - accuracy: 0.7351\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.7462\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.7558\n",
      "Score for fold 13: loss of 1.3309719562530518; accuracy of 42.105263471603394%, F1 of 0.40797213622291023\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3854 - accuracy: 0.2528\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3642 - accuracy: 0.3184\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3071 - accuracy: 0.3971\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2338 - accuracy: 0.4304\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1326 - accuracy: 0.5217\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9957 - accuracy: 0.5873\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8254 - accuracy: 0.6953\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7079 - accuracy: 0.7462\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.7654\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.7871\n",
      "Score for fold 14: loss of 2.5056209564208984; accuracy of 42.105263471603394%, F1 of 0.40869534948482317\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3766 - accuracy: 0.2989\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3142 - accuracy: 0.3632\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2397 - accuracy: 0.4271\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1756 - accuracy: 0.4645\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1114 - accuracy: 0.5142\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0558 - accuracy: 0.5532\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9506 - accuracy: 0.6348\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8585 - accuracy: 0.6854\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7175 - accuracy: 0.7391\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.7563\n",
      "Score for fold 15: loss of 1.7522836923599243; accuracy of 32.608696818351746%, F1 of 0.20473890039107429\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3821 - accuracy: 0.2872\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3450 - accuracy: 0.3638\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2521 - accuracy: 0.4414\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1590 - accuracy: 0.4972\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0679 - accuracy: 0.5632\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9276 - accuracy: 0.6489\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8289 - accuracy: 0.6839\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.7387\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.7661\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.7829\n",
      "Score for fold 16: loss of 1.4388662576675415; accuracy of 53.06122303009033%, F1 of 0.47602703419029946\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3767 - accuracy: 0.2877\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3222 - accuracy: 0.3635\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2489 - accuracy: 0.4196\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1919 - accuracy: 0.4651\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1334 - accuracy: 0.5061\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.0700 - accuracy: 0.5404\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9747 - accuracy: 0.5981\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8575 - accuracy: 0.6608\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7476 - accuracy: 0.7255\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.7467\n",
      "Score for fold 17: loss of 1.0934953689575195; accuracy of 66.66666865348816%, F1 of 0.6521494378637236\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3784 - accuracy: 0.2918\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3259 - accuracy: 0.3747\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2464 - accuracy: 0.4381\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1666 - accuracy: 0.4879\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0834 - accuracy: 0.5458\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9282 - accuracy: 0.6328\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7712 - accuracy: 0.7002\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.7354\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7636\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6128 - accuracy: 0.7646\n",
      "Score for fold 18: loss of 3.635305881500244; accuracy of 34.375%, F1 of 0.21255060728744937\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3810 - accuracy: 0.2823\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3398 - accuracy: 0.3684\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2695 - accuracy: 0.4289\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1519 - accuracy: 0.5071\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0071 - accuracy: 0.6053\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8529 - accuracy: 0.6739\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7750 - accuracy: 0.7011\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7111 - accuracy: 0.7349\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6654 - accuracy: 0.7550\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6309 - accuracy: 0.7717\n",
      "Score for fold 19: loss of 0.5722720623016357; accuracy of 80.55555820465088%, F1 of 0.808452290145654\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3844 - accuracy: 0.2690\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3391 - accuracy: 0.3732\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.2482 - accuracy: 0.4364\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.1649 - accuracy: 0.4840\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0678 - accuracy: 0.5621\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9799 - accuracy: 0.6137\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8318 - accuracy: 0.6909\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7250 - accuracy: 0.7295\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.7435\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6439 - accuracy: 0.7575\n",
      "Score for fold 20: loss of 1.1605077981948853; accuracy of 37.5%, F1 of 0.2886650386650387\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3778 - accuracy: 0.2881\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3349 - accuracy: 0.3739\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2544 - accuracy: 0.4284\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1436 - accuracy: 0.5111\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0251 - accuracy: 0.5711\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8968 - accuracy: 0.6433\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7994 - accuracy: 0.6826\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7615 - accuracy: 0.7038\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7167 - accuracy: 0.7240\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6639 - accuracy: 0.7563\n",
      "Score for fold 21: loss of 0.8651168942451477; accuracy of 60.52631735801697%, F1 of 0.5335497835497837\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3736 - accuracy: 0.2945\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3447 - accuracy: 0.3583\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2755 - accuracy: 0.4216\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1206 - accuracy: 0.5369\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9470 - accuracy: 0.6113\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8243 - accuracy: 0.6731\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7548 - accuracy: 0.7029\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7128 - accuracy: 0.7110\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6664 - accuracy: 0.7384\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6318 - accuracy: 0.7571\n",
      "Score for fold 22: loss of 1.180909514427185; accuracy of 50.0%, F1 of 0.4283046683046683\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3746 - accuracy: 0.3097\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3042 - accuracy: 0.3893\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2344 - accuracy: 0.4339\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1761 - accuracy: 0.4795\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0853 - accuracy: 0.5398\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9615 - accuracy: 0.6123\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8177 - accuracy: 0.6883\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7320 - accuracy: 0.7060\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6811 - accuracy: 0.7425\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6537 - accuracy: 0.7547\n",
      "Score for fold 23: loss of 0.8399780988693237; accuracy of 59.574466943740845%, F1 of 0.5582573454913881\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3716 - accuracy: 0.2997\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3062 - accuracy: 0.3808\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2237 - accuracy: 0.4574\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1557 - accuracy: 0.5076\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0892 - accuracy: 0.5558\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9934 - accuracy: 0.6207\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8212 - accuracy: 0.6876\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7225 - accuracy: 0.7241\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 0.7510\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6216 - accuracy: 0.7591\n",
      "Score for fold 24: loss of 0.9057102203369141; accuracy of 62.5%, F1 of 0.6187254901960785\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3811 - accuracy: 0.2674\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3598 - accuracy: 0.3443\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3112 - accuracy: 0.4267\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2413 - accuracy: 0.4676\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.1117 - accuracy: 0.5480\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.9490 - accuracy: 0.6350\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8136 - accuracy: 0.6997\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7203 - accuracy: 0.7159\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6487 - accuracy: 0.7578\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.7816\n",
      "Score for fold 25: loss of 0.7090939879417419; accuracy of 71.42857313156128%, F1 of 0.688754553204681\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3855 - accuracy: 0.2634\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3550 - accuracy: 0.3478\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2890 - accuracy: 0.4232\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2091 - accuracy: 0.4808\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1357 - accuracy: 0.5162\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0447 - accuracy: 0.5723\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8763 - accuracy: 0.6623\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7455 - accuracy: 0.7305\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.7462\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6466 - accuracy: 0.7543\n",
      "Score for fold 26: loss of 0.6111114025115967; accuracy of 71.42857313156128%, F1 of 0.716468253968254\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3845 - accuracy: 0.2465\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3478 - accuracy: 0.3364\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.2579 - accuracy: 0.4187\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1730 - accuracy: 0.5015\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0374 - accuracy: 0.5859\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8655 - accuracy: 0.6727\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7807 - accuracy: 0.6970\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.7348\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.7470\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.7737\n",
      "Score for fold 27: loss of 1.3416242599487305; accuracy of 55.000001192092896%, F1 of 0.4836688311688312\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.3757 - accuracy: 0.3007\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3252 - accuracy: 0.3616\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2520 - accuracy: 0.4321\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1856 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0671 - accuracy: 0.5859\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8799 - accuracy: 0.6733\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7928 - accuracy: 0.7038\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7128 - accuracy: 0.7383\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.7572\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6274 - accuracy: 0.7692\n",
      "Score for fold 28: loss of 0.8130879402160645; accuracy of 72.22222089767456%, F1 of 0.6534900284900285\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3814 - accuracy: 0.2914\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3507 - accuracy: 0.3556\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2816 - accuracy: 0.4384\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1540 - accuracy: 0.5106\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9703 - accuracy: 0.6247\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8424 - accuracy: 0.6646\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7617 - accuracy: 0.7091\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6998 - accuracy: 0.7278\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.7490\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6261 - accuracy: 0.7596\n",
      "Score for fold 29: loss of 1.030587077140808; accuracy of 60.00000238418579%, F1 of 0.5699096225412015\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3855 - accuracy: 0.2655\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3488 - accuracy: 0.3746\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2713 - accuracy: 0.4154\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1220 - accuracy: 0.5331\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9126 - accuracy: 0.6562\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7830 - accuracy: 0.6956\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7351 - accuracy: 0.7183: 0s - loss: 0.7392 - accuracy: \n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.7426\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6553 - accuracy: 0.7572\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6026 - accuracy: 0.7850\n",
      "Score for fold 30: loss of 1.18007493019104; accuracy of 56.41025900840759%, F1 of 0.5072804972804972\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3704 - accuracy: 0.2985\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3169 - accuracy: 0.3751: 0s - loss: 1.3248 - accuracy: \n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2687 - accuracy: 0.4193\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1893 - accuracy: 0.4746\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0640 - accuracy: 0.5563\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9041 - accuracy: 0.6457\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7814 - accuracy: 0.7041\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7105 - accuracy: 0.7264\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6739 - accuracy: 0.7548\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6214 - accuracy: 0.7624\n",
      "Score for fold 31: loss of 1.1475070714950562; accuracy of 66.00000262260437%, F1 of 0.6586090162560749\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3702 - accuracy: 0.3047\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3159 - accuracy: 0.3855\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2570 - accuracy: 0.4224\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1724 - accuracy: 0.4937\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0722 - accuracy: 0.5619\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9109 - accuracy: 0.6675\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7837 - accuracy: 0.7231\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7025 - accuracy: 0.7453\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.7569\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.7782\n",
      "Score for fold 32: loss of 0.9636569023132324; accuracy of 58.53658318519592%, F1 of 0.5497233411535567\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3748 - accuracy: 0.2951\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3174 - accuracy: 0.3807\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2351 - accuracy: 0.4612\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1342 - accuracy: 0.5297\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9395 - accuracy: 0.6219\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8169 - accuracy: 0.6707\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7636 - accuracy: 0.7064\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.7016 - accuracy: 0.7271\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.7462\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6510 - accuracy: 0.7417\n",
      "Score for fold 33: loss of 2.053558826446533; accuracy of 41.17647111415863%, F1 of 0.35656108597285074\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3876 - accuracy: 0.2716\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3581 - accuracy: 0.3690\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2574 - accuracy: 0.4462\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0773 - accuracy: 0.5548\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8934 - accuracy: 0.6386\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7960 - accuracy: 0.6956\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.7258 - accuracy: 0.7163\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6732 - accuracy: 0.7385\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.7633\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.5966 - accuracy: 0.7789\n",
      "Score for fold 34: loss of 0.8705970644950867; accuracy of 58.974361419677734%, F1 of 0.5694207027540361\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3850 - accuracy: 0.2674\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3592 - accuracy: 0.3539\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2620 - accuracy: 0.4307: 0s - loss: 1.2910 - accuracy: \n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1016 - accuracy: 0.5561\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9102 - accuracy: 0.6355\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8239 - accuracy: 0.6739\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7483 - accuracy: 0.7139\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7110 - accuracy: 0.7224\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6621 - accuracy: 0.7346\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.7604\n",
      "Score for fold 35: loss of 1.401336908340454; accuracy of 61.90476417541504%, F1 of 0.5711598746081504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3774 - accuracy: 0.2846\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3246 - accuracy: 0.3602\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2510 - accuracy: 0.4277\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1750 - accuracy: 0.4917\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0584 - accuracy: 0.5567: 0s - loss: 1.1175 - accuracy: \n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9055 - accuracy: 0.6529\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7956 - accuracy: 0.6922\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7064 - accuracy: 0.7320\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.7516\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.7557\n",
      "Score for fold 36: loss of 0.6901620626449585; accuracy of 74.28571581840515%, F1 of 0.7368796992481204\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3780 - accuracy: 0.2924\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3361 - accuracy: 0.3781\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2541 - accuracy: 0.4222\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1505 - accuracy: 0.5124\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9794 - accuracy: 0.6143\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8284 - accuracy: 0.6766\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7510 - accuracy: 0.7086\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.7456\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.7582\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.7734\n",
      "Score for fold 37: loss of 0.7670972943305969; accuracy of 68.08510422706604%, F1 of 0.664998516062346\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3830 - accuracy: 0.2573\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3376 - accuracy: 0.3678\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2600 - accuracy: 0.4420\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1717 - accuracy: 0.4970\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0951 - accuracy: 0.5439\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9242 - accuracy: 0.6297\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8199 - accuracy: 0.6816\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7494 - accuracy: 0.7104\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.7291\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.7442\n",
      "Score for fold 38: loss of 1.4391573667526245; accuracy of 39.47368562221527%, F1 of 0.36532507739938075\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3769 - accuracy: 0.2987\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3280 - accuracy: 0.3592\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2483 - accuracy: 0.4193\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1740 - accuracy: 0.4738\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0990 - accuracy: 0.5156\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.9971 - accuracy: 0.5827\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.8780 - accuracy: 0.6514\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7414 - accuracy: 0.7255\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.7553\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6153 - accuracy: 0.7709\n",
      "Score for fold 39: loss of 1.3751965761184692; accuracy of 42.105263471603394%, F1 of 0.37783887938686705\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3754 - accuracy: 0.2940\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3402 - accuracy: 0.3722: 0s - loss: 1.3495 - accuracy: \n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2595 - accuracy: 0.4539\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1330 - accuracy: 0.5250\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.9433 - accuracy: 0.6228\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8243 - accuracy: 0.6808\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7486 - accuracy: 0.7191\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7219 - accuracy: 0.7252\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6637 - accuracy: 0.7534\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6347 - accuracy: 0.7615\n",
      "Score for fold 40: loss of 0.40041324496269226; accuracy of 91.89189076423645%, F1 of 0.9175032175032175\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3814 - accuracy: 0.2816\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3508 - accuracy: 0.3670\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2711 - accuracy: 0.4317\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1589 - accuracy: 0.5273\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0118 - accuracy: 0.5930\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8699 - accuracy: 0.6587\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7806 - accuracy: 0.6860\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7276 - accuracy: 0.7194\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6775 - accuracy: 0.7386\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.7381\n",
      "Score for fold 41: loss of 0.9670069813728333; accuracy of 59.52380895614624%, F1 of 0.5182714851645243\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3816 - accuracy: 0.2797\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3263 - accuracy: 0.3847\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2227 - accuracy: 0.4659\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1069 - accuracy: 0.5452\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9791 - accuracy: 0.5866\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8691 - accuracy: 0.6254\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.8088 - accuracy: 0.6774\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7603 - accuracy: 0.6926\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.7108\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6759 - accuracy: 0.7355\n",
      "Score for fold 42: loss of 0.5547577738761902; accuracy of 82.05128312110901%, F1 of 0.8141858141858143\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3775 - accuracy: 0.2862\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3173 - accuracy: 0.3858\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.2304 - accuracy: 0.4502: 0s - loss: 1.2484 - accuracy: \n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1437 - accuracy: 0.5181\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0149 - accuracy: 0.5926\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8571 - accuracy: 0.6600\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7624 - accuracy: 0.6911\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7199 - accuracy: 0.7213\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.7445\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6512 - accuracy: 0.7465\n",
      "Score for fold 43: loss of 0.9135024547576904; accuracy of 53.125%, F1 of 0.5506771618903972\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3862 - accuracy: 0.2704\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3415 - accuracy: 0.3461\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2663 - accuracy: 0.4178\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1957 - accuracy: 0.4717\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0867 - accuracy: 0.5429\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.9049 - accuracy: 0.6453: 0s - loss: 0.9594 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7640 - accuracy: 0.7104\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7024 - accuracy: 0.7397\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6590 - accuracy: 0.7578\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.7750\n",
      "Score for fold 44: loss of 1.2223490476608276; accuracy of 52.63158082962036%, F1 of 0.47139001349527665\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3788 - accuracy: 0.2887\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3408 - accuracy: 0.3589\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - ETA: 0s - loss: 1.2782 - accuracy: 0.40 - 0s 3ms/step - loss: 1.2752 - accuracy: 0.4059\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2025 - accuracy: 0.4740\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.1509 - accuracy: 0.5290\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0658 - accuracy: 0.5714\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9363 - accuracy: 0.6547\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8140 - accuracy: 0.7092\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7239 - accuracy: 0.7294\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.7501\n",
      "Score for fold 45: loss of 0.8640475273132324; accuracy of 61.538463830947876%, F1 of 0.5622477650063857\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 46 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3767 - accuracy: 0.2812\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3430 - accuracy: 0.3637\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3034 - accuracy: 0.4178\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2156 - accuracy: 0.4906\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.0558 - accuracy: 0.5746\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8907 - accuracy: 0.6292\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.8081 - accuracy: 0.6778\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7574 - accuracy: 0.7021\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6803 - accuracy: 0.7355\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6580 - accuracy: 0.7547\n",
      "Score for fold 46: loss of 1.614271879196167; accuracy of 44.186046719551086%, F1 of 0.36071163384715915\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 47 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3837 - accuracy: 0.2700\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3588 - accuracy: 0.3370\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3018 - accuracy: 0.4176\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2102 - accuracy: 0.4851\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1444 - accuracy: 0.5134\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0180 - accuracy: 0.5975\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9052 - accuracy: 0.6348\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8218 - accuracy: 0.6771\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7754 - accuracy: 0.6957\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7162 - accuracy: 0.7194\n",
      "Score for fold 47: loss of 0.6341947317123413; accuracy of 77.14285850524902%, F1 of 0.7711111111111112\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 48 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3831 - accuracy: 0.2641\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3542 - accuracy: 0.3461\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.2875 - accuracy: 0.4195\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.1812 - accuracy: 0.5045\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0933 - accuracy: 0.5584\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9420 - accuracy: 0.6353\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.8235 - accuracy: 0.6740\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7637 - accuracy: 0.7057\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7138 - accuracy: 0.7339\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6594 - accuracy: 0.7550\n",
      "Score for fold 48: loss of 0.6146137714385986; accuracy of 78.125%, F1 of 0.7768308080808081\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 49 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3833 - accuracy: 0.2665\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3496 - accuracy: 0.3412\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2693 - accuracy: 0.4124\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.1753 - accuracy: 0.4871\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.0504 - accuracy: 0.5623\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8918 - accuracy: 0.6492\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7961 - accuracy: 0.6916\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.7350\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.7506\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.7718\n",
      "Score for fold 49: loss of 2.0752573013305664; accuracy of 41.025641560554504%, F1 of 0.3146853146853147\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 50 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.3835 - accuracy: 0.2782\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.3222 - accuracy: 0.4148\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.2091 - accuracy: 0.4919\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 1.0387 - accuracy: 0.5832\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8687 - accuracy: 0.6557\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.8029 - accuracy: 0.6754\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 2ms/step - loss: 0.7397 - accuracy: 0.7172\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7015 - accuracy: 0.7283\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6511 - accuracy: 0.7566\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.7752\n",
      "Score for fold 50: loss of 0.5086686611175537; accuracy of 86.11111044883728%, F1 of 0.8465166539537249\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 51 ...\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3809 - accuracy: 0.2877\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.3443 - accuracy: 0.3964\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 1.2649 - accuracy: 0.4772\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1088 - accuracy: 0.5597\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.9014 - accuracy: 0.6380\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.8195 - accuracy: 0.6775\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.7629 - accuracy: 0.7078\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.7280\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.7417\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.7604\n",
      "Score for fold 51: loss of 0.784246563911438; accuracy of 69.04761791229248%, F1 of 0.6630965078170047\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 52 ...\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3790 - accuracy: 0.2831\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3163 - accuracy: 0.3682\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2342 - accuracy: 0.4447\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1823 - accuracy: 0.4882\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.1229 - accuracy: 0.5333\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.9995 - accuracy: 0.6038\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8620 - accuracy: 0.6738\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.7674 - accuracy: 0.7069\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7098 - accuracy: 0.7224\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.7284\n",
      "Score for fold 52: loss of 1.3768973350524902; accuracy of 47.61904776096344%, F1 of 0.40500240500240503\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8619233965873718 - Accuracy: 75.60975551605225% - F1:0.7471931862175765%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.8652870655059814 - Accuracy: 61.90476417541504% - F1:0.5715216682958618%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.2575099468231201 - Accuracy: 47.36842215061188% - F1:0.48728602883727534%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.9983530640602112 - Accuracy: 65.90909361839294% - F1:0.6603970741901776%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.9873672127723694 - Accuracy: 58.69565010070801% - F1:0.576923076923077%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.127271056175232 - Accuracy: 52.49999761581421% - F1:0.5019157088122606%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.7299988269805908 - Accuracy: 41.025641560554504% - F1:0.26832426832426837%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.0068931579589844 - Accuracy: 51.28205418586731% - F1:0.4627084614677666%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.0127556324005127 - Accuracy: 51.51515007019043% - F1:0.5184448725118582%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.5831376314163208 - Accuracy: 72.22222089767456% - F1:0.7226170568561874%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 1.2808054685592651 - Accuracy: 51.428574323654175% - F1:0.46231826063758835%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.129085898399353 - Accuracy: 51.11111402511597% - F1:0.47190566635011083%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 1.3309719562530518 - Accuracy: 42.105263471603394% - F1:0.40797213622291023%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.5056209564208984 - Accuracy: 42.105263471603394% - F1:0.40869534948482317%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 1.7522836923599243 - Accuracy: 32.608696818351746% - F1:0.20473890039107429%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 1.4388662576675415 - Accuracy: 53.06122303009033% - F1:0.47602703419029946%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 1.0934953689575195 - Accuracy: 66.66666865348816% - F1:0.6521494378637236%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 3.635305881500244 - Accuracy: 34.375% - F1:0.21255060728744937%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.5722720623016357 - Accuracy: 80.55555820465088% - F1:0.808452290145654%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 1.1605077981948853 - Accuracy: 37.5% - F1:0.2886650386650387%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 0.8651168942451477 - Accuracy: 60.52631735801697% - F1:0.5335497835497837%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 1.180909514427185 - Accuracy: 50.0% - F1:0.4283046683046683%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 0.8399780988693237 - Accuracy: 59.574466943740845% - F1:0.5582573454913881%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.9057102203369141 - Accuracy: 62.5% - F1:0.6187254901960785%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.7090939879417419 - Accuracy: 71.42857313156128% - F1:0.688754553204681%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.6111114025115967 - Accuracy: 71.42857313156128% - F1:0.716468253968254%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 1.3416242599487305 - Accuracy: 55.000001192092896% - F1:0.4836688311688312%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 0.8130879402160645 - Accuracy: 72.22222089767456% - F1:0.6534900284900285%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.030587077140808 - Accuracy: 60.00000238418579% - F1:0.5699096225412015%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 1.18007493019104 - Accuracy: 56.41025900840759% - F1:0.5072804972804972%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 1.1475070714950562 - Accuracy: 66.00000262260437% - F1:0.6586090162560749%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.9636569023132324 - Accuracy: 58.53658318519592% - F1:0.5497233411535567%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 2.053558826446533 - Accuracy: 41.17647111415863% - F1:0.35656108597285074%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 0.8705970644950867 - Accuracy: 58.974361419677734% - F1:0.5694207027540361%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 1.401336908340454 - Accuracy: 61.90476417541504% - F1:0.5711598746081504%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.6901620626449585 - Accuracy: 74.28571581840515% - F1:0.7368796992481204%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.7670972943305969 - Accuracy: 68.08510422706604% - F1:0.664998516062346%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.4391573667526245 - Accuracy: 39.47368562221527% - F1:0.36532507739938075%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.3751965761184692 - Accuracy: 42.105263471603394% - F1:0.37783887938686705%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 0.40041324496269226 - Accuracy: 91.89189076423645% - F1:0.9175032175032175%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.9670069813728333 - Accuracy: 59.52380895614624% - F1:0.5182714851645243%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.5547577738761902 - Accuracy: 82.05128312110901% - F1:0.8141858141858143%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.9135024547576904 - Accuracy: 53.125% - F1:0.5506771618903972%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.2223490476608276 - Accuracy: 52.63158082962036% - F1:0.47139001349527665%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 0.8640475273132324 - Accuracy: 61.538463830947876% - F1:0.5622477650063857%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 46 - Loss: 1.614271879196167 - Accuracy: 44.186046719551086% - F1:0.36071163384715915%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 47 - Loss: 0.6341947317123413 - Accuracy: 77.14285850524902% - F1:0.7711111111111112%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 48 - Loss: 0.6146137714385986 - Accuracy: 78.125% - F1:0.7768308080808081%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 49 - Loss: 2.0752573013305664 - Accuracy: 41.025641560554504% - F1:0.3146853146853147%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 50 - Loss: 0.5086686611175537 - Accuracy: 86.11111044883728% - F1:0.8465166539537249%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 51 - Loss: 0.784246563911438 - Accuracy: 69.04761791229248% - F1:0.6630965078170047%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 52 - Loss: 1.3768973350524902 - Accuracy: 47.61904776096344% - F1:0.40500240500240503%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 58.523112076979416 (+- 13.857248871867027)\n",
      "> F1: 0.5479223323549023 (+- 0.16323709097626496)\n",
      "> Loss: 1.1354904616108308\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = train_SID\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/N1/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/30_TF_FE_Phys_only_balanced/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/30_TF_FE_Phys_only_balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/30_TF_FE_Phys_only_balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 950us/step - loss: 1.2869 - accuracy: 0.4516\n",
      "Test loss, Test acc: [1.2868767976760864, 0.4516128897666931]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=32)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67, 49,  9, 20],\n",
       "       [46, 36,  9, 24],\n",
       "       [ 0,  0, 17,  8],\n",
       "       [ 0,  1,  4, 20]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46206897, 0.33793103, 0.06206897, 0.13793103],\n",
       "       [0.4       , 0.31304348, 0.07826087, 0.20869565],\n",
       "       [0.        , 0.        , 0.68      , 0.32      ],\n",
       "       [0.        , 0.04      , 0.16      , 0.8       ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Feature Engineered STEP Phys Only balanced - Confusion Matrix')\n",
    "plt.savefig('30_figures/30_feat_engin_window_ANN_Phys_only_balanced.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.452 \n",
      "F1 Score: 0.452\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

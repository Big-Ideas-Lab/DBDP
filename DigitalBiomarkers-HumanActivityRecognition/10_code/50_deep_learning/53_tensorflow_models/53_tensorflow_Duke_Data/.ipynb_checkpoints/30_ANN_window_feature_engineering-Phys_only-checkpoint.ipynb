{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Classification Model: Random Forest w/ Feature Engineering - Phys Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is composed of a random forest classification model to evaluate a general accuracy level of traditional ML methods in classifying our HAR data based on activity. We also used Leave-One-Out Cross-Validation to validate our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INPUT: .csv files containing the rolled sensor data with feature engineering (engineered_features.csv)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT: Random Forest Multi-Classification Model (F1 Score = )__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('/Users/N1/Data7/Data-2020/10_code/40_usable_data_for_models/41_Duke_Data/engineered_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(count=df.groupby(df.Activity.ne(df.Activity.shift()).cumsum()).cumcount().add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Round</th>\n",
       "      <th>ACC1_mean</th>\n",
       "      <th>ACC2_mean</th>\n",
       "      <th>ACC3_mean</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>Magnitude_mean</th>\n",
       "      <th>ACC1_std</th>\n",
       "      <th>ACC2_std</th>\n",
       "      <th>ACC3_std</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>Magnitude_std</th>\n",
       "      <th>ACC1_skew</th>\n",
       "      <th>ACC2_skew</th>\n",
       "      <th>ACC3_skew</th>\n",
       "      <th>TEMP_skew</th>\n",
       "      <th>EDA_skew</th>\n",
       "      <th>BVP_skew</th>\n",
       "      <th>HR_skew</th>\n",
       "      <th>Magnitude_skew</th>\n",
       "      <th>ACC1_min</th>\n",
       "      <th>ACC2_min</th>\n",
       "      <th>ACC3_min</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>EDA_min</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>HR_min</th>\n",
       "      <th>Magnitude_min</th>\n",
       "      <th>ACC1_max</th>\n",
       "      <th>ACC2_max</th>\n",
       "      <th>ACC3_max</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>EDA_max</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>HR_max</th>\n",
       "      <th>Magnitude_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...</td>\n",
       "      <td>[27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...</td>\n",
       "      <td>[40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...</td>\n",
       "      <td>[32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.275354 0.276634 0.270231 0.270231 0.26895 0...</td>\n",
       "      <td>[15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...</td>\n",
       "      <td>[78.98 78.83500000000002 78.69 78.545 78.4 78....</td>\n",
       "      <td>[63.410093833710725 63.453053512025726 63.4961...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.248370</td>\n",
       "      <td>28.012880</td>\n",
       "      <td>38.824457</td>\n",
       "      <td>32.350</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>62.553853</td>\n",
       "      <td>0.701573</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.632616</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>-0.082592</td>\n",
       "      <td>-0.558848</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.714533</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>-0.392823</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.531557</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.456522</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-42.99</td>\n",
       "      <td>69.7650</td>\n",
       "      <td>61.692787</td>\n",
       "      <td>41.543478</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.39</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>34.83</td>\n",
       "      <td>78.98</td>\n",
       "      <td>63.757353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39.0 39.06521739130435 39.130434782608695 39....</td>\n",
       "      <td>[29.0 28.93478260869565 28.869565217391305 28....</td>\n",
       "      <td>[38.0 38.02173913043478 38.04347826086956 38.0...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.25998499999999997 0.25998499999999997 0.258...</td>\n",
       "      <td>[-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...</td>\n",
       "      <td>[73.52 73.435 73.35 73.265 73.18 73.0925 73.00...</td>\n",
       "      <td>[61.69278726074872 61.7168170027034 61.7409828...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>40.820000</td>\n",
       "      <td>26.815000</td>\n",
       "      <td>38.192500</td>\n",
       "      <td>32.339</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>62.021872</td>\n",
       "      <td>1.192214</td>\n",
       "      <td>1.149559</td>\n",
       "      <td>0.529382</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>0.542348</td>\n",
       "      <td>0.515544</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.188071</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.170923</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.254862</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>64.8025</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>37.72</td>\n",
       "      <td>73.52</td>\n",
       "      <td>62.936476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[41.60869565217392 41.67391304347826 41.739130...</td>\n",
       "      <td>[26.39130434782609 26.32608695652174 26.260869...</td>\n",
       "      <td>[38.869565217391305 38.89130434782609 38.91304...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.265108 0.263827 0.266389 0.265108 0.266389 ...</td>\n",
       "      <td>[-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...</td>\n",
       "      <td>[69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...</td>\n",
       "      <td>[62.758486272725364 62.78782873035957 62.81730...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.252235</td>\n",
       "      <td>25.312684</td>\n",
       "      <td>37.488043</td>\n",
       "      <td>32.337</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>62.621785</td>\n",
       "      <td>2.109896</td>\n",
       "      <td>0.815025</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>0.227966</td>\n",
       "      <td>1.329886</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>-0.274279</td>\n",
       "      <td>0.185657</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.252301</td>\n",
       "      <td>-48.52</td>\n",
       "      <td>60.9950</td>\n",
       "      <td>60.778286</td>\n",
       "      <td>45.532258</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>47.14</td>\n",
       "      <td>69.63</td>\n",
       "      <td>64.010791</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43.971428571428575 44.14285714285715 44.31428...</td>\n",
       "      <td>[24.514285714285712 24.42857142857143 24.34285...</td>\n",
       "      <td>[37.17142857142857 37.142857142857146 37.11428...</td>\n",
       "      <td>[32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....</td>\n",
       "      <td>[0.258704 0.258704 0.258704 0.257424 0.257424 ...</td>\n",
       "      <td>[17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....</td>\n",
       "      <td>[64.68 64.555 64.43 64.305 64.18 64.0475 63.91...</td>\n",
       "      <td>[62.579164557660036 62.649331804179724 62.7200...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>44.905798</td>\n",
       "      <td>24.915984</td>\n",
       "      <td>37.638218</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>63.734171</td>\n",
       "      <td>1.832017</td>\n",
       "      <td>1.509593</td>\n",
       "      <td>1.773398</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>0.841361</td>\n",
       "      <td>-4.941414</td>\n",
       "      <td>-1.040349</td>\n",
       "      <td>3.435987</td>\n",
       "      <td>0.672586</td>\n",
       "      <td>0.734482</td>\n",
       "      <td>-0.828441</td>\n",
       "      <td>0.406263</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.985816</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.8025</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>47.14</td>\n",
       "      <td>64.68</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[45.54838709677418 45.564516129032256 45.58064...</td>\n",
       "      <td>[25.64516129032258 25.69354838709677 25.741935...</td>\n",
       "      <td>[37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...</td>\n",
       "      <td>[32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....</td>\n",
       "      <td>[0.253581 0.253581 0.253581 0.252301 0.252301 ...</td>\n",
       "      <td>[-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...</td>\n",
       "      <td>[60.92 60.8475 60.77500000000001 60.7025 60.63...</td>\n",
       "      <td>[64.04162603123257 64.07248675362091 64.103373...</td>\n",
       "      <td>19-001</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>43.577055</td>\n",
       "      <td>22.974382</td>\n",
       "      <td>38.971144</td>\n",
       "      <td>32.389</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>62.913435</td>\n",
       "      <td>2.115371</td>\n",
       "      <td>2.585687</td>\n",
       "      <td>1.809092</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1.365652</td>\n",
       "      <td>-1.857224</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>1.380509</td>\n",
       "      <td>-0.686481</td>\n",
       "      <td>1.239716</td>\n",
       "      <td>-0.833856</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.408229</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.702128</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.249739</td>\n",
       "      <td>-101.74</td>\n",
       "      <td>58.5300</td>\n",
       "      <td>61.392182</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.43</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>39.00</td>\n",
       "      <td>60.92</td>\n",
       "      <td>65.711491</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ACC1  \\\n",
       "0  [41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 41.0 ...   \n",
       "1  [39.0 39.06521739130435 39.130434782608695 39....   \n",
       "2  [41.60869565217392 41.67391304347826 41.739130...   \n",
       "3  [43.971428571428575 44.14285714285715 44.31428...   \n",
       "4  [45.54838709677418 45.564516129032256 45.58064...   \n",
       "\n",
       "                                                ACC2  \\\n",
       "0  [27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 ...   \n",
       "1  [29.0 28.93478260869565 28.869565217391305 28....   \n",
       "2  [26.39130434782609 26.32608695652174 26.260869...   \n",
       "3  [24.514285714285712 24.42857142857143 24.34285...   \n",
       "4  [25.64516129032258 25.69354838709677 25.741935...   \n",
       "\n",
       "                                                ACC3  \\\n",
       "0  [40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 40.0 ...   \n",
       "1  [38.0 38.02173913043478 38.04347826086956 38.0...   \n",
       "2  [38.869565217391305 38.89130434782609 38.91304...   \n",
       "3  [37.17142857142857 37.142857142857146 37.11428...   \n",
       "4  [37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 37.0 ...   \n",
       "\n",
       "                                                TEMP  \\\n",
       "0  [32.39 32.39 32.39 32.39 32.34 32.34 32.34 32....   \n",
       "1  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "2  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "3  [32.33 32.33 32.33 32.33 32.34 32.34 32.34 32....   \n",
       "4  [32.34 32.34 32.34 32.34 32.33 32.33 32.33 32....   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [0.275354 0.276634 0.270231 0.270231 0.26895 0...   \n",
       "1  [0.25998499999999997 0.25998499999999997 0.258...   \n",
       "2  [0.265108 0.263827 0.266389 0.265108 0.266389 ...   \n",
       "3  [0.258704 0.258704 0.258704 0.257424 0.257424 ...   \n",
       "4  [0.253581 0.253581 0.253581 0.252301 0.252301 ...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [15.25 -12.75 -42.99 18.39 13.61 -9.66 -35.47 ...   \n",
       "1  [-18.83 -0.3 11.03 6.09 -15.3 14.61 6.75 -2.38...   \n",
       "2  [-27.69 30.51 14.64 1.97 -13.65 -48.52 17.77 2...   \n",
       "3  [17.18 2.41 -22.11 5.12 18.43 5.34 -14.33 -22....   \n",
       "4  [-32.0 15.14 24.41 3.88 -22.97 -0.34 17.89 3.0...   \n",
       "\n",
       "                                                  HR  \\\n",
       "0  [78.98 78.83500000000002 78.69 78.545 78.4 78....   \n",
       "1  [73.52 73.435 73.35 73.265 73.18 73.0925 73.00...   \n",
       "2  [69.63 69.515 69.4 69.285 69.17 69.04 68.91 68...   \n",
       "3  [64.68 64.555 64.43 64.305 64.18 64.0475 63.91...   \n",
       "4  [60.92 60.8475 60.77500000000001 60.7025 60.63...   \n",
       "\n",
       "                                           Magnitude Subject_ID  Activity  \\\n",
       "0  [63.410093833710725 63.453053512025726 63.4961...     19-001  Baseline   \n",
       "1  [61.69278726074872 61.7168170027034 61.7409828...     19-001  Baseline   \n",
       "2  [62.758486272725364 62.78782873035957 62.81730...     19-001  Baseline   \n",
       "3  [62.579164557660036 62.649331804179724 62.7200...     19-001  Baseline   \n",
       "4  [64.04162603123257 64.07248675362091 64.103373...     19-001  Baseline   \n",
       "\n",
       "   Round  ACC1_mean  ACC2_mean  ACC3_mean  TEMP_mean  EDA_mean  BVP_mean  \\\n",
       "0      1  40.248370  28.012880  38.824457     32.350  0.262354 -0.109875   \n",
       "1      1  40.820000  26.815000  38.192500     32.339  0.261058  0.321375   \n",
       "2      1  43.252235  25.312684  37.488043     32.337  0.259585  0.684000   \n",
       "3      1  44.905798  24.915984  37.638218     32.356  0.254510 -0.180875   \n",
       "4      1  43.577055  22.974382  38.971144     32.389  0.252733 -0.209750   \n",
       "\n",
       "     HR_mean  Magnitude_mean  ACC1_std  ACC2_std  ACC3_std  TEMP_std  \\\n",
       "0  73.931187       62.553853  0.701573  0.687590  0.632616  0.017607   \n",
       "1  69.481750       62.021872  1.192214  1.149559  0.529382  0.012610   \n",
       "2  64.893188       62.621785  2.109896  0.815025  0.647914  0.010536   \n",
       "3  61.157687       63.734171  1.832017  1.509593  1.773398  0.025377   \n",
       "4  59.226438       62.913435  2.115371  2.585687  1.809092  0.027000   \n",
       "\n",
       "    EDA_std    BVP_std    HR_std  Magnitude_std  ACC1_skew  ACC2_skew  \\\n",
       "0  0.004877  18.439453  2.574676       0.609756  -0.082592  -0.558848   \n",
       "1  0.003007  20.104717  2.608254       0.542348   0.515544   0.109446   \n",
       "2  0.004337  23.756276  2.639037       0.942868  -0.473020   0.227966   \n",
       "3  0.002396  25.635645  1.674001       0.841361  -4.941414  -1.040349   \n",
       "4  0.002055  25.593597  0.684349       1.365652  -1.857224   0.511935   \n",
       "\n",
       "   ACC3_skew  TEMP_skew  EDA_skew  BVP_skew   HR_skew  Magnitude_skew  \\\n",
       "0   0.705668   0.714533  0.896382 -0.392823  0.296262        0.531557   \n",
       "1  -0.188071   0.787066  0.212943 -0.322900 -0.170923       -0.438037   \n",
       "2   1.329886   0.620801  0.072564 -0.274279  0.185657       -0.382833   \n",
       "3   3.435987   0.672586  0.734482 -0.828441  0.406263       -0.532117   \n",
       "4   1.380509  -0.686481  1.239716 -0.833856  0.996232        0.408229   \n",
       "\n",
       "   ACC1_min   ACC2_min  ACC3_min  TEMP_min   EDA_min  BVP_min   HR_min  \\\n",
       "0      39.0  26.456522      38.0     32.33  0.254862   -42.99  69.7650   \n",
       "1      39.0  24.600000      37.2     32.31  0.254862   -48.52  64.8025   \n",
       "2      39.0  24.000000      37.0     32.31  0.252301   -48.52  60.9950   \n",
       "3      32.0  20.985816      37.0     32.33  0.251020  -101.74  58.8025   \n",
       "4      32.0  20.702128      37.0     32.33  0.249739  -101.74  58.5300   \n",
       "\n",
       "   Magnitude_min   ACC1_max  ACC2_max  ACC3_max  TEMP_max   EDA_max  BVP_max  \\\n",
       "0      61.692787  41.543478      29.0      40.0     32.39  0.276634    34.83   \n",
       "1      60.778286  43.800000      29.0      39.0     32.37  0.266389    37.72   \n",
       "2      60.778286  45.532258      27.0      39.0     32.37  0.266389    47.14   \n",
       "3      61.392182  46.000000      27.0      48.0     32.41  0.262546    47.14   \n",
       "4      61.392182  46.000000      27.0      48.0     32.43  0.258704    39.00   \n",
       "\n",
       "   HR_max  Magnitude_max  count  \n",
       "0   78.98      63.757353      1  \n",
       "1   73.52      62.936476      2  \n",
       "2   69.63      64.010791      3  \n",
       "3   64.68      65.711491      4  \n",
       "4   60.92      65.711491      5  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "df['Activity'] = le1.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le1.classes_, le1.transform(le1.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['Subject_ID'] = le.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 17 45  9 52  7 25 38 10 36  6]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "rands = np.random.choice(df.Subject_ID.unique(),11, replace=False)\n",
    "print(rands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Subjects into Test and Train Sets (n=44,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Subject_ID'].isin(rands)] \n",
    "train = df[-df['Subject_ID'].isin(rands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this for only Physiological Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std',\n",
    "       'EDA_std', 'BVP_std', 'HR_std', 'Subject_ID', 'count', 'Activity']]\n",
    "test = test[['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std',\n",
    "       'EDA_std', 'BVP_std', 'HR_std', 'Subject_ID', 'count', 'Activity']]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train_df is made so we can use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SID = train['Subject_ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] =1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'], prefix = 'SID')], axis =1).drop('Subject_ID', axis =1)\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['count'], prefix = 'count')], axis =1).drop('count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270, 122) (1302, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/N1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop([\"train\"], axis = 1, inplace = True)\n",
    "test.drop([\"train\"], axis = 1, inplace = True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TEMP_mean', 'EDA_mean', 'BVP_mean', 'HR_mean', 'TEMP_std', 'EDA_std',\n",
       "       'BVP_std', 'HR_std', 'Activity', 'SID_0',\n",
       "       ...\n",
       "       'count_49', 'count_50', 'count_51', 'count_52', 'count_53', 'count_54',\n",
       "       'count_55', 'count_56', 'count_57', 'count_58'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = train.drop(\"Activity\", axis =1)\n",
    "test_f = test.drop(\"Activity\", axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_f\n",
    "y_train = train.Activity\n",
    "X_test = test_f\n",
    "y_test = test.Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>EDA_mean</th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>HR_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>EDA_std</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>SID_0</th>\n",
       "      <th>SID_1</th>\n",
       "      <th>SID_2</th>\n",
       "      <th>SID_3</th>\n",
       "      <th>SID_4</th>\n",
       "      <th>SID_5</th>\n",
       "      <th>SID_6</th>\n",
       "      <th>SID_7</th>\n",
       "      <th>SID_8</th>\n",
       "      <th>SID_9</th>\n",
       "      <th>SID_10</th>\n",
       "      <th>SID_11</th>\n",
       "      <th>SID_12</th>\n",
       "      <th>SID_13</th>\n",
       "      <th>SID_14</th>\n",
       "      <th>SID_15</th>\n",
       "      <th>SID_16</th>\n",
       "      <th>SID_17</th>\n",
       "      <th>SID_18</th>\n",
       "      <th>SID_19</th>\n",
       "      <th>SID_20</th>\n",
       "      <th>SID_21</th>\n",
       "      <th>SID_22</th>\n",
       "      <th>SID_23</th>\n",
       "      <th>SID_24</th>\n",
       "      <th>SID_25</th>\n",
       "      <th>SID_26</th>\n",
       "      <th>SID_27</th>\n",
       "      <th>SID_28</th>\n",
       "      <th>SID_29</th>\n",
       "      <th>SID_30</th>\n",
       "      <th>SID_31</th>\n",
       "      <th>SID_32</th>\n",
       "      <th>SID_33</th>\n",
       "      <th>SID_34</th>\n",
       "      <th>SID_35</th>\n",
       "      <th>SID_36</th>\n",
       "      <th>SID_37</th>\n",
       "      <th>SID_38</th>\n",
       "      <th>SID_39</th>\n",
       "      <th>SID_40</th>\n",
       "      <th>SID_41</th>\n",
       "      <th>SID_42</th>\n",
       "      <th>SID_43</th>\n",
       "      <th>SID_44</th>\n",
       "      <th>SID_45</th>\n",
       "      <th>SID_46</th>\n",
       "      <th>SID_47</th>\n",
       "      <th>SID_48</th>\n",
       "      <th>SID_49</th>\n",
       "      <th>SID_50</th>\n",
       "      <th>SID_51</th>\n",
       "      <th>SID_52</th>\n",
       "      <th>SID_53</th>\n",
       "      <th>SID_54</th>\n",
       "      <th>count_1</th>\n",
       "      <th>count_2</th>\n",
       "      <th>count_3</th>\n",
       "      <th>count_4</th>\n",
       "      <th>count_5</th>\n",
       "      <th>count_6</th>\n",
       "      <th>count_7</th>\n",
       "      <th>count_8</th>\n",
       "      <th>count_9</th>\n",
       "      <th>count_10</th>\n",
       "      <th>count_11</th>\n",
       "      <th>count_12</th>\n",
       "      <th>count_13</th>\n",
       "      <th>count_14</th>\n",
       "      <th>count_15</th>\n",
       "      <th>count_16</th>\n",
       "      <th>count_17</th>\n",
       "      <th>count_18</th>\n",
       "      <th>count_19</th>\n",
       "      <th>count_20</th>\n",
       "      <th>count_21</th>\n",
       "      <th>count_22</th>\n",
       "      <th>count_23</th>\n",
       "      <th>count_24</th>\n",
       "      <th>count_25</th>\n",
       "      <th>count_26</th>\n",
       "      <th>count_27</th>\n",
       "      <th>count_28</th>\n",
       "      <th>count_29</th>\n",
       "      <th>count_30</th>\n",
       "      <th>count_31</th>\n",
       "      <th>count_32</th>\n",
       "      <th>count_33</th>\n",
       "      <th>count_34</th>\n",
       "      <th>count_35</th>\n",
       "      <th>count_36</th>\n",
       "      <th>count_37</th>\n",
       "      <th>count_38</th>\n",
       "      <th>count_39</th>\n",
       "      <th>count_40</th>\n",
       "      <th>count_41</th>\n",
       "      <th>count_42</th>\n",
       "      <th>count_43</th>\n",
       "      <th>count_44</th>\n",
       "      <th>count_45</th>\n",
       "      <th>count_46</th>\n",
       "      <th>count_47</th>\n",
       "      <th>count_48</th>\n",
       "      <th>count_49</th>\n",
       "      <th>count_50</th>\n",
       "      <th>count_51</th>\n",
       "      <th>count_52</th>\n",
       "      <th>count_53</th>\n",
       "      <th>count_54</th>\n",
       "      <th>count_55</th>\n",
       "      <th>count_56</th>\n",
       "      <th>count_57</th>\n",
       "      <th>count_58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.3500</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>-0.109875</td>\n",
       "      <td>73.931187</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>18.439453</td>\n",
       "      <td>2.574676</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.3390</td>\n",
       "      <td>0.261058</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>69.481750</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>20.104717</td>\n",
       "      <td>2.608254</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.3370</td>\n",
       "      <td>0.259585</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>64.893188</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>23.756276</td>\n",
       "      <td>2.639037</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.3560</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>-0.180875</td>\n",
       "      <td>61.157687</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>25.635645</td>\n",
       "      <td>1.674001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.3890</td>\n",
       "      <td>0.252733</td>\n",
       "      <td>-0.209750</td>\n",
       "      <td>59.226438</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>25.593597</td>\n",
       "      <td>0.684349</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>32.1945</td>\n",
       "      <td>0.443490</td>\n",
       "      <td>2.167500</td>\n",
       "      <td>97.022312</td>\n",
       "      <td>0.020609</td>\n",
       "      <td>0.099720</td>\n",
       "      <td>21.499873</td>\n",
       "      <td>1.280447</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>32.1705</td>\n",
       "      <td>0.541785</td>\n",
       "      <td>-0.566125</td>\n",
       "      <td>95.047438</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.064759</td>\n",
       "      <td>12.241906</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>32.1590</td>\n",
       "      <td>0.576188</td>\n",
       "      <td>-0.373250</td>\n",
       "      <td>93.412750</td>\n",
       "      <td>0.015780</td>\n",
       "      <td>0.054757</td>\n",
       "      <td>3.497406</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>32.1435</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>-0.307875</td>\n",
       "      <td>92.144750</td>\n",
       "      <td>0.019046</td>\n",
       "      <td>0.061583</td>\n",
       "      <td>2.692347</td>\n",
       "      <td>0.656914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>32.1190</td>\n",
       "      <td>0.603963</td>\n",
       "      <td>-0.032375</td>\n",
       "      <td>91.983625</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.070546</td>\n",
       "      <td>2.921700</td>\n",
       "      <td>0.412381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5270 rows  121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEMP_mean  EDA_mean  BVP_mean    HR_mean  TEMP_std   EDA_std    BVP_std  \\\n",
       "0       32.3500  0.262354 -0.109875  73.931187  0.017607  0.004877  18.439453   \n",
       "1       32.3390  0.261058  0.321375  69.481750  0.012610  0.003007  20.104717   \n",
       "2       32.3370  0.259585  0.684000  64.893188  0.010536  0.004337  23.756276   \n",
       "3       32.3560  0.254510 -0.180875  61.157687  0.025377  0.002396  25.635645   \n",
       "4       32.3890  0.252733 -0.209750  59.226438  0.027000  0.002055  25.593597   \n",
       "...         ...       ...       ...        ...       ...       ...        ...   \n",
       "6567    32.1945  0.443490  2.167500  97.022312  0.020609  0.099720  21.499873   \n",
       "6568    32.1705  0.541785 -0.566125  95.047438  0.019615  0.064759  12.241906   \n",
       "6569    32.1590  0.576188 -0.373250  93.412750  0.015780  0.054757   3.497406   \n",
       "6570    32.1435  0.593157 -0.307875  92.144750  0.019046  0.061583   2.692347   \n",
       "6571    32.1190  0.603963 -0.032375  91.983625  0.019468  0.070546   2.921700   \n",
       "\n",
       "        HR_std  SID_0  SID_1  SID_2  SID_3  SID_4  SID_5  SID_6  SID_7  SID_8  \\\n",
       "0     2.574676      1      0      0      0      0      0      0      0      0   \n",
       "1     2.608254      1      0      0      0      0      0      0      0      0   \n",
       "2     2.639037      1      0      0      0      0      0      0      0      0   \n",
       "3     1.674001      1      0      0      0      0      0      0      0      0   \n",
       "4     0.684349      1      0      0      0      0      0      0      0      0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "6567  1.280447      0      0      0      0      0      0      0      0      0   \n",
       "6568  0.998228      0      0      0      0      0      0      0      0      0   \n",
       "6569  0.921024      0      0      0      0      0      0      0      0      0   \n",
       "6570  0.656914      0      0      0      0      0      0      0      0      0   \n",
       "6571  0.412381      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      SID_9  SID_10  SID_11  SID_12  SID_13  SID_14  SID_15  SID_16  SID_17  \\\n",
       "0         0       0       0       0       0       0       0       0       0   \n",
       "1         0       0       0       0       0       0       0       0       0   \n",
       "2         0       0       0       0       0       0       0       0       0   \n",
       "3         0       0       0       0       0       0       0       0       0   \n",
       "4         0       0       0       0       0       0       0       0       0   \n",
       "...     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567      0       0       0       0       0       0       0       0       0   \n",
       "6568      0       0       0       0       0       0       0       0       0   \n",
       "6569      0       0       0       0       0       0       0       0       0   \n",
       "6570      0       0       0       0       0       0       0       0       0   \n",
       "6571      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_18  SID_19  SID_20  SID_21  SID_22  SID_23  SID_24  SID_25  SID_26  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_27  SID_28  SID_29  SID_30  SID_31  SID_32  SID_33  SID_34  SID_35  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_36  SID_37  SID_38  SID_39  SID_40  SID_41  SID_42  SID_43  SID_44  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_45  SID_46  SID_47  SID_48  SID_49  SID_50  SID_51  SID_52  SID_53  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6567       0       0       0       0       0       0       0       0       0   \n",
       "6568       0       0       0       0       0       0       0       0       0   \n",
       "6569       0       0       0       0       0       0       0       0       0   \n",
       "6570       0       0       0       0       0       0       0       0       0   \n",
       "6571       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      SID_54  count_1  count_2  count_3  count_4  count_5  count_6  count_7  \\\n",
       "0          0        1        0        0        0        0        0        0   \n",
       "1          0        0        1        0        0        0        0        0   \n",
       "2          0        0        0        1        0        0        0        0   \n",
       "3          0        0        0        0        1        0        0        0   \n",
       "4          0        0        0        0        0        1        0        0   \n",
       "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "6567       1        1        0        0        0        0        0        0   \n",
       "6568       1        0        1        0        0        0        0        0   \n",
       "6569       1        0        0        1        0        0        0        0   \n",
       "6570       1        0        0        0        1        0        0        0   \n",
       "6571       1        0        0        0        0        1        0        0   \n",
       "\n",
       "      count_8  count_9  count_10  count_11  count_12  count_13  count_14  \\\n",
       "0           0        0         0         0         0         0         0   \n",
       "1           0        0         0         0         0         0         0   \n",
       "2           0        0         0         0         0         0         0   \n",
       "3           0        0         0         0         0         0         0   \n",
       "4           0        0         0         0         0         0         0   \n",
       "...       ...      ...       ...       ...       ...       ...       ...   \n",
       "6567        0        0         0         0         0         0         0   \n",
       "6568        0        0         0         0         0         0         0   \n",
       "6569        0        0         0         0         0         0         0   \n",
       "6570        0        0         0         0         0         0         0   \n",
       "6571        0        0         0         0         0         0         0   \n",
       "\n",
       "      count_15  count_16  count_17  count_18  count_19  count_20  count_21  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_22  count_23  count_24  count_25  count_26  count_27  count_28  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_29  count_30  count_31  count_32  count_33  count_34  count_35  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_36  count_37  count_38  count_39  count_40  count_41  count_42  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_43  count_44  count_45  count_46  count_47  count_48  count_49  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_50  count_51  count_52  count_53  count_54  count_55  count_56  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6567         0         0         0         0         0         0         0   \n",
       "6568         0         0         0         0         0         0         0   \n",
       "6569         0         0         0         0         0         0         0   \n",
       "6570         0         0         0         0         0         0         0   \n",
       "6571         0         0         0         0         0         0         0   \n",
       "\n",
       "      count_57  count_58  \n",
       "0            0         0  \n",
       "1            0         0  \n",
       "2            0         0  \n",
       "3            0         0  \n",
       "4            0         0  \n",
       "...        ...       ...  \n",
       "6567         0         0  \n",
       "6568         0         0  \n",
       "6569         0         0  \n",
       "6570         0         0  \n",
       "6571         0         0  \n",
       "\n",
       "[5270 rows x 121 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:16] = sc.fit_transform(X_train.iloc[:,:16])\n",
    "X_test.iloc[:,:16] = sc.transform(X_test.iloc[:,:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Object Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training and validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1623 - accuracy: 0.4786\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9786 - accuracy: 0.5614\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8207 - accuracy: 0.6049\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7322 - accuracy: 0.6485\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.6916\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.7186\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.7400\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.5630 - accuracy: 0.7544\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.5365 - accuracy: 0.7709\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7925\n",
      "Score for fold 1: loss of 4.374955654144287; accuracy of 37.09677457809448%, F1 of 0.20075901328273243\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1696 - accuracy: 0.4749\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9736 - accuracy: 0.5692\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8170 - accuracy: 0.6117\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7438 - accuracy: 0.6461\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6956 - accuracy: 0.6726\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.6967\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.7229\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7433\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7581\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7779\n",
      "Score for fold 2: loss of 1.576340913772583; accuracy of 18.54838728904724%, F1 of 0.17764716602879535\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1342 - accuracy: 0.4738\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9677 - accuracy: 0.5752\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8159 - accuracy: 0.6061\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7264 - accuracy: 0.6564\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6742 - accuracy: 0.6937\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.7237\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5807 - accuracy: 0.7466\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7750\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5048 - accuracy: 0.7909\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4753 - accuracy: 0.7942\n",
      "Score for fold 3: loss of 1.0437103509902954; accuracy of 50.0%, F1 of 0.4712484880766115\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1569 - accuracy: 0.4693\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9843 - accuracy: 0.5721\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8352 - accuracy: 0.6079\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7314 - accuracy: 0.6574\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.6871\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6175 - accuracy: 0.7217\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.7382\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.7540\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7777\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.7847\n",
      "Score for fold 4: loss of 2.1889803409576416; accuracy of 37.09677457809448%, F1 of 0.3276549311840024\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1473 - accuracy: 0.4883\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9875 - accuracy: 0.5861\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8498 - accuracy: 0.6077\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7485 - accuracy: 0.6364\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.6665\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.7093\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5965 - accuracy: 0.7256\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.7443\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7565\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.7796\n",
      "Score for fold 5: loss of 3.4817512035369873; accuracy of 16.93548411130905%, F1 of 0.125503775229886\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1444 - accuracy: 0.4784\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9665 - accuracy: 0.5698\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8217 - accuracy: 0.6195\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.6889\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.7178\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.7332\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5656 - accuracy: 0.7511\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5260 - accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7872\n",
      "Score for fold 6: loss of 2.9748363494873047; accuracy of 16.129031777381897%, F1 of 0.15278722566324157\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1639 - accuracy: 0.4394\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0194 - accuracy: 0.5431\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8675 - accuracy: 0.5968\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7627 - accuracy: 0.6306\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7025 - accuracy: 0.6741\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.7003\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6080 - accuracy: 0.7309\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.7435\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7664\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7804\n",
      "Score for fold 7: loss of 1.1601483821868896; accuracy of 42.741936445236206%, F1 of 0.4169339432497327\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1677 - accuracy: 0.4835\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0051 - accuracy: 0.5670\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8431 - accuracy: 0.6306\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7427 - accuracy: 0.6582\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.6873\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7200\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5884 - accuracy: 0.7425\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.7645\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7855\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.8006\n",
      "Score for fold 8: loss of 1.0872321128845215; accuracy of 59.67742204666138%, F1 of 0.5954093012417191\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1397 - accuracy: 0.4984\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9781 - accuracy: 0.5803\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8639 - accuracy: 0.6065\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7423 - accuracy: 0.6461\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6889\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.7157\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5899 - accuracy: 0.7437\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7596\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7789\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7866\n",
      "Score for fold 9: loss of 1.4950278997421265; accuracy of 47.580644488334656%, F1 of 0.4007616487455197\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1501 - accuracy: 0.4812\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9786 - accuracy: 0.5680\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8341 - accuracy: 0.6092\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7262 - accuracy: 0.6489\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6671 - accuracy: 0.6963\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.7295\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.7396\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7625\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7808\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7962\n",
      "Score for fold 10: loss of 1.1569911241531372; accuracy of 47.580644488334656%, F1 of 0.48602569846349153\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 1.1464 - accuracy: 0.4821\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9750 - accuracy: 0.5754\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8285 - accuracy: 0.6187\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7460 - accuracy: 0.6384\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.6726\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6982\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.7116\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5813 - accuracy: 0.7344\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7495\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7647\n",
      "Score for fold 11: loss of 1.2553602457046509; accuracy of 45.967742800712585%, F1 of 0.45113478914193167\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1690 - accuracy: 0.4538\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0084 - accuracy: 0.5008\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8790 - accuracy: 0.5556\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7917 - accuracy: 0.6150\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7309 - accuracy: 0.6553\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.6817\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.7097\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5929 - accuracy: 0.7301\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7565\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7732\n",
      "Score for fold 12: loss of 4.134762287139893; accuracy of 49.193549156188965%, F1 of 0.36006144393241163\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1538 - accuracy: 0.4856\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9733 - accuracy: 0.5839\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8200 - accuracy: 0.6082\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.6479\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.6813\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.7054\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.7314\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7551\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7715\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.7905\n",
      "Score for fold 13: loss of 0.8532889485359192; accuracy of 70.16128897666931%, F1 of 0.7022919369273649\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1876 - accuracy: 0.4565\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0086 - accuracy: 0.5606\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8578 - accuracy: 0.6036\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7411 - accuracy: 0.6409\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.6875\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.7217\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5932 - accuracy: 0.7447\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5528 - accuracy: 0.7660\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7831\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7940\n",
      "Score for fold 14: loss of 2.8085341453552246; accuracy of 10.483870655298233%, F1 of 0.07060531033893963\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1521 - accuracy: 0.4841\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0149 - accuracy: 0.5602\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8707 - accuracy: 0.5991\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7608 - accuracy: 0.6395\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.6696\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.7044\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6127 - accuracy: 0.7270\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.7462\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7711\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7754\n",
      "Score for fold 15: loss of 0.4809292256832123; accuracy of 80.64516186714172%, F1 of 0.8303749088708006\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1305 - accuracy: 0.4937\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.9755 - accuracy: 0.5672\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.8260 - accuracy: 0.6058\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.7349 - accuracy: 0.6454\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6730 - accuracy: 0.6909\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.7181\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5934 - accuracy: 0.7358\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7602\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.7642\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7813\n",
      "Score for fold 16: loss of 0.9308536648750305; accuracy of 53.22580933570862%, F1 of 0.5735222438851472\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1677 - accuracy: 0.4516\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9681 - accuracy: 0.5585\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8024 - accuracy: 0.6152\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7208 - accuracy: 0.6669\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6660 - accuracy: 0.6955\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6322 - accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.7342\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7450\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7678\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7864\n",
      "Score for fold 17: loss of 0.9711511731147766; accuracy of 62.90322542190552%, F1 of 0.6159055997765676\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1600 - accuracy: 0.4623\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9702 - accuracy: 0.5709\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8146 - accuracy: 0.6044\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7360 - accuracy: 0.6465\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.6780\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6508 - accuracy: 0.7029\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.7270\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5800 - accuracy: 0.7447\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5453 - accuracy: 0.7606\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7763\n",
      "Score for fold 18: loss of 0.6632579565048218; accuracy of 74.19354915618896%, F1 of 0.7664543099956134\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1608 - accuracy: 0.4598\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0097 - accuracy: 0.5428\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8494 - accuracy: 0.5952\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7526 - accuracy: 0.6463\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6889 - accuracy: 0.6786\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6479 - accuracy: 0.7035\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.7276\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.7489\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5352 - accuracy: 0.7686\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.7785\n",
      "Score for fold 19: loss of 1.2432355880737305; accuracy of 43.54838728904724%, F1 of 0.41295366656258253\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1362 - accuracy: 0.4965\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9865 - accuracy: 0.5771\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8468 - accuracy: 0.6010\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7443 - accuracy: 0.6506\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6942 - accuracy: 0.6827\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6440 - accuracy: 0.7070\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6068 - accuracy: 0.7254\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5751 - accuracy: 0.7501\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5390 - accuracy: 0.7625\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7787\n",
      "Score for fold 20: loss of 0.6114289164543152; accuracy of 73.38709831237793%, F1 of 0.7516922742041665\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1353 - accuracy: 0.4743\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9876 - accuracy: 0.5527\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8623 - accuracy: 0.5906\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7675 - accuracy: 0.6238\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7114 - accuracy: 0.6644\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6685 - accuracy: 0.6899\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6315 - accuracy: 0.7165\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6020 - accuracy: 0.7357\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7511\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.7653\n",
      "Score for fold 21: loss of 1.1903718709945679; accuracy of 57.258063554763794%, F1 of 0.5671608091351581\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1662 - accuracy: 0.4722\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9916 - accuracy: 0.5729\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8343 - accuracy: 0.6224\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7491 - accuracy: 0.6512\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6959 - accuracy: 0.6757\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6430 - accuracy: 0.7118\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6110 - accuracy: 0.7289\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7528\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5497 - accuracy: 0.7633\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7756\n",
      "Score for fold 22: loss of 0.7066933512687683; accuracy of 66.12903475761414%, F1 of 0.6758418296050602\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 1.1538 - accuracy: 0.4510\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.9807 - accuracy: 0.5733: 0s - loss: 0.9998 - accuracy: 0.56\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.8286 - accuracy: 0.6148\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.7412 - accuracy: 0.6557\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6801 - accuracy: 0.6993\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6371 - accuracy: 0.7183\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5913 - accuracy: 0.7419\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7602\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7776\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7894\n",
      "Score for fold 23: loss of 1.6694086790084839; accuracy of 32.258063554763794%, F1 of 0.2844718532574321\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1537 - accuracy: 0.4901\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9957 - accuracy: 0.5816\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8719 - accuracy: 0.6063\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7587 - accuracy: 0.6417\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6885 - accuracy: 0.6846\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6447 - accuracy: 0.7062\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.7291\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.7493\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.7686\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7785\n",
      "Score for fold 24: loss of 0.8564228415489197; accuracy of 61.29032373428345%, F1 of 0.6359948037598019\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1495 - accuracy: 0.4798\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9969 - accuracy: 0.5478\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8328 - accuracy: 0.6022\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7316 - accuracy: 0.6533\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.6869\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6213 - accuracy: 0.7233\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.7472\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7651\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.7849\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7917\n",
      "Score for fold 25: loss of 0.93131023645401; accuracy of 56.45161271095276%, F1 of 0.4933771940375101\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1551 - accuracy: 0.4757\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9961 - accuracy: 0.5626\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8353 - accuracy: 0.6174\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7427 - accuracy: 0.6570\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.68 - 0s 1ms/step - loss: 0.6933 - accuracy: 0.6869\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6503 - accuracy: 0.7021\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6085 - accuracy: 0.7340\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.7526\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5354 - accuracy: 0.7787\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7927\n",
      "Score for fold 26: loss of 0.8541756868362427; accuracy of 67.7419364452362%, F1 of 0.6645333208302077\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1645 - accuracy: 0.4510\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0017 - accuracy: 0.5412\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8356 - accuracy: 0.6003\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7397 - accuracy: 0.6413\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.6776\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6527 - accuracy: 0.7099\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.7295\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.7379\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7672\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.7756\n",
      "Score for fold 27: loss of 0.82612544298172; accuracy of 58.870965242385864%, F1 of 0.5773518124961249\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1549 - accuracy: 0.4487\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0018 - accuracy: 0.5142\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8409 - accuracy: 0.5888\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7632 - accuracy: 0.6265\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.6728\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6727 - accuracy: 0.6902\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.7206\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.7344\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7472\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7664\n",
      "Score for fold 28: loss of 1.334408164024353; accuracy of 34.67741906642914%, F1 of 0.34218495690920253\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1545 - accuracy: 0.4872\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9748 - accuracy: 0.5812\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8407 - accuracy: 0.6112\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7435 - accuracy: 0.6506\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6887 - accuracy: 0.6792\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6420 - accuracy: 0.7037\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6099 - accuracy: 0.7219\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.7414\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7600\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7726\n",
      "Score for fold 29: loss of 1.3260084390640259; accuracy of 50.806450843811035%, F1 of 0.4871535887885866\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1734 - accuracy: 0.4798\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9869 - accuracy: 0.5797\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8459 - accuracy: 0.6094\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7282 - accuracy: 0.6601\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.7038\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6216 - accuracy: 0.7262\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.7414\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5449 - accuracy: 0.7641\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 995us/step - loss: 0.5104 - accuracy: 0.7841\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.7888\n",
      "Score for fold 30: loss of 0.6895602941513062; accuracy of 74.19354915618896%, F1 of 0.7368643154506722\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 931us/step - loss: 1.1563 - accuracy: 0.4607\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 961us/step - loss: 1.0006 - accuracy: 0.5208\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8486 - accuracy: 0.5919\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 922us/step - loss: 0.7490 - accuracy: 0.6518\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 954us/step - loss: 0.6883 - accuracy: 0.6817\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.6385 - accuracy: 0.7095\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 901us/step - loss: 0.5962 - accuracy: 0.7305\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 901us/step - loss: 0.5674 - accuracy: 0.7551\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7686\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7862\n",
      "Score for fold 31: loss of 0.8741259574890137; accuracy of 62.90322542190552%, F1 of 0.6275090472908557\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1440 - accuracy: 0.4940\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9899 - accuracy: 0.5655\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8388 - accuracy: 0.6061\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7424 - accuracy: 0.6479\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7027 - accuracy: 0.6768\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6495 - accuracy: 0.7056\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.7260\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7483\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7581\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5256 - accuracy: 0.7757\n",
      "Score for fold 32: loss of 0.8537026047706604; accuracy of 62.09677457809448%, F1 of 0.6460235341854331\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1436 - accuracy: 0.4660\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 995us/step - loss: 1.0241 - accuracy: 0.5089\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 975us/step - loss: 0.8977 - accuracy: 0.5682\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8016 - accuracy: 0.5995\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 960us/step - loss: 0.7390 - accuracy: 0.6356\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 961us/step - loss: 0.6810 - accuracy: 0.6786\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6263 - accuracy: 0.7122\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6019 - accuracy: 0.7274\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.7557\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5214 - accuracy: 0.7754\n",
      "Score for fold 33: loss of 0.7097625136375427; accuracy of 72.5806474685669%, F1 of 0.7194485421921933\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1397 - accuracy: 0.4778\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.9941 - accuracy: 0.5428\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8513 - accuracy: 0.5832\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7568 - accuracy: 0.6353\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.6747\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6542 - accuracy: 0.7009\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6082 - accuracy: 0.7244\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.7476\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7620\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4984 - accuracy: 0.7808\n",
      "Score for fold 34: loss of 0.6744511127471924; accuracy of 69.35483813285828%, F1 of 0.6948014408518787\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1725 - accuracy: 0.4763\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0127 - accuracy: 0.5641\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8795 - accuracy: 0.6061\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7687 - accuracy: 0.6281\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7071 - accuracy: 0.6691\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.6998\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.7106\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5877 - accuracy: 0.7342\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7474\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7666\n",
      "Score for fold 35: loss of 0.40577974915504456; accuracy of 83.87096524238586%, F1 of 0.8509983401425606\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1520 - accuracy: 0.4551\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0134 - accuracy: 0.5672\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8851 - accuracy: 0.6092\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7697 - accuracy: 0.6261\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6981 - accuracy: 0.6648\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.6980\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6141 - accuracy: 0.7169\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5801 - accuracy: 0.7338\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5531 - accuracy: 0.7561\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5284 - accuracy: 0.7744\n",
      "Score for fold 36: loss of 0.8754152059555054; accuracy of 54.03226017951965%, F1 of 0.5171706989247311\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1599 - accuracy: 0.4652\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9612 - accuracy: 0.5773\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8066 - accuracy: 0.6180\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7125 - accuracy: 0.6695\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 0.6970\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6209 - accuracy: 0.7124\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.7324\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.7557\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7668\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7868\n",
      "Score for fold 37: loss of 1.5712981224060059; accuracy of 43.54838728904724%, F1 of 0.39175627240143374\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1293 - accuracy: 0.4963\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9542 - accuracy: 0.5834\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8064 - accuracy: 0.6162\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7250 - accuracy: 0.6576\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6690 - accuracy: 0.6941\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6294 - accuracy: 0.7145\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5936 - accuracy: 0.7384\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.7515\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5371 - accuracy: 0.7668\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5108 - accuracy: 0.7791\n",
      "Score for fold 38: loss of 1.2652814388275146; accuracy of 50.806450843811035%, F1 of 0.4192085921239907\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1566 - accuracy: 0.4920\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9635 - accuracy: 0.5865\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8185 - accuracy: 0.6164\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7278 - accuracy: 0.6564\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.6840\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6389 - accuracy: 0.7116\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5992 - accuracy: 0.7303\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5648 - accuracy: 0.7569\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7860\n",
      "Score for fold 39: loss of 1.0397230386734009; accuracy of 54.03226017951965%, F1 of 0.5066082913207738\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1510 - accuracy: 0.4895\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9808 - accuracy: 0.5855\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8401 - accuracy: 0.6174\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7425 - accuracy: 0.6465\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6827 - accuracy: 0.6844\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6381 - accuracy: 0.7029\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6066 - accuracy: 0.7272\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5717 - accuracy: 0.7485\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7588\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.7789\n",
      "Score for fold 40: loss of 0.633290708065033; accuracy of 75.0%, F1 of 0.7480874665306644\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1340 - accuracy: 0.4899\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9794 - accuracy: 0.5762\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8375 - accuracy: 0.6032\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7546 - accuracy: 0.6321\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.6700\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6550 - accuracy: 0.7085\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6164 - accuracy: 0.7192\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5871 - accuracy: 0.7427\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.7600\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5315 - accuracy: 0.7688\n",
      "Score for fold 41: loss of 1.1051561832427979; accuracy of 58.06451439857483%, F1 of 0.5670565993146639\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1433 - accuracy: 0.5006\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0001 - accuracy: 0.5898\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8608 - accuracy: 0.6302\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7389 - accuracy: 0.6611\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6765 - accuracy: 0.6922\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.7299\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.7466\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5444 - accuracy: 0.7627\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7763\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7932\n",
      "Score for fold 42: loss of 1.9318463802337646; accuracy of 47.580644488334656%, F1 of 0.3927560492076621\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.1514 - accuracy: 0.4872\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9913 - accuracy: 0.5702\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8443 - accuracy: 0.6113\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7400 - accuracy: 0.6582\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.6939\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6434 - accuracy: 0.7106\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6120 - accuracy: 0.7295\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5845 - accuracy: 0.7437\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7608\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5323 - accuracy: 0.7726\n",
      "Score for fold 43: loss of 1.0158849954605103; accuracy of 56.45161271095276%, F1 of 0.5078880587628452\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 1.1658 - accuracy: 0.4846\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.9825 - accuracy: 0.5718\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.8436 - accuracy: 0.6127\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.7551 - accuracy: 0.6415\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.6849\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6481 - accuracy: 0.7091\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.7327\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.7506\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7705\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.7815\n",
      "Score for fold 44: loss of 1.037868618965149; accuracy of 53.22580933570862%, F1 of 0.5599937759165066\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 4.374955654144287 - Accuracy: 37.09677457809448% - F1:0.20075901328273243%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.576340913772583 - Accuracy: 18.54838728904724% - F1:0.17764716602879535%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.0437103509902954 - Accuracy: 50.0% - F1:0.4712484880766115%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 2.1889803409576416 - Accuracy: 37.09677457809448% - F1:0.3276549311840024%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 3.4817512035369873 - Accuracy: 16.93548411130905% - F1:0.125503775229886%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 2.9748363494873047 - Accuracy: 16.129031777381897% - F1:0.15278722566324157%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.1601483821868896 - Accuracy: 42.741936445236206% - F1:0.4169339432497327%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.0872321128845215 - Accuracy: 59.67742204666138% - F1:0.5954093012417191%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.4950278997421265 - Accuracy: 47.580644488334656% - F1:0.4007616487455197%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.1569911241531372 - Accuracy: 47.580644488334656% - F1:0.48602569846349153%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 1.2553602457046509 - Accuracy: 45.967742800712585% - F1:0.45113478914193167%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 4.134762287139893 - Accuracy: 49.193549156188965% - F1:0.36006144393241163%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 0.8532889485359192 - Accuracy: 70.16128897666931% - F1:0.7022919369273649%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.8085341453552246 - Accuracy: 10.483870655298233% - F1:0.07060531033893963%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 0.4809292256832123 - Accuracy: 80.64516186714172% - F1:0.8303749088708006%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 0.9308536648750305 - Accuracy: 53.22580933570862% - F1:0.5735222438851472%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.9711511731147766 - Accuracy: 62.90322542190552% - F1:0.6159055997765676%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 0.6632579565048218 - Accuracy: 74.19354915618896% - F1:0.7664543099956134%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 1.2432355880737305 - Accuracy: 43.54838728904724% - F1:0.41295366656258253%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.6114289164543152 - Accuracy: 73.38709831237793% - F1:0.7516922742041665%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 1.1903718709945679 - Accuracy: 57.258063554763794% - F1:0.5671608091351581%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 0.7066933512687683 - Accuracy: 66.12903475761414% - F1:0.6758418296050602%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.6694086790084839 - Accuracy: 32.258063554763794% - F1:0.2844718532574321%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.8564228415489197 - Accuracy: 61.29032373428345% - F1:0.6359948037598019%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.93131023645401 - Accuracy: 56.45161271095276% - F1:0.4933771940375101%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.8541756868362427 - Accuracy: 67.7419364452362% - F1:0.6645333208302077%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.82612544298172 - Accuracy: 58.870965242385864% - F1:0.5773518124961249%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 1.334408164024353 - Accuracy: 34.67741906642914% - F1:0.34218495690920253%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.3260084390640259 - Accuracy: 50.806450843811035% - F1:0.4871535887885866%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 0.6895602941513062 - Accuracy: 74.19354915618896% - F1:0.7368643154506722%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.8741259574890137 - Accuracy: 62.90322542190552% - F1:0.6275090472908557%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.8537026047706604 - Accuracy: 62.09677457809448% - F1:0.6460235341854331%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.7097625136375427 - Accuracy: 72.5806474685669% - F1:0.7194485421921933%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 0.6744511127471924 - Accuracy: 69.35483813285828% - F1:0.6948014408518787%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.40577974915504456 - Accuracy: 83.87096524238586% - F1:0.8509983401425606%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.8754152059555054 - Accuracy: 54.03226017951965% - F1:0.5171706989247311%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 1.5712981224060059 - Accuracy: 43.54838728904724% - F1:0.39175627240143374%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.2652814388275146 - Accuracy: 50.806450843811035% - F1:0.4192085921239907%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.0397230386734009 - Accuracy: 54.03226017951965% - F1:0.5066082913207738%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 0.633290708065033 - Accuracy: 75.0% - F1:0.7480874665306644%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 1.1051561832427979 - Accuracy: 58.06451439857483% - F1:0.5670565993146639%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 1.9318463802337646 - Accuracy: 47.580644488334656% - F1:0.3927560492076621%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 1.0158849954605103 - Accuracy: 56.45161271095276% - F1:0.5078880587628452%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.037868618965149 - Accuracy: 53.22580933570862% - F1:0.5599937759165066%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 53.18914982066913 (+- 17.006862104927965)\n",
      "> F1: 0.511453837914482 (+- 0.18971120919422926)\n",
      "> Loss: 1.3379738208922474\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = train_SID\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/30_TF_FE_Phys_Only/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/30_TF_FE_Phys_Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/30_TF_FE_Phys_Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 769us/step - loss: 1.0607 - accuracy: 0.5430\n",
      "Test loss, Test acc: [1.060665249824524, 0.5430107712745667]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=32)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[277, 242,  33,  57],\n",
       "       [ 88, 290,  49,  56],\n",
       "       [  3,  15,  56,  31],\n",
       "       [  9,   4,   8,  84]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45484401, 0.39737274, 0.05418719, 0.09359606],\n",
       "       [0.18219462, 0.60041408, 0.10144928, 0.11594203],\n",
       "       [0.02857143, 0.14285714, 0.53333333, 0.2952381 ],\n",
       "       [0.08571429, 0.03809524, 0.07619048, 0.8       ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le1.classes_, yticklabels = le1.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Feature Engineered STEP ANN Phys only - Confusion Matrix')\n",
    "plt.savefig('30_figures/30_feat_engin_window_ANN_phys_only.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.543 \n",
      "F1 Score: 0.548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 1: Artificial Neural Network - Mechanical Sensors Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **rolled_data.csv**\n",
    "\n",
    "This notebook takes rolled data, label encodes Subject_ID, and Activity (our y variable). It then one-hot encodes Subject_ID to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('rolled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.reindex(columns = ['ACC1', 'ACC2', 'ACC3', 'Magnitude', 'Subject_ID', 'Round', 'Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.22500</td>\n",
       "      <td>28.29750</td>\n",
       "      <td>39.225000</td>\n",
       "      <td>62.913102</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.17500</td>\n",
       "      <td>28.34250</td>\n",
       "      <td>39.175000</td>\n",
       "      <td>62.870169</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.12663</td>\n",
       "      <td>28.38337</td>\n",
       "      <td>39.125543</td>\n",
       "      <td>62.826763</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACC1      ACC2       ACC3  Magnitude Subject_ID  Round  Activity\n",
       "0  40.22500  28.29750  39.225000  62.913102     19-001      1  Baseline\n",
       "1  40.17500  28.34250  39.175000  62.870169     19-001      1  Baseline\n",
       "2  40.12663  28.38337  39.125543  62.826763     19-001      1  Baseline"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Activity'] = le.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "df['Subject_ID'] = le1.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train sets (by Subject ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = list(df['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 7) (47196, 7)\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Subject_ID'].isin(ID_list[:45])]\n",
    "test = df[df['Subject_ID'].isin(ID_list[45:])]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the test train split can be changed by changing the index below. For our purposes, n = 45 for train and n = 10 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 5) (216108,) (47196, 5) (47196,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:,0:5]\n",
    "X_test = test.iloc[:,0:5]\n",
    "\n",
    "y_train = train.iloc[:,-1].values\n",
    "y_test = test.iloc[:,-1].values\n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create X_train_df below so that we are able to use the Subject_ID column later on, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID, so that it may be used as a variable in our model. This allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0\n",
    "\n",
    "combined = pd.concat([X_train, X_test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 59) (47196, 59) 263304\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:5] = ss.fit_transform(X_train.iloc[:,:5])\n",
    "X_test.iloc[:,:5] = ss.transform(X_test.iloc[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4963 - accuracy: 0.8084\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3638 - accuracy: 0.8627\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3423 - accuracy: 0.8705\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3309 - accuracy: 0.8741: 0s - loss: 0.3312 \n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3234 - accuracy: 0.8768: 0s - loss: 0.3235 - accuracy\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3173 - accuracy: 0.8790\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3125 - accuracy: 0.8811\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3094 - accuracy: 0.8817\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3055 - accuracy: 0.8836\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 963us/step - loss: 0.3033 - accuracy: 0.8847\n",
      "Score for fold 1: loss of 67.04203297588944; accuracy of 37.07729468599034%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4891 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3594 - accuracy: 0.8606\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3356 - accuracy: 0.8682\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3254 - accuracy: 0.8724\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3168 - accuracy: 0.8745\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3119 - accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3059 - accuracy: 0.8800\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3027 - accuracy: 0.8813\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.2999 - accuracy: 0.8820\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.2966 - accuracy: 0.8834\n",
      "Score for fold 2: loss of 1.528048598041065; accuracy of 49.55716586151369%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5041 - accuracy: 0.8034\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3691 - accuracy: 0.8593\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3451 - accuracy: 0.8677\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3323 - accuracy: 0.8730\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3255 - accuracy: 0.8756\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3204 - accuracy: 0.8776\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3124 - accuracy: 0.8804\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3132 - accuracy: 0.8805\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3093 - accuracy: 0.8819 0s - loss: 0.3\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3088 - accuracy: 0.8823\n",
      "Score for fold 3: loss of 0.562207725412401; accuracy of 87.4597423510467%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4919 - accuracy: 0.8106\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3626 - accuracy: 0.8611\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3380 - accuracy: 0.8697\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3272 - accuracy: 0.8732\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3201 - accuracy: 0.8754\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3117 - accuracy: 0.8779\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3082 - accuracy: 0.8805\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3056 - accuracy: 0.8806\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3015 - accuracy: 0.8824\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3004 - accuracy: 0.8829\n",
      "Score for fold 4: loss of 1.1463259256950276; accuracy of 61.89613526570048%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 951us/step - loss: 0.4901 - accuracy: 0.8087\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 993us/step - loss: 0.3625 - accuracy: 0.8631\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 985us/step - loss: 0.3378 - accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 966us/step - loss: 0.3246 - accuracy: 0.8748\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 975us/step - loss: 0.3177 - accuracy: 0.8772\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 991us/step - loss: 0.3108 - accuracy: 0.8795\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3094 - accuracy: 0.8797\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3069 - accuracy: 0.8804\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3050 - accuracy: 0.8818\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 866us/step - loss: 0.3008 - accuracy: 0.8838\n",
      "Score for fold 5: loss of 1.7610556886376942; accuracy of 40.27777777777778%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 848us/step - loss: 0.5020 - accuracy: 0.8015\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 986us/step - loss: 0.3730 - accuracy: 0.8559\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3486 - accuracy: 0.8636\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 887us/step - loss: 0.3370 - accuracy: 0.8689\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 950us/step - loss: 0.3287 - accuracy: 0.8719\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 932us/step - loss: 0.3221 - accuracy: 0.8752\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 891us/step - loss: 0.3182 - accuracy: 0.8771\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 875us/step - loss: 0.3127 - accuracy: 0.8786\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 916us/step - loss: 0.3106 - accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 891us/step - loss: 0.3083 - accuracy: 0.8803\n",
      "Score for fold 6: loss of 0.5823300871608146; accuracy of 83.35346215780999%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4947 - accuracy: 0.8058\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 988us/step - loss: 0.3652 - accuracy: 0.8595\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 952us/step - loss: 0.3427 - accuracy: 0.8664\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3313 - accuracy: 0.8706\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3248 - accuracy: 0.8730\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3206 - accuracy: 0.8750\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3159 - accuracy: 0.8761\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3110 - accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3099 - accuracy: 0.8785 0s - loss: 0.3099 - accuracy: 0.87\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3073 - accuracy: 0.8798\n",
      "Score for fold 7: loss of 0.8166594869665645; accuracy of 67.00885668276972%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4846 - accuracy: 0.8123\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3597 - accuracy: 0.8644\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3367 - accuracy: 0.8719 0s - loss: 0.3366 - accuracy: 0.\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3238 - accuracy: 0.8768\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3154 - accuracy: 0.8790\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3114 - accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3058 - accuracy: 0.8832\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3012 - accuracy: 0.8857\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2967 - accuracy: 0.8868 1s - l - ETA: 0s - loss: 0.2970 - accu\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2958 - accuracy: 0.8871 0s - loss: 0.2958 - accuracy: 0.\n",
      "Score for fold 8: loss of 1.830106995627884; accuracy of 59.60144927536232%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.4953 - accuracy: 0.8055 0s - loss: 0.5020 - accuracy - ETA: 0s - loss:\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3676 - accuracy: 0.8571\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3439 - accuracy: 0.8647\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.3338 - accuracy: 0.8688\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3253 - accuracy: 0.8720\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3210 - accuracy: 0.8739\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3157 - accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3139 - accuracy: 0.8760 1s - - ETA: 0s - loss: 0.314\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3102 - accuracy: 0.8778\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.3084 - accuracy: 0.8781\n",
      "Score for fold 9: loss of 0.5618358487595133; accuracy of 81.38083735909822%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.5056 - accuracy: 0.7992\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3702 - accuracy: 0.8549\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3414 - accuracy: 0.8653\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3265 - accuracy: 0.8712\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3172 - accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3110 - accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3057 - accuracy: 0.8786\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3032 - accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3032 - accuracy: 0.8800\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3001 - accuracy: 0.8810\n",
      "Score for fold 10: loss of 0.9041861664129552; accuracy of 78.18035426731079%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.5000 - accuracy: 0.8025\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3721 - accuracy: 0.8563\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3473 - accuracy: 0.8645\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3319 - accuracy: 0.8696 0s - loss: 0.3319 - accuracy: 0.86\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3201 - accuracy: 0.8742 0s - loss: 0\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3146 - accuracy: 0.8762\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3101 - accuracy: 0.8781 0s - loss: 0.3101 - accuracy: \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3076 - accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3030 - accuracy: 0.8813\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3009 - accuracy: 0.8817\n",
      "Score for fold 11: loss of 0.7639384787655683; accuracy of 74.45652173913044%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4859 - accuracy: 0.8109\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3571 - accuracy: 0.8649\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3335 - accuracy: 0.8725\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3220 - accuracy: 0.8746\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3137 - accuracy: 0.8773\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3083 - accuracy: 0.8801\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3037 - accuracy: 0.8812\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3003 - accuracy: 0.8821A: 0s - loss: 0.3003 - accura\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2974 - accuracy: 0.8840 0s - loss: 0\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2946 - accuracy: 0.8850\n",
      "Score for fold 12: loss of 1.39349656241233; accuracy of 47.50402576489533%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4990 - accuracy: 0.8048\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3679 - accuracy: 0.8607\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3418 - accuracy: 0.8705\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3285 - accuracy: 0.8745\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3193 - accuracy: 0.8785\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3136 - accuracy: 0.8800\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3092 - accuracy: 0.8815\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3052 - accuracy: 0.8830\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3024 - accuracy: 0.8830\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3001 - accuracy: 0.8847\n",
      "Score for fold 13: loss of 0.8063018283633899; accuracy of 78.16022544283415%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.5007 - accuracy: 0.8042\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3663 - accuracy: 0.8597\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3399 - accuracy: 0.8688 2s - loss: 0\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3295 - accuracy: 0.8728\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3239 - accuracy: 0.8745\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3178 - accuracy: 0.8770\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3140 - accuracy: 0.8779\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3086 - accuracy: 0.8794\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3070 - accuracy: 0.8797\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3043 - accuracy: 0.8810\n",
      "Score for fold 14: loss of 2.2689984949071698; accuracy of 43.84057971014493%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4969 - accuracy: 0.8052\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3604 - accuracy: 0.8606\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3360 - accuracy: 0.8679 2s - loss: 0 - ETA: 0s - l\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3225 - accuracy: 0.8728 0s - loss: 0.3223 - ac\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3161 - accuracy: 0.8758 1s - los - ETA\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3100 - accuracy: 0.8784 1s - loss: 0.3108 - accu - ETA: 0s - loss: 0.3106 -  - ETA: 0s - loss:\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3065 - accuracy: 0.8796 1s - loss: 0 - ETA: 0s - loss: 0.3063 - ac\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3018 - accuracy: 0.8811\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2990 - accuracy: 0.8821\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2961 - accuracy: 0.8841\n",
      "Score for fold 15: loss of 0.8024364940780132; accuracy of 78.56280193236715%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4975 - accuracy: 0.8027\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3742 - accuracy: 0.8536\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3509 - accuracy: 0.8619 0s - loss: 0.3511 - accuracy: 0. - ETA: 0s - loss: 0.3511 \n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3404 - accuracy: 0.8650\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3340 - accuracy: 0.8681\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3274 - accuracy: 0.8702\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3235 - accuracy: 0.8713\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3199 - accuracy: 0.8726 \n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3187 - accuracy: 0.8727\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3150 - accuracy: 0.8746\n",
      "Score for fold 16: loss of 0.42529012354725565; accuracy of 87.76167471819646%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.5024 - accuracy: 0.8050\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3774 - accuracy: 0.8538\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3530 - accuracy: 0.8623\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3412 - accuracy: 0.8647\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.3338 - accuracy: 0.8670\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3256 - accuracy: 0.8699\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3228 - accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3189 - accuracy: 0.8732\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3144 - accuracy: 0.8743\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3143 - accuracy: 0.8745\n",
      "Score for fold 17: loss of 0.5913985059967778; accuracy of 68.37761674718197%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 5s 774us/step - loss: 0.4940 - accuracy: 0.8054\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 6s 871us/step - loss: 0.3686 - accuracy: 0.8576\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 10s 1ms/step - loss: 0.3460 - accuracy: 0.8648\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.3329 - accuracy: 0.8701\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.3245 - accuracy: 0.8735\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.3167 - accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.3127 - accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.3117 - accuracy: 0.8785\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 5s 789us/step - loss: 0.3084 - accuracy: 0.8799\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.3049 - accuracy: 0.8817\n",
      "Score for fold 18: loss of 3.365384145827301; accuracy of 34.98389694041868%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4857 - accuracy: 0.8096\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3662 - accuracy: 0.8588\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3410 - accuracy: 0.8677\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3287 - accuracy: 0.8723\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3210 - accuracy: 0.8738\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3160 - accuracy: 0.8757\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3126 - accuracy: 0.8781\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 941us/step - loss: 0.3072 - accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3045 - accuracy: 0.8809\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3044 - accuracy: 0.8811\n",
      "Score for fold 19: loss of 0.9016994455418341; accuracy of 63.38566827697263%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.5025 - accuracy: 0.8041\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3745 - accuracy: 0.8579\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3480 - accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3327 - accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3255 - accuracy: 0.8740\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3175 - accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3140 - accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3105 - accuracy: 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3078 - accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3059 - accuracy: 0.8811\n",
      "Score for fold 20: loss of 0.20666198333651847; accuracy of 89.65378421900161%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4946 - accuracy: 0.8083\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3690 - accuracy: 0.8602\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3457 - accuracy: 0.8673\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3320 - accuracy: 0.8719\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3218 - accuracy: 0.8754\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3169 - accuracy: 0.8763\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3113 - accuracy: 0.8785 0s - loss:\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3066 - accuracy: 0.8804\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3065 - accuracy: 0.8802\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 947us/step - loss: 0.3024 - accuracy: 0.8824\n",
      "Score for fold 21: loss of 1.6105672103801258; accuracy of 56.803542673107884%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 922us/step - loss: 0.4980 - accuracy: 0.8046\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3708 - accuracy: 0.8563\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3475 - accuracy: 0.8638\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3367 - accuracy: 0.8678\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3294 - accuracy: 0.8697\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3208 - accuracy: 0.8737\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3165 - accuracy: 0.8742\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3121 - accuracy: 0.8769\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3093 - accuracy: 0.8787\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3061 - accuracy: 0.8793\n",
      "Score for fold 22: loss of 0.5077942548369873; accuracy of 87.3792270531401%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4990 - accuracy: 0.8033\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3733 - accuracy: 0.8556\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3468 - accuracy: 0.8659\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3330 - accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3256 - accuracy: 0.8735\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3172 - accuracy: 0.8764\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3111 - accuracy: 0.8786\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3071 - accuracy: 0.8816\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3044 - accuracy: 0.8819\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3016 - accuracy: 0.8827\n",
      "Score for fold 23: loss of 0.5508493119490824; accuracy of 81.21980676328504%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5061 - accuracy: 0.8017\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3797 - accuracy: 0.8548\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3537 - accuracy: 0.8633\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3445 - accuracy: 0.8657\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3350 - accuracy: 0.8693\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3273 - accuracy: 0.8722\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3232 - accuracy: 0.8738\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3193 - accuracy: 0.8761\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3140 - accuracy: 0.8774\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3126 - accuracy: 0.8779\n",
      "Score for fold 24: loss of 0.4286198262942598; accuracy of 83.61513687600645%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5062 - accuracy: 0.8035\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3764 - accuracy: 0.8555\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3486 - accuracy: 0.8655\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3359 - accuracy: 0.8695\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3255 - accuracy: 0.8744\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3201 - accuracy: 0.8749\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3162 - accuracy: 0.8775\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3140 - accuracy: 0.8785\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3089 - accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3046 - accuracy: 0.8816\n",
      "Score for fold 25: loss of 0.8802579280888763; accuracy of 81.11916264090178%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5123 - accuracy: 0.7942\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3798 - accuracy: 0.8501\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3547 - accuracy: 0.8602\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3434 - accuracy: 0.8643\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3357 - accuracy: 0.8679\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3290 - accuracy: 0.8706\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3258 - accuracy: 0.8715\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3225 - accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3166 - accuracy: 0.8766\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3146 - accuracy: 0.8767\n",
      "Score for fold 26: loss of 0.9577749511455923; accuracy of 62.68115942028986%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.5066 - accuracy: 0.8003\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3735 - accuracy: 0.8570\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3499 - accuracy: 0.8660\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3360 - accuracy: 0.8707\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3278 - accuracy: 0.8727\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3229 - accuracy: 0.8754\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3172 - accuracy: 0.8766\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3148 - accuracy: 0.8770\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3108 - accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3062 - accuracy: 0.8808\n",
      "Score for fold 27: loss of 0.6114575950450993; accuracy of 77.79790660225443%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4860 - accuracy: 0.8115\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3583 - accuracy: 0.8639\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3342 - accuracy: 0.8723\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3205 - accuracy: 0.8751\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3135 - accuracy: 0.8773\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3077 - accuracy: 0.8792\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3042 - accuracy: 0.8806\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3018 - accuracy: 0.8813\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2984 - accuracy: 0.8831\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2971 - accuracy: 0.8831\n",
      "Score for fold 28: loss of 2.837761624747716; accuracy of 28.844605475040257%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5003 - accuracy: 0.8027\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3721 - accuracy: 0.8571: 0s -\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3436 - accuracy: 0.8676\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3335 - accuracy: 0.8710\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3238 - accuracy: 0.8743\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3170 - accuracy: 0.8776\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3155 - accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3107 - accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3089 - accuracy: 0.8802\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3055 - accuracy: 0.8810: 0s - loss: 0.3055 - accuracy: 0.\n",
      "Score for fold 29: loss of 0.5517044730333481; accuracy of 68.29710144927536%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.5083 - accuracy: 0.7983\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3777 - accuracy: 0.8515\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3503 - accuracy: 0.8609\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3355 - accuracy: 0.8672\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3283 - accuracy: 0.8693\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3212 - accuracy: 0.8721\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3170 - accuracy: 0.8739\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3156 - accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3119 - accuracy: 0.8757\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3096 - accuracy: 0.8761\n",
      "Score for fold 30: loss of 0.8678766820369208; accuracy of 82.89049919484702%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5018 - accuracy: 0.8053\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3762 - accuracy: 0.8574\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3546 - accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3420 - accuracy: 0.8685\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3337 - accuracy: 0.8715\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3288 - accuracy: 0.8729\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3223 - accuracy: 0.8749\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3196 - accuracy: 0.8760: 0s - loss: 0.3197 - accuracy: 0.\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3161 - accuracy: 0.8774\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3125 - accuracy: 0.8791\n",
      "Score for fold 31: loss of 0.3024770279575698; accuracy of 83.45410628019324%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5016 - accuracy: 0.8015\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3798 - accuracy: 0.8524\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3551 - accuracy: 0.8615\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3426 - accuracy: 0.8658\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3333 - accuracy: 0.8709\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3258 - accuracy: 0.8737\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3196 - accuracy: 0.8762\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3160 - accuracy: 0.8764\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3134 - accuracy: 0.8777\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3091 - accuracy: 0.8796\n",
      "Score for fold 32: loss of 0.8478825528320111; accuracy of 61.090982286634464%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5043 - accuracy: 0.8006\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3828 - accuracy: 0.8504\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3581 - accuracy: 0.8596\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3460 - accuracy: 0.8647\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3381 - accuracy: 0.8671\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3321 - accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3255 - accuracy: 0.8724\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3233 - accuracy: 0.8738\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3185 - accuracy: 0.8754\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3161 - accuracy: 0.8765\n",
      "Score for fold 33: loss of 0.4860432565574243; accuracy of 87.5402576489533%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5008 - accuracy: 0.8049\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3725 - accuracy: 0.8579\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3481 - accuracy: 0.8666\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3355 - accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3255 - accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3195 - accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3161 - accuracy: 0.8777\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3132 - accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3104 - accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3075 - accuracy: 0.8804\n",
      "Score for fold 34: loss of 0.3536606361165753; accuracy of 87.15780998389694%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5064 - accuracy: 0.8035\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3726 - accuracy: 0.8585\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3506 - accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3388 - accuracy: 0.8699\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3323 - accuracy: 0.8721\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3266 - accuracy: 0.8742\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3218 - accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3203 - accuracy: 0.8769\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3176 - accuracy: 0.8773\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3142 - accuracy: 0.8787\n",
      "Score for fold 35: loss of 0.506048986196142; accuracy of 83.59500805152979%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.5048 - accuracy: 0.8028\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3740 - accuracy: 0.8552\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3492 - accuracy: 0.8639\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3342 - accuracy: 0.8693\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3248 - accuracy: 0.8728: 0s\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3178 - accuracy: 0.8756\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3152 - accuracy: 0.8756\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3125 - accuracy: 0.8765\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3092 - accuracy: 0.8784\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3079 - accuracy: 0.8790\n",
      "Score for fold 36: loss of 0.49745190681195284; accuracy of 86.37278582930756%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.5053 - accuracy: 0.8040\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3774 - accuracy: 0.8587\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 10s 1ms/step - loss: 0.3518 - accuracy: 0.8677\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3382 - accuracy: 0.8723\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3313 - accuracy: 0.8743\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3258 - accuracy: 0.8760: 0s - loss: 0.3257 - accuracy: 0.87\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3190 - accuracy: 0.8794\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3146 - accuracy: 0.8809\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3115 - accuracy: 0.8825: 0s - loss: 0.3112 - ac\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.3115 - accuracy: 0.8826\n",
      "Score for fold 37: loss of 0.8248809854477359; accuracy of 77.05314009661835%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5091 - accuracy: 0.8007\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3774 - accuracy: 0.8534\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3485 - accuracy: 0.8647\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3330 - accuracy: 0.8698\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3259 - accuracy: 0.8731: 1s - - ETA: 0s -\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3185 - accuracy: 0.8759\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3129 - accuracy: 0.8785\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3083 - accuracy: 0.8806\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3039 - accuracy: 0.8827\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3030 - accuracy: 0.8828\n",
      "Score for fold 38: loss of 0.3265576028950777; accuracy of 88.08373590982286%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.5088 - accuracy: 0.8009\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3740 - accuracy: 0.8567\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3469 - accuracy: 0.8651\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3339 - accuracy: 0.8700: 0s - loss: 0.3338 - accuracy: 0.87\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3252 - accuracy: 0.8742\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3196 - accuracy: 0.8759\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3125 - accuracy: 0.8786\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3088 - accuracy: 0.8806\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3067 - accuracy: 0.8808\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3048 - accuracy: 0.8812\n",
      "Score for fold 39: loss of 1.065624365954804; accuracy of 56.09903381642513%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.4902 - accuracy: 0.8060\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3617 - accuracy: 0.8587\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3363 - accuracy: 0.8669\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3254 - accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3172 - accuracy: 0.8737\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3101 - accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 26s 4ms/step - loss: 0.3075 - accuracy: 0.8777\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3038 - accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3016 - accuracy: 0.8799\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 28s 4ms/step - loss: 0.2972 - accuracy: 0.8811\n",
      "Score for fold 40: loss of 2.311640059494454; accuracy of 26.489533011272144%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 21s 3ms/step - loss: 0.5056 - accuracy: 0.8022\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3761 - accuracy: 0.8569\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 26s 4ms/step - loss: 0.3498 - accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3391 - accuracy: 0.8698\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 23s 4ms/step - loss: 0.3299 - accuracy: 0.8734\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3223 - accuracy: 0.8748\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 25s 4ms/step - loss: 0.3178 - accuracy: 0.8765 \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 27s 4ms/step - loss: 0.3159 - accuracy: 0.8772\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 25s 4ms/step - loss: 0.3136 - accuracy: 0.8776\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3097 - accuracy: 0.8785\n",
      "Score for fold 41: loss of 0.546151663194712; accuracy of 78.64331723027375%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.4996 - accuracy: 0.8067\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3699 - accuracy: 0.8585\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3432 - accuracy: 0.8680\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3287 - accuracy: 0.8745 0s - loss:\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3205 - accuracy: 0.8770\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3137 - accuracy: 0.8795\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 23s 4ms/step - loss: 0.3104 - accuracy: 0.8794\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 23s 4ms/step - loss: 0.3082 - accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 25s 4ms/step - loss: 0.3021 - accuracy: 0.8831\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3010 - accuracy: 0.8833\n",
      "Score for fold 42: loss of 0.5958706078096331; accuracy of 79.30756843800322%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.5032 - accuracy: 0.8020 0s -\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3724 - accuracy: 0.8587\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 25s 4ms/step - loss: 0.3464 - accuracy: 0.8689\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 23s 4ms/step - loss: 0.3338 - accuracy: 0.8727\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3241 - accuracy: 0.8762\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3189 - accuracy: 0.8781\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3116 - accuracy: 0.8809\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3091 - accuracy: 0.8817\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 21s 3ms/step - loss: 0.3064 - accuracy: 0.8826\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3025 - accuracy: 0.8842\n",
      "Score for fold 43: loss of 0.5660888225999116; accuracy of 78.94524959742351%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.4969 - accuracy: 0.8063 0s - loss: 0.500 - ETA: 0s - loss: 0.4971 - accuracy: 0.80\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3677 - accuracy: 0.8598\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3435 - accuracy: 0.8680\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.3296 - accuracy: 0.8731 2s\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3193 - accuracy: 0.8767\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3121 - accuracy: 0.8790\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 24s 4ms/step - loss: 0.3078 - accuracy: 0.8804\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 23s 3ms/step - loss: 0.3017 - accuracy: 0.8820\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.2993 - accuracy: 0.8833\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 22s 3ms/step - loss: 0.2948 - accuracy: 0.8845\n",
      "Score for fold 44: loss of 1.3804131544981715; accuracy of 51.77133655394525%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.4914 - accuracy: 0.8091\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3688 - accuracy: 0.8572\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 23s 3ms/step - loss: 0.3442 - accuracy: 0.8659\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3324 - accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 24s 4ms/step - loss: 0.3243 - accuracy: 0.8725\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3194 - accuracy: 0.8748 0s - loss: 0\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3157 - accuracy: 0.8759\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3132 - accuracy: 0.8764\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3110 - accuracy: 0.8777\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 22s 3ms/step - loss: 0.3105 - accuracy: 0.8779\n",
      "Score for fold 45: loss of 1.0054385895426903; accuracy of 55.434782608695656%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 67.04203297588944 - Accuracy: 37.07729468599034%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.528048598041065 - Accuracy: 49.55716586151369%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.562207725412401 - Accuracy: 87.4597423510467%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1463259256950276 - Accuracy: 61.89613526570048%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.7610556886376942 - Accuracy: 40.27777777777778%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.5823300871608146 - Accuracy: 83.35346215780999%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.8166594869665645 - Accuracy: 67.00885668276972%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.830106995627884 - Accuracy: 59.60144927536232%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.5618358487595133 - Accuracy: 81.38083735909822%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.9041861664129552 - Accuracy: 78.18035426731079%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 0.7639384787655683 - Accuracy: 74.45652173913044%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.39349656241233 - Accuracy: 47.50402576489533%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 0.8063018283633899 - Accuracy: 78.16022544283415%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.2689984949071698 - Accuracy: 43.84057971014493%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 0.8024364940780132 - Accuracy: 78.56280193236715%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 0.42529012354725565 - Accuracy: 87.76167471819646%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.5913985059967778 - Accuracy: 68.37761674718197%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 3.365384145827301 - Accuracy: 34.98389694041868%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.9016994455418341 - Accuracy: 63.38566827697263%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.20666198333651847 - Accuracy: 89.65378421900161%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 1.6105672103801258 - Accuracy: 56.803542673107884%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 0.5077942548369873 - Accuracy: 87.3792270531401%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 0.5508493119490824 - Accuracy: 81.21980676328504%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.4286198262942598 - Accuracy: 83.61513687600645%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.8802579280888763 - Accuracy: 81.11916264090178%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 0.9577749511455923 - Accuracy: 62.68115942028986%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.6114575950450993 - Accuracy: 77.79790660225443%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 2.837761624747716 - Accuracy: 28.844605475040257%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 0.5517044730333481 - Accuracy: 68.29710144927536%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 0.8678766820369208 - Accuracy: 82.89049919484702%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.3024770279575698 - Accuracy: 83.45410628019324%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 0.8478825528320111 - Accuracy: 61.090982286634464%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.4860432565574243 - Accuracy: 87.5402576489533%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 0.3536606361165753 - Accuracy: 87.15780998389694%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.506048986196142 - Accuracy: 83.59500805152979%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.49745190681195284 - Accuracy: 86.37278582930756%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.8248809854477359 - Accuracy: 77.05314009661835%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 0.3265576028950777 - Accuracy: 88.08373590982286%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.065624365954804 - Accuracy: 56.09903381642513%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 2.311640059494454 - Accuracy: 26.489533011272144%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.546151663194712 - Accuracy: 78.64331723027375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.5958706078096331 - Accuracy: 79.30756843800322%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.5660888225999116 - Accuracy: 78.94524959742351%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.3804131544981715 - Accuracy: 51.77133655394525%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 1.0054385895426903 - Accuracy: 55.434782608695656%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 68.98148148148148 (+- 17.624390735883996)\n",
      "> Loss: 2.4373619919299196\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = X_train_df['Subject_ID'].values \n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/N1/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/10_TF_ANN_Only_ACC/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/10_TF_ANN_Only_ACC') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/10_TF_ANN_Only_ACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 [==============================] - 0s 781us/step - loss: 0.8316 - accuracy: 0.6875\n",
      "Test loss, Test acc: [0.8315563062148092, 0.6874523264683448]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17746,  1483,   316,     4],\n",
       "       [ 3986, 13332,  2788,  3138],\n",
       "       [  327,   731,   690,     0],\n",
       "       [    0,  1953,    25,   677]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 362
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le.classes_, yticklabels = le.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data Rolled ACC Only - Confusion Matrix')\n",
    "plt.savefig('Duke_Data_ANN_Rolled_ACC_Only_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.687 \n",
      "F1 Score: 0.678\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

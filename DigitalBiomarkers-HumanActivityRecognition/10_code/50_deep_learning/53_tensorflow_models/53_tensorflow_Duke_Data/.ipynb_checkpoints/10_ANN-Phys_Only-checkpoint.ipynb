{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 1: Artificial Neural Network - Physiological Sensors Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **rolled_data.csv**\n",
    "\n",
    "This notebook label encodes Subject_ID and Activity (our y variable). It then one-hot encodes Subject_ID to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('rolled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.reindex(columns = ['TEMP', 'EDA', 'BVP', 'HR', 'Subject_ID', 'Round', 'Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.35600</td>\n",
       "      <td>0.265940</td>\n",
       "      <td>-0.21875</td>\n",
       "      <td>76.10525</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.35475</td>\n",
       "      <td>0.265556</td>\n",
       "      <td>-1.07075</td>\n",
       "      <td>75.96875</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.35350</td>\n",
       "      <td>0.265140</td>\n",
       "      <td>-0.75950</td>\n",
       "      <td>75.83375</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TEMP       EDA      BVP        HR Subject_ID  Round  Activity\n",
       "0  32.35600  0.265940 -0.21875  76.10525     19-001      1  Baseline\n",
       "1  32.35475  0.265556 -1.07075  75.96875     19-001      1  Baseline\n",
       "2  32.35350  0.265140 -0.75950  75.83375     19-001      1  Baseline"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1af4db49b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mactivity_name_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Activity'] = le.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "df['Subject_ID'] = le1.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train sets (by Subject ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = list(df['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 7) (47196, 7)\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Subject_ID'].isin(ID_list[:45])]\n",
    "test = df[df['Subject_ID'].isin(ID_list[45:])]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the test train split can be changed by changing the index below. For our purposes, n = 45 for train and n = 10 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 5) (216108,) (47196, 5) (47196,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:,0:5]\n",
    "X_test = test.iloc[:,0:5]\n",
    "\n",
    "y_train = train.iloc[:,-1].values\n",
    "y_test = test.iloc[:,-1].values\n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create X_train_df below so that we are able to use the Subject_ID column later on, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID, so that it may be used as a variable in our model. This allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0\n",
    "\n",
    "combined = pd.concat([X_train, X_test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 59) (47196, 59) 263304\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:4] = ss.fit_transform(X_train.iloc[:,:4])\n",
    "X_test.iloc[:,:4] = ss.transform(X_test.iloc[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 16, 18, 19,\n",
       "       20, 22, 24, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 41, 42,\n",
       "       43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df['Subject_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.9001 - accuracy: 0.6344 0s - loss:\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.7539 - accuracy: 0.6909 1s -\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.7098 - accuracy: 0.7080\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.6850 - accuracy: 0.7179 0s -\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6701 - accuracy: 0.7225\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6605 - accuracy: 0.7253 0s - loss: 0.6610 - accu\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6524 - accuracy: 0.7282\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6460 - accuracy: 0.7312\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6413 - accuracy: 0.7334\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6358 - accuracy: 0.7352\n",
      "Score for fold 1: loss of 0.9121555423367043; accuracy of 67.25040257648953%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.8807 - accuracy: 0.6371\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.7308 - accuracy: 0.6944 1s - los - ETA: 0s - loss: 0.7309 - accuracy: \n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6870 - accuracy: 0.7087\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.6653 - accuracy: 0.7149\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.6541 - accuracy: 0.7221\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6457 - accuracy: 0.7232\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.6389 - accuracy: 0.7278\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6305 - accuracy: 0.7302\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6247 - accuracy: 0.7335\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6198 - accuracy: 0.7348\n",
      "Score for fold 2: loss of 6.581012145140705; accuracy of 35.18518518518518%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.8880 - accuracy: 0.6369\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.7370 - accuracy: 0.6946\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6859 - accuracy: 0.7136\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6608 - accuracy: 0.7228\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6429 - accuracy: 0.7292\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.6323 - accuracy: 0.7327\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6207 - accuracy: 0.7361ETA: 2s - loss: 0.6208 -  - ETA: 1s\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6166 - accuracy: 0.7370\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6098 - accuracy: 0.7400\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.6053 - accuracy: 0.7406 0s - los\n",
      "Score for fold 3: loss of 2.3449566419987717; accuracy of 41.0024154589372%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.8902 - accuracy: 0.6386 4s - l\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.7439 - accuracy: 0.6948\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6931 - accuracy: 0.7114\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6714 - accuracy: 0.7183\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6569 - accuracy: 0.7241\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.6449 - accuracy: 0.7275 0s - l\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6357 - accuracy: 0.7331 0s - loss: 0.6355 \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6293 - accuracy: 0.7359- ETA: 0s - loss: 0.6\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6222 - accuracy: 0.7394 1s - loss: 0.6232 - accu\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6188 - accuracy: 0.7416\n",
      "Score for fold 4: loss of 2.2035863733793803; accuracy of 39.95571658615137%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.8971 - accuracy: 0.6373 \n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.7374 - accuracy: 0.6986 \n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6843 - accuracy: 0.7182\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6580 - accuracy: 0.7268 0s - los - ETA: 0s - loss: 0.6580 - accuracy: 0.72\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6425 - accuracy: 0.7318\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6319 - accuracy: 0.7345 0s - loss: 0.6329 \n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6236 - accuracy: 0.7370\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6170 - accuracy: 0.7392\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6108 - accuracy: 0.7417\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6088 - accuracy: 0.7426\n",
      "Score for fold 5: loss of 4.490456232894062; accuracy of 37.13768115942029%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.8973 - accuracy: 0.6341\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.7467 - accuracy: 0.6905\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6922 - accuracy: 0.7120: 0s - loss: 0.6925 - accuracy: \n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.6644 - accuracy: 0.7240\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6483 - accuracy: 0.7302 1s - loss: 0.6487 - accuracy:  - E\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6413 - accuracy: 0.7321 0s - loss: 0.6417 - accuracy - ETA: 0s - loss: 0.6413 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6342 - accuracy: 0.7336\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6268 - accuracy: 0.7368\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6230 - accuracy: 0.7369\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6197 - accuracy: 0.7382\n",
      "Score for fold 6: loss of 4.933298281233475; accuracy of 50.78502415458937%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.8883 - accuracy: 0.6429\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 29s 4ms/step - loss: 0.7491 - accuracy: 0.6936\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.7014 - accuracy: 0.7098\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.6716 - accuracy: 0.7207\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.6590 - accuracy: 0.7258\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6493 - accuracy: 0.7297\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6420 - accuracy: 0.7312\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6367 - accuracy: 0.7330 3s - loss: 0.6397 -  - ETA: \n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.6290 - accuracy: 0.7371\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6228 - accuracy: 0.7404\n",
      "Score for fold 7: loss of 1.451400052042207; accuracy of 50.44283413848631%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.8952 - accuracy: 0.6360\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.7410 - accuracy: 0.6972\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6897 - accuracy: 0.7146\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6661 - accuracy: 0.7235 0s - l\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.6481 - accuracy: 0.7308\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.6381 - accuracy: 0.7357\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6305 - accuracy: 0.7394\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6238 - accuracy: 0.7416\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6178 - accuracy: 0.7442\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6113 - accuracy: 0.7453\n",
      "Score for fold 8: loss of 1.1064116640223398; accuracy of 60.6280193236715%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.8980 - accuracy: 0.6349\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.7549 - accuracy: 0.6848\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.7094 - accuracy: 0.7027\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6821 - accuracy: 0.7146\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6675 - accuracy: 0.7191\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6572 - accuracy: 0.7225\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6435 - accuracy: 0.7276\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 998us/step - loss: 0.6403 - accuracy: 0.7287\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6317 - accuracy: 0.7334\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6256 - accuracy: 0.7358\n",
      "Score for fold 9: loss of 0.9079315734370319; accuracy of 65.76086956521739%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8882 - accuracy: 0.6422\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7320 - accuracy: 0.6945\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6844 - accuracy: 0.7081\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 992us/step - loss: 0.6576 - accuracy: 0.7189\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 978us/step - loss: 0.6396 - accuracy: 0.7254\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6273 - accuracy: 0.7304\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6200 - accuracy: 0.7332\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6129 - accuracy: 0.7370\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6086 - accuracy: 0.7378\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6049 - accuracy: 0.7392\n",
      "Score for fold 10: loss of 10.340666910372494; accuracy of 36.97665056360708%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8913 - accuracy: 0.6372\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7440 - accuracy: 0.6914\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6976 - accuracy: 0.7090\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6730 - accuracy: 0.7197\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6570 - accuracy: 0.7262\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6462 - accuracy: 0.7295\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6359 - accuracy: 0.7337\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6286 - accuracy: 0.7371\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 968us/step - loss: 0.6229 - accuracy: 0.7392\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 924us/step - loss: 0.6198 - accuracy: 0.7405\n",
      "Score for fold 11: loss of 13.393314939188725; accuracy of 34.40016103059581%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 909us/step - loss: 0.8914 - accuracy: 0.6368\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 932us/step - loss: 0.7402 - accuracy: 0.6899\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 977us/step - loss: 0.6930 - accuracy: 0.7059\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 939us/step - loss: 0.6707 - accuracy: 0.7136\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 924us/step - loss: 0.6574 - accuracy: 0.7192\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 909us/step - loss: 0.6469 - accuracy: 0.7240\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 911us/step - loss: 0.6390 - accuracy: 0.7272\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 932us/step - loss: 0.6308 - accuracy: 0.7302\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 881us/step - loss: 0.6258 - accuracy: 0.7335\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 891us/step - loss: 0.6226 - accuracy: 0.7340\n",
      "Score for fold 12: loss of 3.794035475971042; accuracy of 35.30595813204509%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 881us/step - loss: 0.8841 - accuracy: 0.6411\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 907us/step - loss: 0.7390 - accuracy: 0.6932\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 893us/step - loss: 0.6933 - accuracy: 0.7114\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 888us/step - loss: 0.6690 - accuracy: 0.7206\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 885us/step - loss: 0.6525 - accuracy: 0.7270\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 884us/step - loss: 0.6412 - accuracy: 0.7319\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 894us/step - loss: 0.6314 - accuracy: 0.7347\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 870us/step - loss: 0.6225 - accuracy: 0.7388\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 884us/step - loss: 0.6160 - accuracy: 0.7407\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 874us/step - loss: 0.6100 - accuracy: 0.7413\n",
      "Score for fold 13: loss of 4.3053552401828075; accuracy of 33.27294685990338%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 904us/step - loss: 0.8944 - accuracy: 0.6396\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 6s 908us/step - loss: 0.7398 - accuracy: 0.6958\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 915us/step - loss: 0.6901 - accuracy: 0.7147\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 909us/step - loss: 0.6688 - accuracy: 0.7231\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 906us/step - loss: 0.6544 - accuracy: 0.7286\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6432 - accuracy: 0.7335\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6378 - accuracy: 0.7347\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6298 - accuracy: 0.7390\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6240 - accuracy: 0.7407: \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6166 - accuracy: 0.7438\n",
      "Score for fold 14: loss of 3.6143067799142643; accuracy of 26.42914653784219%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.9112 - accuracy: 0.6307\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7674 - accuracy: 0.6840\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7140 - accuracy: 0.7068\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 994us/step - loss: 0.6861 - accuracy: 0.7156\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 995us/step - loss: 0.6690 - accuracy: 0.7208\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 943us/step - loss: 0.6578 - accuracy: 0.7254\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 990us/step - loss: 0.6499 - accuracy: 0.7276\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 959us/step - loss: 0.6425 - accuracy: 0.7303\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 894us/step - loss: 0.6383 - accuracy: 0.7320\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 939us/step - loss: 0.6339 - accuracy: 0.7338\n",
      "Score for fold 15: loss of 3.0039012258851394; accuracy of 43.11594202898551%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 889us/step - loss: 0.8912 - accuracy: 0.6373\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 957us/step - loss: 0.7452 - accuracy: 0.68941s - l - ETA: 0s - loss: 0.7 - ETA: 0s - loss: 0.7460 - accu\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 911us/step - loss: 0.7025 - accuracy: 0.7049\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 978us/step - loss: 0.6795 - accuracy: 0.7128\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 929us/step - loss: 0.6665 - accuracy: 0.7170\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 894us/step - loss: 0.6545 - accuracy: 0.7231\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 869us/step - loss: 0.6447 - accuracy: 0.7276\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 911us/step - loss: 0.6375 - accuracy: 0.7304\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 921us/step - loss: 0.6329 - accuracy: 0.7316\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6257 - accuracy: 0.7359\n",
      "Score for fold 16: loss of 39.633907815684445; accuracy of 8.091787439613526%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 943us/step - loss: 0.9065 - accuracy: 0.6296\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 978us/step - loss: 0.7508 - accuracy: 0.6858\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 949us/step - loss: 0.6973 - accuracy: 0.7054\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 931us/step - loss: 0.6728 - accuracy: 0.7136\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 907us/step - loss: 0.6550 - accuracy: 0.7200\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 897us/step - loss: 0.6470 - accuracy: 0.7240\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 899us/step - loss: 0.6382 - accuracy: 0.7249\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 955us/step - loss: 0.6326 - accuracy: 0.72700s - loss: 0.633\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6305 - accuracy: 0.7279\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6246 - accuracy: 0.7307\n",
      "Score for fold 17: loss of 0.8880197524112587; accuracy of 58.313204508856685%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.8913 - accuracy: 0.6365\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 6s 941us/step - loss: 0.7428 - accuracy: 0.6966\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 6s 900us/step - loss: 0.6982 - accuracy: 0.7120\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 6s 911us/step - loss: 0.6694 - accuracy: 0.7213\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 6s 900us/step - loss: 0.6578 - accuracy: 0.7242\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 6s 897us/step - loss: 0.6470 - accuracy: 0.7278\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 6s 888us/step - loss: 0.6383 - accuracy: 0.7313\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 6s 889us/step - loss: 0.6309 - accuracy: 0.7342\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 6s 911us/step - loss: 0.6260 - accuracy: 0.7368\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 6s 886us/step - loss: 0.6221 - accuracy: 0.7385\n",
      "Score for fold 18: loss of 4.318453690302956; accuracy of 22.946859903381643%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 883us/step - loss: 0.9093 - accuracy: 0.6297\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7591 - accuracy: 0.6857\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 923us/step - loss: 0.7070 - accuracy: 0.7048\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 884us/step - loss: 0.6810 - accuracy: 0.7149\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 890us/step - loss: 0.6642 - accuracy: 0.7209\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6521 - accuracy: 0.7246\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 892us/step - loss: 0.6424 - accuracy: 0.72860s - loss:\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 910us/step - loss: 0.6368 - accuracy: 0.7301\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6285 - accuracy: 0.7347\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6227 - accuracy: 0.7358\n",
      "Score for fold 19: loss of 7.173457339800813; accuracy of 28.864734299516908%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.9016 - accuracy: 0.6337\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7513 - accuracy: 0.6906\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7008 - accuracy: 0.7066\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6761 - accuracy: 0.7146\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6585 - accuracy: 0.7215\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6493 - accuracy: 0.7234\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6404 - accuracy: 0.7268\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 926us/step - loss: 0.6333 - accuracy: 0.7306\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 977us/step - loss: 0.6295 - accuracy: 0.7319\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6240 - accuracy: 0.7360\n",
      "Score for fold 20: loss of 0.8688877393969975; accuracy of 61.35265700483091%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8942 - accuracy: 0.6362\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7452 - accuracy: 0.6901\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6967 - accuracy: 0.7073\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6733 - accuracy: 0.7145\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 934us/step - loss: 0.6575 - accuracy: 0.7213\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 913us/step - loss: 0.6482 - accuracy: 0.7236\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 906us/step - loss: 0.6394 - accuracy: 0.7271\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 901us/step - loss: 0.6318 - accuracy: 0.7286\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 889us/step - loss: 0.6248 - accuracy: 0.7324\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 915us/step - loss: 0.6206 - accuracy: 0.7351\n",
      "Score for fold 21: loss of 2.655126227368863; accuracy of 43.71980676328502%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8907 - accuracy: 0.6342\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.7433 - accuracy: 0.6909: 1s - loss: 0.7461 - accuracy: 0.68 - ETA: 1s -\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6988 - accuracy: 0.7073\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 989us/step - loss: 0.6764 - accuracy: 0.7175\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 987us/step - loss: 0.6616 - accuracy: 0.7235\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6517 - accuracy: 0.7272\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6433 - accuracy: 0.7313\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6354 - accuracy: 0.7324\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6322 - accuracy: 0.7339\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.6270 - accuracy: 0.7359\n",
      "Score for fold 22: loss of 3.88150986391997; accuracy of 60.97020933977456%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.8910 - accuracy: 0.6387 2s - loss: 0.9181 - accura - ETA\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7329 - accuracy: 0.6981\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6849 - accuracy: 0.7133\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6608 - accuracy: 0.7243\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6438 - accuracy: 0.7308\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6333 - accuracy: 0.7346\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6240 - accuracy: 0.7386\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6191 - accuracy: 0.7395\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6124 - accuracy: 0.7410\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.6073 - accuracy: 0.7428\n",
      "Score for fold 23: loss of 4.441856228041608; accuracy of 25.60386473429952%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8955 - accuracy: 0.6384\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.7508 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.7025 - accuracy: 0.7081\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6752 - accuracy: 0.7198\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6567 - accuracy: 0.7269\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 985us/step - loss: 0.6445 - accuracy: 0.7322\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6341 - accuracy: 0.7362\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6282 - accuracy: 0.7376\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6216 - accuracy: 0.7410: 1s\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6152 - accuracy: 0.7435\n",
      "Score for fold 24: loss of 3.3436483121896052; accuracy of 44.826892109500804%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.9009 - accuracy: 0.6380\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.7571 - accuracy: 0.6899\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.7103 - accuracy: 0.7061\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6841 - accuracy: 0.7157\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 886us/step - loss: 0.6678 - accuracy: 0.7219\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6546 - accuracy: 0.7280\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 999us/step - loss: 0.6448 - accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6365 - accuracy: 0.7339\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 941us/step - loss: 0.6304 - accuracy: 0.73540s - loss: 0.6304 - accura\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6254 - accuracy: 0.7381: 0s - loss: 0.6255 - accuracy - ETA: 0s - loss: 0.6255 - accuracy: 0.73\n",
      "Score for fold 25: loss of 0.8748118503412139; accuracy of 66.78743961352657%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - ETA: 0s - loss: 0.8873 - accuracy: 0.63 - 9s 1ms/step - loss: 0.8867 - accuracy: 0.6384\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7420 - accuracy: 0.6892\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.6978 - accuracy: 0.7057\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6750 - accuracy: 0.7137\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 904us/step - loss: 0.6594 - accuracy: 0.7214\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 856us/step - loss: 0.6442 - accuracy: 0.7267\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 895us/step - loss: 0.6345 - accuracy: 0.7311\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 878us/step - loss: 0.6301 - accuracy: 0.7321\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 873us/step - loss: 0.6243 - accuracy: 0.7339\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 850us/step - loss: 0.6166 - accuracy: 0.7366\n",
      "Score for fold 26: loss of 16.81857179048719; accuracy of 22.48389694041868%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 899us/step - loss: 0.8894 - accuracy: 0.6392\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 903us/step - loss: 0.7325 - accuracy: 0.6983\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 947us/step - loss: 0.6807 - accuracy: 0.7184\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6532 - accuracy: 0.7272\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6367 - accuracy: 0.7337\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.6251 - accuracy: 0.7385\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6147 - accuracy: 0.7415\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 962us/step - loss: 0.6076 - accuracy: 0.7449\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 857us/step - loss: 0.5986 - accuracy: 0.7486\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 953us/step - loss: 0.5933 - accuracy: 0.7500\n",
      "Score for fold 27: loss of 10.514507703786421; accuracy of 46.73913043478261%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 983us/step - loss: 0.8938 - accuracy: 0.6377\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 917us/step - loss: 0.7484 - accuracy: 0.6901\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 945us/step - loss: 0.6997 - accuracy: 0.7067\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 923us/step - loss: 0.6687 - accuracy: 0.7213\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 958us/step - loss: 0.6532 - accuracy: 0.7268\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6430 - accuracy: 0.7314\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6345 - accuracy: 0.7345\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6266 - accuracy: 0.7387: 1s -\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 891us/step - loss: 0.6214 - accuracy: 0.7397\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 994us/step - loss: 0.6168 - accuracy: 0.7414\n",
      "Score for fold 28: loss of 5.561801055323291; accuracy of 34.17874396135266%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.8997 - accuracy: 0.6314\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7549 - accuracy: 0.6864\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 946us/step - loss: 0.7074 - accuracy: 0.7055\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 997us/step - loss: 0.6814 - accuracy: 0.7150\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 867us/step - loss: 0.6640 - accuracy: 0.7212\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 909us/step - loss: 0.6512 - accuracy: 0.7259\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 923us/step - loss: 0.6423 - accuracy: 0.7285\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6356 - accuracy: 0.7309\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6281 - accuracy: 0.7335\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 977us/step - loss: 0.6238 - accuracy: 0.7353\n",
      "Score for fold 29: loss of 3.4094788382791923; accuracy of 41.30434782608695%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 986us/step - loss: 0.8881 - accuracy: 0.6409\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 990us/step - loss: 0.7374 - accuracy: 0.6943\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 912us/step - loss: 0.6894 - accuracy: 0.7103\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6629 - accuracy: 0.7194\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 869us/step - loss: 0.6443 - accuracy: 0.7273\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 882us/step - loss: 0.6328 - accuracy: 0.7330\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 894us/step - loss: 0.6223 - accuracy: 0.7365\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 921us/step - loss: 0.6132 - accuracy: 0.7404\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 985us/step - loss: 0.6067 - accuracy: 0.7409\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 985us/step - loss: 0.6033 - accuracy: 0.7426\n",
      "Score for fold 30: loss of 4.5222382175984706; accuracy of 59.82286634460547%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.8990 - accuracy: 0.6335\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7490 - accuracy: 0.6885\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7023 - accuracy: 0.7054\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6748 - accuracy: 0.7152: 0s - loss: 0.6746 - accuracy\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6606 - accuracy: 0.7209: 1s\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.6488 - accuracy: 0.7232\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 894us/step - loss: 0.6409 - accuracy: 0.7263\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 917us/step - loss: 0.6331 - accuracy: 0.7299\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 900us/step - loss: 0.6279 - accuracy: 0.7322\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 896us/step - loss: 0.6243 - accuracy: 0.7332\n",
      "Score for fold 31: loss of 8.323032479764723; accuracy of 21.4170692431562%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 902us/step - loss: 0.8987 - accuracy: 0.6359\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 987us/step - loss: 0.7425 - accuracy: 0.6926\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6943 - accuracy: 0.7096\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6708 - accuracy: 0.7173\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 989us/step - loss: 0.6560 - accuracy: 0.7211\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6454 - accuracy: 0.7249\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 993us/step - loss: 0.6374 - accuracy: 0.7276\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 989us/step - loss: 0.6313 - accuracy: 0.7309\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 935us/step - loss: 0.6260 - accuracy: 0.7329\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 970us/step - loss: 0.6219 - accuracy: 0.7336\n",
      "Score for fold 32: loss of 8.028156020127929; accuracy of 41.48550724637681%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 937us/step - loss: 0.8945 - accuracy: 0.6335\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 944us/step - loss: 0.7417 - accuracy: 0.6895\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6866 - accuracy: 0.7100\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6608 - accuracy: 0.7192\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6385 - accuracy: 0.7277 0s - loss:\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 897us/step - loss: 0.6279 - accuracy: 0.7325\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6195 - accuracy: 0.7350\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 994us/step - loss: 0.6123 - accuracy: 0.7376\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6087 - accuracy: 0.7389\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 981us/step - loss: 0.6035 - accuracy: 0.7414\n",
      "Score for fold 33: loss of 1.0330617661202794; accuracy of 58.87681159420289%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8892 - accuracy: 0.6396\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 949us/step - loss: 0.7365 - accuracy: 0.6978\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 963us/step - loss: 0.6874 - accuracy: 0.7167\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 985us/step - loss: 0.6637 - accuracy: 0.7257\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 991us/step - loss: 0.6493 - accuracy: 0.7307\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6395 - accuracy: 0.7355: 0s -\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6317 - accuracy: 0.7373\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 935us/step - loss: 0.6256 - accuracy: 0.7402\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 930us/step - loss: 0.6189 - accuracy: 0.7422\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6156 - accuracy: 0.7445\n",
      "Score for fold 34: loss of 1.1209480397271265; accuracy of 48.85265700483092%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 873us/step - loss: 0.9035 - accuracy: 0.6300\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 860us/step - loss: 0.7489 - accuracy: 0.6937\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 869us/step - loss: 0.6988 - accuracy: 0.7139\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 852us/step - loss: 0.6728 - accuracy: 0.7236\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 852us/step - loss: 0.6559 - accuracy: 0.7278\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 844us/step - loss: 0.6428 - accuracy: 0.7317\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 851us/step - loss: 0.6334 - accuracy: 0.7368\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 856us/step - loss: 0.6275 - accuracy: 0.7361\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 893us/step - loss: 0.6211 - accuracy: 0.7397\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 995us/step - loss: 0.6174 - accuracy: 0.7402\n",
      "Score for fold 35: loss of 2.885437054166663; accuracy of 61.87600644122383%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 925us/step - loss: 0.9030 - accuracy: 0.6308\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7552 - accuracy: 0.6871\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.7041 - accuracy: 0.7064\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 915us/step - loss: 0.6779 - accuracy: 0.7180\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6618 - accuracy: 0.7243\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6507 - accuracy: 0.7273\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 902us/step - loss: 0.6432 - accuracy: 0.7287\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 883us/step - loss: 0.6339 - accuracy: 0.7342\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 893us/step - loss: 0.6273 - accuracy: 0.7361\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 880us/step - loss: 0.6229 - accuracy: 0.7372\n",
      "Score for fold 36: loss of 1.602369954554595; accuracy of 52.79790660225443%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 6s 870us/step - loss: 0.8956 - accuracy: 0.6378\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 6s 865us/step - loss: 0.7472 - accuracy: 0.6926\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.7025 - accuracy: 0.7085\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.6788 - accuracy: 0.7174\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 7s 981us/step - loss: 0.6636 - accuracy: 0.7223\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.6516 - accuracy: 0.7277\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.6439 - accuracy: 0.7318\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 6s 918us/step - loss: 0.6350 - accuracy: 0.7349\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 6s 974us/step - loss: 0.6288 - accuracy: 0.7378\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.6211 - accuracy: 0.7404\n",
      "Score for fold 37: loss of 1.5330330210223482; accuracy of 31.76328502415459%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.9018 - accuracy: 0.6311\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7664 - accuracy: 0.6805\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7180 - accuracy: 0.6996\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6928 - accuracy: 0.7091\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6738 - accuracy: 0.7164\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 930us/step - loss: 0.6619 - accuracy: 0.7215\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6544 - accuracy: 0.7255\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 955us/step - loss: 0.6486 - accuracy: 0.7283\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6412 - accuracy: 0.7316\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6361 - accuracy: 0.7335\n",
      "Score for fold 38: loss of 1.0022238690927985; accuracy of 47.96698872785829%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 989us/step - loss: 0.8913 - accuracy: 0.6369\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.7421 - accuracy: 0.6946\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6961 - accuracy: 0.7128\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 899us/step - loss: 0.6729 - accuracy: 0.7228\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 984us/step - loss: 0.6563 - accuracy: 0.7295\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 938us/step - loss: 0.6460 - accuracy: 0.7337\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 926us/step - loss: 0.6374 - accuracy: 0.7371\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 976us/step - loss: 0.6300 - accuracy: 0.7398\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 946us/step - loss: 0.6241 - accuracy: 0.7431\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 891us/step - loss: 0.6180 - accuracy: 0.7441\n",
      "Score for fold 39: loss of 4.811007923094538; accuracy of 32.54830917874396%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 926us/step - loss: 0.8987 - accuracy: 0.6367\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7445 - accuracy: 0.6941\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6970 - accuracy: 0.7104\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6734 - accuracy: 0.7179\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6564 - accuracy: 0.7239\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6471 - accuracy: 0.7279: 0s -\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6400 - accuracy: 0.7307: 0s - loss: 0.6402 - accu\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 961us/step - loss: 0.6313 - accuracy: 0.7329\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 6s 954us/step - loss: 0.6264 - accuracy: 0.7359\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 962us/step - loss: 0.6223 - accuracy: 0.7362\n",
      "Score for fold 40: loss of 1.4923472539994165; accuracy of 48.87278582930757%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8859 - accuracy: 0.6409\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 968us/step - loss: 0.7402 - accuracy: 0.6953\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 914us/step - loss: 0.6999 - accuracy: 0.7084\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 920us/step - loss: 0.6775 - accuracy: 0.7146\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 903us/step - loss: 0.6621 - accuracy: 0.7214\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 971us/step - loss: 0.6510 - accuracy: 0.7272\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 980us/step - loss: 0.6400 - accuracy: 0.7316\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 6s 949us/step - loss: 0.6319 - accuracy: 0.7338\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 899us/step - loss: 0.6251 - accuracy: 0.7361\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6211 - accuracy: 0.7361\n",
      "Score for fold 41: loss of 1.352714553643348; accuracy of 56.40096618357487%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8902 - accuracy: 0.6376\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.7401 - accuracy: 0.6894\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.6952 - accuracy: 0.7021\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 950us/step - loss: 0.6705 - accuracy: 0.7112\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 936us/step - loss: 0.6547 - accuracy: 0.7183\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 967us/step - loss: 0.6452 - accuracy: 0.7216\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.6366 - accuracy: 0.7249\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6312 - accuracy: 0.7289\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6259 - accuracy: 0.7305\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6221 - accuracy: 0.7322\n",
      "Score for fold 42: loss of 1.7926940874776978; accuracy of 31.78341384863124%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 6s 926us/step - loss: 0.9004 - accuracy: 0.6331\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 6s 926us/step - loss: 0.7527 - accuracy: 0.6878\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 6s 940us/step - loss: 0.7047 - accuracy: 0.7030\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 6s 923us/step - loss: 0.6802 - accuracy: 0.7119\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 968us/step - loss: 0.6641 - accuracy: 0.7189\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 948us/step - loss: 0.6522 - accuracy: 0.7220\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 7s 988us/step - loss: 0.6451 - accuracy: 0.7242\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6387 - accuracy: 0.7276\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 6s 911us/step - loss: 0.6342 - accuracy: 0.72780s - loss: 0.6343 - accu\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 961us/step - loss: 0.6309 - accuracy: 0.7297\n",
      "Score for fold 43: loss of 0.8590796782834614; accuracy of 75.38244766505636%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.8998 - accuracy: 0.6337\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 7s 991us/step - loss: 0.7448 - accuracy: 0.6927\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 7s 996us/step - loss: 0.7014 - accuracy: 0.7104\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 7s 986us/step - loss: 0.6806 - accuracy: 0.7193\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 6s 946us/step - loss: 0.6640 - accuracy: 0.7249\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 6s 960us/step - loss: 0.6546 - accuracy: 0.7267\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 6s 956us/step - loss: 0.6469 - accuracy: 0.7299\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6396 - accuracy: 0.7331\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.6353 - accuracy: 0.7346\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 6s 925us/step - loss: 0.6308 - accuracy: 0.7355\n",
      "Score for fold 44: loss of 1.7163302575871016; accuracy of 55.27375201288245%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 8s 1ms/step - loss: 0.8933 - accuracy: 0.6343\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 6s 967us/step - loss: 0.7400 - accuracy: 0.6913\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 6s 908us/step - loss: 0.6945 - accuracy: 0.7079\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 6s 942us/step - loss: 0.6733 - accuracy: 0.7142\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.6597 - accuracy: 0.7196\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.6489 - accuracy: 0.7222\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.6422 - accuracy: 0.7262\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 6s 965us/step - loss: 0.6364 - accuracy: 0.7291\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 7s 1ms/step - loss: 0.6275 - accuracy: 0.7315\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 9s 1ms/step - loss: 0.6255 - accuracy: 0.7318\n",
      "Score for fold 45: loss of 2.3637927064669113; accuracy of 37.07729468599034%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.9121555423367043 - Accuracy: 67.25040257648953%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.581012145140705 - Accuracy: 35.18518518518518%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 2.3449566419987717 - Accuracy: 41.0024154589372%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 2.2035863733793803 - Accuracy: 39.95571658615137%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.490456232894062 - Accuracy: 37.13768115942029%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.933298281233475 - Accuracy: 50.78502415458937%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.451400052042207 - Accuracy: 50.44283413848631%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.1064116640223398 - Accuracy: 60.6280193236715%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.9079315734370319 - Accuracy: 65.76086956521739%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 10.340666910372494 - Accuracy: 36.97665056360708%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 13.393314939188725 - Accuracy: 34.40016103059581%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 3.794035475971042 - Accuracy: 35.30595813204509%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 4.3053552401828075 - Accuracy: 33.27294685990338%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 3.6143067799142643 - Accuracy: 26.42914653784219%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 3.0039012258851394 - Accuracy: 43.11594202898551%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 39.633907815684445 - Accuracy: 8.091787439613526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.8880197524112587 - Accuracy: 58.313204508856685%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 4.318453690302956 - Accuracy: 22.946859903381643%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 7.173457339800813 - Accuracy: 28.864734299516908%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.8688877393969975 - Accuracy: 61.35265700483091%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 2.655126227368863 - Accuracy: 43.71980676328502%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 3.88150986391997 - Accuracy: 60.97020933977456%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 4.441856228041608 - Accuracy: 25.60386473429952%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 3.3436483121896052 - Accuracy: 44.826892109500804%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.8748118503412139 - Accuracy: 66.78743961352657%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 16.81857179048719 - Accuracy: 22.48389694041868%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 10.514507703786421 - Accuracy: 46.73913043478261%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 5.561801055323291 - Accuracy: 34.17874396135266%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 3.4094788382791923 - Accuracy: 41.30434782608695%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 4.5222382175984706 - Accuracy: 59.82286634460547%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 8.323032479764723 - Accuracy: 21.4170692431562%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 8.028156020127929 - Accuracy: 41.48550724637681%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 1.0330617661202794 - Accuracy: 58.87681159420289%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.1209480397271265 - Accuracy: 48.85265700483092%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 2.885437054166663 - Accuracy: 61.87600644122383%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 1.602369954554595 - Accuracy: 52.79790660225443%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 1.5330330210223482 - Accuracy: 31.76328502415459%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 1.0022238690927985 - Accuracy: 47.96698872785829%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 4.811007923094538 - Accuracy: 32.54830917874396%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 1.4923472539994165 - Accuracy: 48.87278582930757%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 1.352714553643348 - Accuracy: 56.40096618357487%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 1.7926940874776978 - Accuracy: 31.78341384863124%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.8590796782834614 - Accuracy: 75.38244766505636%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 1.7163302575871016 - Accuracy: 55.27375201288245%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 2.3637927064669113 - Accuracy: 37.07729468599034%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 44.134013240293434 (+- 14.567238595560582)\n",
      "> Loss: 4.715673203734675\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = X_train_df['Subject_ID'].values \n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/N1/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/10_TF_ANN_Phys_Only/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/10_TF_ANN_Phys_Only') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/10_TF_ANN_Phys_Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 [==============================] - 0s 837us/step - loss: 7.4930 - accuracy: 0.4829\n",
      "Test loss, Test acc: [7.493027859437048, 0.48294346978557506]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16799, 11013,  2039,  2170],\n",
       "       [ 4694,  5375,  1419,  1036],\n",
       "       [  433,   260,   160,   154],\n",
       "       [  133,   851,   201,   459]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 362
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le.classes_, yticklabels = le.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data Rolled ANN Phys Only - Confusion Matrix')\n",
    "plt.savefig('Duke_Data_ANN_Phys_Only_Rolled_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.483 \n",
      "F1 Score: 0.442\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke HAR Data Model 1: Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: **rolled_data.csv**\n",
    "\n",
    "This notebook label encodes Subject_ID and Activity (our y variable). It then one-hot encodes Subject_ID to be used as a feature in our model. The validation method used for the model is Leave One Group Out (LOGO) validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('rolled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.reindex(columns = ['ACC1', 'ACC2', 'ACC3', 'TEMP', 'EDA', 'BVP', 'HR', 'Magnitude', 'Subject_ID', 'Round', 'Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC1</th>\n",
       "      <th>ACC2</th>\n",
       "      <th>ACC3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>HR</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>Round</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.22500</td>\n",
       "      <td>28.29750</td>\n",
       "      <td>39.225000</td>\n",
       "      <td>32.35600</td>\n",
       "      <td>0.265940</td>\n",
       "      <td>-0.21875</td>\n",
       "      <td>76.10525</td>\n",
       "      <td>62.913102</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.17500</td>\n",
       "      <td>28.34250</td>\n",
       "      <td>39.175000</td>\n",
       "      <td>32.35475</td>\n",
       "      <td>0.265556</td>\n",
       "      <td>-1.07075</td>\n",
       "      <td>75.96875</td>\n",
       "      <td>62.870169</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.12663</td>\n",
       "      <td>28.38337</td>\n",
       "      <td>39.125543</td>\n",
       "      <td>32.35350</td>\n",
       "      <td>0.265140</td>\n",
       "      <td>-0.75950</td>\n",
       "      <td>75.83375</td>\n",
       "      <td>62.826763</td>\n",
       "      <td>19-001</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACC1      ACC2       ACC3      TEMP       EDA      BVP        HR  \\\n",
       "0  40.22500  28.29750  39.225000  32.35600  0.265940 -0.21875  76.10525   \n",
       "1  40.17500  28.34250  39.175000  32.35475  0.265556 -1.07075  75.96875   \n",
       "2  40.12663  28.38337  39.125543  32.35350  0.265140 -0.75950  75.83375   \n",
       "\n",
       "   Magnitude Subject_ID  Round  Activity  \n",
       "0  62.913102     19-001      1  Baseline  \n",
       "1  62.870169     19-001      1  Baseline  \n",
       "2  62.826763     19-001      1  Baseline  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Activity and Subject_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. The label each class is associated with is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Activity': 0, 'Baseline': 1, 'DB': 2, 'Type': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Activity'] = le.fit_transform(df['Activity'])\n",
    "\n",
    "activity_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(activity_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "df['Subject_ID'] = le1.fit_transform(df['Subject_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train sets (by Subject ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = list(df['Subject_ID'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the test train split can be changed by changing the index below. For our purposes, n = 45 for train and n = 10 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 11) (47196, 11)\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Subject_ID'].isin(ID_list[:45])]\n",
    "test = df[df['Subject_ID'].isin(ID_list[45:])]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose features to be used in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the three following code cells to choose what features are used in the model. Do not run them all. To uncomment or comment multiple selected lines, press control + /. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 9) (216108,) (47196, 9) (47196,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:,0:9]\n",
    "X_test = test.iloc[:,0:9]\n",
    "\n",
    "y_train = train.iloc[:,-1].values\n",
    "y_test = test.iloc[:,-1].values\n",
    "\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mechanical Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train.iloc[['ACC1', 'ACC2', 'ACC3', 'Magnitude', 'Subject_ID']]\n",
    "# X_test = test.iloc[['ACC1', 'ACC2', 'ACC3', 'Magnitude', 'Subject_ID']]\n",
    "\n",
    "# y_train = train.iloc[:,-1].values\n",
    "# y_test = test.iloc[:,-1].values\n",
    "\n",
    "# print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Physiological Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train.iloc[['TEMP', 'EDA', 'HR', 'BVP', 'Subject_ID']]\n",
    "# X_test = test.iloc[['TEMP', 'EDA', 'HR', 'BVP', 'Subject_ID']]\n",
    "\n",
    "# y_train = train.iloc[:,-1].values\n",
    "# y_test = test.iloc[:,-1].values\n",
    "\n",
    "# print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create X_train_df below so that we are able to use the Subject_ID column later on, to iterate through our leave one group out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID, so that it may be used as a variable in our model. This allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['train'] =1\n",
    "X_test['train'] = 0\n",
    "\n",
    "combined = pd.concat([X_train, X_test])\n",
    "combined = pd.concat([combined, pd.get_dummies(combined['Subject_ID'])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216108, 63) (47196, 63) 263304\n"
     ]
    }
   ],
   "source": [
    "X_train = combined[combined['train'] == 1]\n",
    "X_test = combined[combined['train'] == 0]\n",
    "\n",
    "X_train.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"train\", \"Subject_ID\"], axis = 1, inplace = True)\n",
    "print(X_train.shape, X_test.shape, X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the y variable as we need to one-hot encode this y variable for the model. Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train.iloc[:,:8] = ss.fit_transform(X_train.iloc[:,:8])\n",
    "X_test.iloc[:,:8] = ss.transform(X_test.iloc[:,:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 hidden **fully connected** layers with 32 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leave One Out CV:__\n",
    "Each observation is considered as a validation set and the rest n-1 observations are a training set. Fit the model and predict using 1 observation validation set. Repeat this for n times for each observation as a validation set.\n",
    "Test-error rate is average of all n errors.\n",
    "\n",
    "__Advantages:__ takes care of both drawbacks of validation-set method\n",
    "1. No randomness of using some observations for training vs. validation set like in validation-set method as each observation is considered for both training aâ€ºnd validation. So overall less variability than Validation-set method due to no randomness no matter how many times you run it.\n",
    "2. Less bias than validation-set method as training-set is of n-1 size. Because of this reduced bias, reduced over-estimation of test-error, not as much compared to validation-set method.\n",
    "\n",
    "__Disadvantages:__\n",
    "1. Even though each iterations test-error is un-biased, it has a high variability as only one-observation validation-set was used for prediction.\n",
    "2. Computationally expensive (time and power) especially if dataset is big with large n as it requires fitting the model n times. Also some statistical models have computationally intensive fitting so with large dataset and these models LOOCV might not be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 7s 1ms/step - loss: 0.4871 - accuracy: 0.8106\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 8s 1ms/step - loss: 0.3375 - accuracy: 0.8690\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3129 - accuracy: 0.8773\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2968 - accuracy: 0.8826 0s - loss: 0.2972 - accura\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2867 - accuracy: 0.8868\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2800 - accuracy: 0.8895\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2742 - accuracy: 0.8920\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2691 - accuracy: 0.8934 0s - loss: 0.2693 - accuracy\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2663 - accuracy: 0.8944\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2643 - accuracy: 0.8956\n",
      "Score for fold 1: loss of 0.7279043721971121; accuracy of 75.54347826086956%, F1 of 0.7217814579225815\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4780 - accuracy: 0.8149\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3323 - accuracy: 0.8734\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3024 - accuracy: 0.8836\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2869 - accuracy: 0.8894\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2765 - accuracy: 0.8926 0s - los\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 21s 3ms/step - loss: 0.2705 - accuracy: 0.8952 0s - loss: 0.2704 - accuracy: 0.89 - ETA: 0s - loss: 0.2706 - \n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2647 - accuracy: 0.8968\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2614 - accuracy: 0.8983 1s - l - ETA: 0s -\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2583 - accuracy: 0.8992\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2547 - accuracy: 0.9005\n",
      "Score for fold 2: loss of 2.1269179219365024; accuracy of 47.52415458937198%, F1 of 0.468007069855597\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4772 - accuracy: 0.8148\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3329 - accuracy: 0.8723\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3064 - accuracy: 0.8825\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2924 - accuracy: 0.8875 0s - loss: 0.2925 - ac\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2844 - accuracy: 0.8897\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2791 - accuracy: 0.8929\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2724 - accuracy: 0.8944\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2685 - accuracy: 0.8956\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2642 - accuracy: 0.8969 0s - l\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2624 - accuracy: 0.8970 4s - loss: 0.2634 - accuracy: 0. - ETA: 4s - los\n",
      "Score for fold 3: loss of 1.320735329694371; accuracy of 68.49838969404188%, F1 of 0.683833126314028\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4821 - accuracy: 0.8116\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3330 - accuracy: 0.8689\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3051 - accuracy: 0.8799 2s - loss: 0.3073 - accuracy: 0.87 - ETA: 2s - loss: - ETA: 1s - loss: 0.3072  - ETA: 1s - loss: 0.3066  - - ETA: 0s - loss: 0.3052 - accuracy: 0.87\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2931 - accuracy: 0.8834\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2837 - accuracy: 0.8867\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2792 - accuracy: 0.8876\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2740 - accuracy: 0.8895\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2705 - accuracy: 0.8907\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2662 - accuracy: 0.8935 2s - loss: 0.2654 - accuracy:  - ETA: 1s - loss: 0.2669 - ac - ETA: 1s - loss: 0 - ETA: \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2654 - accuracy: 0.8919\n",
      "Score for fold 4: loss of 3.933882490777801; accuracy of 53.22061191626409%, F1 of 0.5224419520825596\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4875 - accuracy: 0.8092 4s - loss: 0.594 - ETA: 0s - loss: 0.4916 - ac\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3331 - accuracy: 0.8698\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3050 - accuracy: 0.8790 4s - loss: 0.3088 - accuracy: 0.87 - ETA: 0s - loss: 0.3050 - accuracy:  - ETA: 0s - loss: 0.3052 - accuracy: 0.\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2903 - accuracy: 0.8850\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2779 - accuracy: 0.8899\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2703 - accuracy: 0.8924\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2653 - accuracy: 0.8941\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2607 - accuracy: 0.8967\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2574 - accuracy: 0.8973\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2557 - accuracy: 0.8980\n",
      "Score for fold 5: loss of 1.5375648307758; accuracy of 44.14251207729469%, F1 of 0.33753154925833734\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4899 - accuracy: 0.8098\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3374 - accuracy: 0.8713\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3086 - accuracy: 0.8811\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2929 - accuracy: 0.8871\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2830 - accuracy: 0.8911 0s - loss: 0.2830 - accuracy\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2759 - accuracy: 0.8935 0s - l\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2731 - accuracy: 0.8950\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2679 - accuracy: 0.8958\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2656 - accuracy: 0.8976\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2624 - accuracy: 0.8983\n",
      "Score for fold 6: loss of 0.9995661624802054; accuracy of 79.81078904991948%, F1 of 0.7543788193342718\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4919 - accuracy: 0.8080\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.3380 - accuracy: 0.8697\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3121 - accuracy: 0.8775\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2938 - accuracy: 0.8848\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2869 - accuracy: 0.8875\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2780 - accuracy: 0.8901\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2726 - accuracy: 0.8915\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2686 - accuracy: 0.8936 1s - loss: 0.2\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2671 - accuracy: 0.8937\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2652 - accuracy: 0.8952\n",
      "Score for fold 7: loss of 0.46170328207383; accuracy of 80.61594202898551%, F1 of 0.7716449154475873\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.4774 - accuracy: 0.8156\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3158 - accuracy: 0.8829\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2905 - accuracy: 0.8912 0s - loss: 0.2908 - ac\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 19s 3ms/step - loss: 0.2787 - accuracy: 0.8941\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2708 - accuracy: 0.8964\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2642 - accuracy: 0.8994\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2610 - accuracy: 0.9004\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2554 - accuracy: 0.9029\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2555 - accuracy: 0.9026 0s - loss: 0.2555 - accuracy: 0.90\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2528 - accuracy: 0.9027\n",
      "Score for fold 8: loss of 1.1703203459896991; accuracy of 42.310789049919485%, F1 of 0.4673006181458903\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4911 - accuracy: 0.8064 0s - los\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3415 - accuracy: 0.8687  - ETA: 0s - los\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3092 - accuracy: 0.8789 0s - loss: 0.3093 - accuracy\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2935 - accuracy: 0.8844\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2857 - accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2779 - accuracy: 0.8905 0s - loss: 0.2786 - accuracy:  - ETA: 0s - loss: 0.2\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2740 - accuracy: 0.8918\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2679 - accuracy: 0.8933\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2648 - accuracy: 0.8954 3s - loss: 0.2657 \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2631 - accuracy: 0.8960\n",
      "Score for fold 9: loss of 0.6760739295832873; accuracy of 81.13929146537842%, F1 of 0.7497557798905224\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4762 - accuracy: 0.8153: 0s - loss: 0.4831 - accuracy: 0.81 - ETA: 0s - loss: 0.482\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3241 - accuracy: 0.8757\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2982 - accuracy: 0.8839\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2840 - accuracy: 0.8891\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2750 - accuracy: 0.8925\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2690 - accuracy: 0.8952\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2654 - accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2612 - accuracy: 0.8975\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2585 - accuracy: 0.8986: 0s - loss: 0.2583 \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2566 - accuracy: 0.8990\n",
      "Score for fold 10: loss of 2.2320190051010202; accuracy of 58.9573268921095%, F1 of 0.6213655161846366\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4806 - accuracy: 0.8116\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3268 - accuracy: 0.8740\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3012 - accuracy: 0.8828 0s - loss: 0.3\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2871 - accuracy: 0.8879\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2780 - accuracy: 0.8907\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2717 - accuracy: 0.8930\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2668 - accuracy: 0.8954\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2623 - accuracy: 0.8961\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2593 - accuracy: 0.8968\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2583 - accuracy: 0.8981\n",
      "Score for fold 11: loss of 4.032384144486454; accuracy of 30.3341384863124%, F1 of 0.18821804673781192\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.4941 - accuracy: 0.8081\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3406 - accuracy: 0.8685\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3106 - accuracy: 0.8797\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2959 - accuracy: 0.8852\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2840 - accuracy: 0.8902\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2775 - accuracy: 0.8934 1s - loss: 0.2779 - accuracy: 0.89 - ETA: 1s - loss: 0.2779 - accuracy:  - ETA: \n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2703 - accuracy: 0.8954\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2679 - accuracy: 0.8961\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2628 - accuracy: 0.8983- ETA: 2s - loss: 0.2628  - ETA: 0s -\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2621 - accuracy: 0.8982\n",
      "Score for fold 12: loss of 1.7746444513716138; accuracy of 49.73832528180354%, F1 of 0.41135910677684656\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4823 - accuracy: 0.8115\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3290 - accuracy: 0.8718 2s - loss: 0.3313 - accu - ETA: 1s - loss: 0.3309 - ac - ETA: 1s - loss: 0.3305 - accu\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3014 - accuracy: 0.8802\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2876 - accuracy: 0.8860\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2786 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2720 - accuracy: 0.8922 0s - loss: 0.2717 - ac\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2674 - accuracy: 0.8936\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2640 - accuracy: 0.8959  - ETA: 0s - l\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2603 - accuracy: 0.8976\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2583 - accuracy: 0.8983\n",
      "Score for fold 13: loss of 1.0430887365965937; accuracy of 73.45008051529791%, F1 of 0.7201527867016063\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4866 - accuracy: 0.8137\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3339 - accuracy: 0.8751\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3057 - accuracy: 0.8842\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2928 - accuracy: 0.8886\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2834 - accuracy: 0.8923: 0s - loss:\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2785 - accuracy: 0.8935\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2742 - accuracy: 0.8947\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2699 - accuracy: 0.8969\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2660 - accuracy: 0.8980 0s - loss: 0.2660 -  - ETA: 0s - loss: 0.2661 - accuracy: \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2627 - accuracy: 0.8982\n",
      "Score for fold 14: loss of 2.215742040205001; accuracy of 47.00080515297906%, F1 of 0.44876849599794666\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4850 - accuracy: 0.8074\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3365 - accuracy: 0.8673\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3100 - accuracy: 0.8762\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2979 - accuracy: 0.8807ETA:  - ETA: 0s - loss: 0.2983 \n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2884 - accuracy: 0.8846 1s - los - ETA: 0s - loss: 0.2888 - accu\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2818 - accuracy: 0.8872: 0s - loss: 0.2817 - accuracy\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2776 - accuracy: 0.8893\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2728 - accuracy: 0.8919\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2673 - accuracy: 0.8941\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2649 - accuracy: 0.8957 0s - loss: 0.2651 - ac\n",
      "Score for fold 15: loss of 1.0566557856772891; accuracy of 67.65297906602254%, F1 of 0.6705697758112833\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4814 - accuracy: 0.8133\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3342 - accuracy: 0.8725\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3068 - accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2908 - accuracy: 0.8862 0s - loss: 0.2909 - accu\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2772 - accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2702 - accuracy: 0.8938 0s - loss: 0.270\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2650 - accuracy: 0.8964\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2607 - accuracy: 0.8970\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2589 - accuracy: 0.8976 2s - loss: 0.2570 - \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2549 - accuracy: 0.8984\n",
      "Score for fold 16: loss of 11.300597836249066; accuracy of 17.652979066022546%, F1 of 0.18009375974130296\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.4961 - accuracy: 0.8065\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3444 - accuracy: 0.8682\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3149 - accuracy: 0.8783\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3016 - accuracy: 0.8827 0s\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2920 - accuracy: 0.8860\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2871 - accuracy: 0.8875\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2836 - accuracy: 0.8885 0s -\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2790 - accuracy: 0.8907\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.2755 - accuracy: 0.8917\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 18s 3ms/step - loss: 0.2743 - accuracy: 0.8915\n",
      "Score for fold 17: loss of 0.4163872363251284; accuracy of 78.98550724637681%, F1 of 0.7710407821940911\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 15s 2ms/step - loss: 0.4998 - accuracy: 0.8047 2s - loss: 0.5247 - accu - ETA: 0s - loss: 0.5079 - accuracy: 0. - ETA: 0s -\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 14s 2ms/step - loss: 0.3441 - accuracy: 0.8684 1s - los - ETA: 0s - loss: 0.3\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 14s 2ms/step - loss: 0.3117 - accuracy: 0.8801 0s - loss: 0.312\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 14s 2ms/step - loss: 0.2976 - accuracy: 0.8840\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2864 - accuracy: 0.8883\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 19s 3ms/step - loss: 0.2803 - accuracy: 0.8905\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 15s 2ms/step - loss: 0.2746 - accuracy: 0.8921\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2720 - accuracy: 0.8933\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2694 - accuracy: 0.8948\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2675 - accuracy: 0.8947\n",
      "Score for fold 18: loss of 2.004098236953747; accuracy of 28.26086956521739%, F1 of 0.208967700153585\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 19 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.4856 - accuracy: 0.8112 4s - loss: 0.527 - ETA: 0s - loss: 0\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3351 - accuracy: 0.8702 0s - loss: 0.3361 \n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3035 - accuracy: 0.8819\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2886 - accuracy: 0.8879 1s - loss: 0.2890 - accuracy: 0. - ETA: 0s - l\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2802 - accuracy: 0.8907\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2748 - accuracy: 0.8925\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2682 - accuracy: 0.8941\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2652 - accuracy: 0.8958\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2616 - accuracy: 0.8979 0s - loss: 0.2619 - accuracy: \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2594 - accuracy: 0.8982\n",
      "Score for fold 19: loss of 0.7769496936891708; accuracy of 70.0684380032206%, F1 of 0.6656490127527848\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 20 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.4885 - accuracy: 0.8106\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3430 - accuracy: 0.8687\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3106 - accuracy: 0.8802 0s\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2946 - accuracy: 0.8857 0s -\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2862 - accuracy: 0.8877 0s - loss: 0\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2771 - accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2719 - accuracy: 0.8929\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2696 - accuracy: 0.8937\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2665 - accuracy: 0.8950\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2649 - accuracy: 0.8953\n",
      "Score for fold 20: loss of 0.3138655421869067; accuracy of 87.86231884057972%, F1 of 0.8434849181175504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 21 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4948 - accuracy: 0.8067\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3440 - accuracy: 0.8674\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3121 - accuracy: 0.8804 0s - loss: 0.3122 - accuracy: 0.\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2978 - accuracy: 0.8852 0s - loss: 0.2978 - ac\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2867 - accuracy: 0.8892\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.2809 - accuracy: 0.8914\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2762 - accuracy: 0.8928\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2718 - accuracy: 0.8946 0s - loss: 0.2719 - accu\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2681 - accuracy: 0.8960- ETA: 0s - loss: 0.2682 - accura\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2655 - accuracy: 0.8967\n",
      "Score for fold 21: loss of 3.5060109015735654; accuracy of 45.69243156199678%, F1 of 0.32905792068454315\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 22 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4863 - accuracy: 0.8106\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3313 - accuracy: 0.8726\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3030 - accuracy: 0.8838\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2889 - accuracy: 0.8888\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2805 - accuracy: 0.8921\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2744 - accuracy: 0.8936\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2696 - accuracy: 0.8961\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2659 - accuracy: 0.8972\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2606 - accuracy: 0.8987 0s - los\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2598 - accuracy: 0.8987 0s - loss: 0.2598 - accuracy: 0.89\n",
      "Score for fold 22: loss of 1.7317743389059201; accuracy of 52.79790660225443%, F1 of 0.4474313219920702\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 23 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4852 - accuracy: 0.8082\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3407 - accuracy: 0.8671 \n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3106 - accuracy: 0.8785\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2971 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2880 - accuracy: 0.8872\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2799 - accuracy: 0.8900 2s - loss: 0.2807 - accuracy\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2776 - accuracy: 0.8920 0s - loss: 0.2\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2734 - accuracy: 0.8923 1s - loss: 0.2726  - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.2734 - accuracy: 0.\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2691 - accuracy: 0.8950 0s - l\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2659 - accuracy: 0.8957\n",
      "Score for fold 23: loss of 1.2432553950738194; accuracy of 59.44041867954911%, F1 of 0.5532798843104608\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 24 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.5034 - accuracy: 0.8034 2s - loss: 0.5359 - accura - ETA: 0s -\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3472 - accuracy: 0.8680\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3147 - accuracy: 0.8797\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3000 - accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2904 - accuracy: 0.8878 4s - loss: 0.2912 - ac - ETA: 0s - loss: 0.2904 - accura\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2812 - accuracy: 0.8905\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2777 - accuracy: 0.8917 0s -\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2725 - accuracy: 0.8939 0s - loss: 0.2723 - ac\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2695 - accuracy: 0.8945\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2676 - accuracy: 0.8952\n",
      "Score for fold 24: loss of 0.1729148007478147; accuracy of 94.30354267310788%, F1 of 0.9381431156405197\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 25 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.4963 - accuracy: 0.8075\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3480 - accuracy: 0.8673 5s\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3156 - accuracy: 0.8780\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2997 - accuracy: 0.8828 0s -\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2905 - accuracy: 0.8860 0s - l\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2836 - accuracy: 0.8892 0s - loss: 0.2836 - accuracy: 0.88\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2794 - accuracy: 0.8905\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2766 - accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2730 - accuracy: 0.8933\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2677 - accuracy: 0.8952\n",
      "Score for fold 25: loss of 0.3650140242678863; accuracy of 84.84299516908213%, F1 of 0.8186145405507917\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 26 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4909 - accuracy: 0.8105\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3433 - accuracy: 0.8704\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3204 - accuracy: 0.8774\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3071 - accuracy: 0.8812\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2995 - accuracy: 0.8839 0s - loss: 0.300 - ETA: 0s - loss: 0.2995 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2933 - accuracy: 0.8860 1s - los\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2881 - accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2854 - accuracy: 0.8892  - ETA - ETA: 0s - loss: 0.2855 - accu\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2826 - accuracy: 0.8898\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2799 - accuracy: 0.8913 0s - loss: 0.279\n",
      "Score for fold 26: loss of 1.3247153194847896; accuracy of 69.24315619967794%, F1 of 0.7027839962616054\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 27 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4911 - accuracy: 0.8085\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.3453 - accuracy: 0.8670\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3155 - accuracy: 0.8779 1s\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3012 - accuracy: 0.8825 0s - loss: 0.3017 \n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2906 - accuracy: 0.8866 0s - l\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2822 - accuracy: 0.8893 0s - loss: 0.2823 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2789 - accuracy: 0.8903\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2741 - accuracy: 0.8922 0s - loss: 0.2742 - accu\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2703 - accuracy: 0.8922\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2688 - accuracy: 0.8938 0s - los\n",
      "Score for fold 27: loss of 0.946427883793474; accuracy of 63.808373590982285%, F1 of 0.6353394144206869\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 28 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.4831 - accuracy: 0.8139 0s - loss: 0.4862 - ac\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3300 - accuracy: 0.8729\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3012 - accuracy: 0.8828 0s - l\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2878 - accuracy: 0.8860\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2769 - accuracy: 0.8902\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2704 - accuracy: 0.8931\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2651 - accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2621 - accuracy: 0.8967\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2581 - accuracy: 0.8986 0s - loss: 0.2580 - \n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2573 - accuracy: 0.8992\n",
      "Score for fold 28: loss of 3.144970661684595; accuracy of 38.486312399355874%, F1 of 0.280539110671669\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 29 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4973 - accuracy: 0.8047 \n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.3363 - accuracy: 0.8667\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.3070 - accuracy: 0.8787 1s - loss: 0.3070  - ETA: 0s -\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2891 - accuracy: 0.8856 2s\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2801 - accuracy: 0.8895 1s - loss: 0.2811  - ETA: 1s - l - ETA: 0s - loss:\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2746 - accuracy: 0.8905\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2694 - accuracy: 0.8924 0s - loss: 0.2694 - accuracy\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2666 - accuracy: 0.8940 0s - loss: 0.2664 - accuracy: 0.\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2633 - accuracy: 0.8946\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2607 - accuracy: 0.8960\n",
      "Score for fold 29: loss of 1.7210771195479637; accuracy of 79.99194847020934%, F1 of 0.8165881068915417\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 30 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4969 - accuracy: 0.8058\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3420 - accuracy: 0.8679 0s - loss: 0.342\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3145 - accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2996 - accuracy: 0.8835\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2914 - accuracy: 0.8864\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2851 - accuracy: 0.8896\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2810 - accuracy: 0.8896\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2758 - accuracy: 0.8922\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2755 - accuracy: 0.8928 1s -\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2725 - accuracy: 0.8942\n",
      "Score for fold 30: loss of 2.6585882966263075; accuracy of 64.69404186795491%, F1 of 0.5864122225581415\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 31 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4976 - accuracy: 0.8054\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3441 - accuracy: 0.8674 0s - loss: 0.344\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3144 - accuracy: 0.8772 2s - - ETA: 0s - loss: 0.3148 - ac\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2971 - accuracy: 0.8826\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2891 - accuracy: 0.8863\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2824 - accuracy: 0.8888\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2754 - accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2719 - accuracy: 0.8922\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2674 - accuracy: 0.8941\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2665 - accuracy: 0.8936ETA: 0s - loss: 0.2667 - accuracy: \n",
      "Score for fold 31: loss of 0.16119741820146508; accuracy of 95.28985507246377%, F1 of 0.9526325000181911\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 32 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4857 - accuracy: 0.8122\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3376 - accuracy: 0.8706 0s - loss: 0.3384 - ac\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3072 - accuracy: 0.8801\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2926 - accuracy: 0.8855\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2836 - accuracy: 0.8885 0s - loss: 0.2839 - accuracy: 0. - ETA: 0s - loss: 0.2838 \n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2756 - accuracy: 0.8925\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2716 - accuracy: 0.8932\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2674 - accuracy: 0.8951\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2637 - accuracy: 0.8963\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2624 - accuracy: 0.8970 0s -\n",
      "Score for fold 32: loss of 1.4505378906634154; accuracy of 63.72785829307569%, F1 of 0.6408824754105032\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 33 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4965 - accuracy: 0.8068\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3387 - accuracy: 0.8697 2s - loss: 0.3 - E\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.3083 - accuracy: 0.8796\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2963 - accuracy: 0.8836\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2864 - accuracy: 0.8872\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2808 - accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2766 - accuracy: 0.8901 - ETA: 2s - loss: 0.2776 - ac\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2718 - accuracy: 0.8934: 0s - loss: 0.2718 - accuracy: 0.\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2674 - accuracy: 0.8946\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2674 - accuracy: 0.8947\n",
      "Score for fold 33: loss of 0.508922589749033; accuracy of 82.89049919484702%, F1 of 0.789970745068947\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 34 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4897 - accuracy: 0.8089: 2s - loss: 0 - ETA: 1s - loss: 0.5159 - accuracy: 0. - ETA: 1s - loss: 0 - ETA\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3378 - accuracy: 0.8711\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3089 - accuracy: 0.8790 0s - l\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2940 - accuracy: 0.8840\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2829 - accuracy: 0.8887\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2770 - accuracy: 0.8891 0s - loss: 0.2 - ETA: 0s - loss: 0.2770 - accuracy: 0.88 - ETA: 0s - loss: 0.2769 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2723 - accuracy: 0.8916\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2701 - accuracy: 0.8914 4s - loss: 0 - ETA: 3s - loss: 0.2 - ETA: \n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2648 - accuracy: 0.8942\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2615 - accuracy: 0.8951 0s - loss: 0.2618 -  - ETA: 0s - loss: 0.2614 - accu\n",
      "Score for fold 34: loss of 1.3149725903019056; accuracy of 71.67874396135265%, F1 of 0.7231715266953751\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 35 ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.4853 - accuracy: 0.8126\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3358 - accuracy: 0.8715\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.3074 - accuracy: 0.8818 1s - loss: 0.3\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2964 - accuracy: 0.8851\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2885 - accuracy: 0.8870: 0s - loss: 0.2887 - accuracy: \n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2836 - accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2789 - accuracy: 0.8903\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2735 - accuracy: 0.8925\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2709 - accuracy: 0.8927\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2692 - accuracy: 0.8941\n",
      "Score for fold 35: loss of 0.9211439458884547; accuracy of 77.29468599033817%, F1 of 0.7296281657391288\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 36 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.5081 - accuracy: 0.7994\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3513 - accuracy: 0.8624 1s - loss: 0.3536  - ETA: 0s - loss: 0.352\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 21s 3ms/step - loss: 0.3197 - accuracy: 0.8730\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.3035 - accuracy: 0.8793\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2951 - accuracy: 0.8827 \n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2860 - accuracy: 0.8868\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2800 - accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2748 - accuracy: 0.8911\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2711 - accuracy: 0.8932\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2674 - accuracy: 0.8956 0s - loss: 0.2674 - accuracy: 0. - ETA: 0s - loss: 0.2674 - accura\n",
      "Score for fold 36: loss of 0.6678197909972338; accuracy of 74.03381642512076%, F1 of 0.7016853123023704\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 37 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.4869 - accuracy: 0.8093\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.3370 - accuracy: 0.8702\n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.3089 - accuracy: 0.8801\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2957 - accuracy: 0.8850\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2847 - accuracy: 0.8887 0s\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2787 - accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2725 - accuracy: 0.8924 2s - loss: 0.273 - ETA: 1s - loss:\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2685 - accuracy: 0.8947\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2636 - accuracy: 0.8959\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2618 - accuracy: 0.8970\n",
      "Score for fold 37: loss of 0.549186755375047; accuracy of 79.30756843800322%, F1 of 0.7446683599916358\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 38 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.4906 - accuracy: 0.8084\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3393 - accuracy: 0.8680\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3093 - accuracy: 0.8795\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2940 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2861 - accuracy: 0.8885\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 16s 2ms/step - loss: 0.2801 - accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2758 - accuracy: 0.8914 0s - loss: 0.2757 - accu\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2711 - accuracy: 0.8943\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.2680 - accuracy: 0.8958\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2661 - accuracy: 0.8960\n",
      "Score for fold 38: loss of 0.5380013489902044; accuracy of 78.58293075684381%, F1 of 0.7569992157666181\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 39 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.4950 - accuracy: 0.8082\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3384 - accuracy: 0.8713\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3093 - accuracy: 0.8806 0s - loss: 0.3104 - accuracy: 0.88 - ETA: 0s -\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2919 - accuracy: 0.8872\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2805 - accuracy: 0.8917\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2752 - accuracy: 0.8933 0s - loss: 0.2749 - accuracy\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2692 - accuracy: 0.8953 0s - loss: 0.2696 - accu\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2656 - accuracy: 0.8962\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2625 - accuracy: 0.8977\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.2586 - accuracy: 0.8997 0s - loss: 0.2590 \n",
      "Score for fold 39: loss of 1.478617203185592; accuracy of 62.5805152979066%, F1 of 0.6211882535294345\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 40 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.4798 - accuracy: 0.8116\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.3280 - accuracy: 0.8728\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3018 - accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2860 - accuracy: 0.8865\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2782 - accuracy: 0.8890 0s - loss: 0.2783 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2696 - accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2632 - accuracy: 0.8953 0s - loss: 0.2633 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2602 - accuracy: 0.8964\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2578 - accuracy: 0.8972\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2559 - accuracy: 0.8985\n",
      "Score for fold 40: loss of 3.5158612222790167; accuracy of 20.853462157809986%, F1 of 0.1839961664458732\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 41 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.5015 - accuracy: 0.8044\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3387 - accuracy: 0.8707 0s - loss: 0.340\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3070 - accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2915 - accuracy: 0.8872\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2808 - accuracy: 0.8908 0s - los\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2755 - accuracy: 0.8926\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2679 - accuracy: 0.8952: 0s -\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2672 - accuracy: 0.8954 0s - loss: 0.2674 \n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2604 - accuracy: 0.8992 0s - los\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 17s 3ms/step - loss: 0.2602 - accuracy: 0.8994\n",
      "Score for fold 41: loss of 0.5426572277436746; accuracy of 82.10547504025764%, F1 of 0.8047940936250424\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 42 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.4947 - accuracy: 0.8066\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3363 - accuracy: 0.8698\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3062 - accuracy: 0.8806 \n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2944 - accuracy: 0.8848 1s - ETA: 0s - loss: 0.2944 - accuracy\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2834 - accuracy: 0.8893: 2s - loss: 0 - ETA:  - ETA: 0s - los\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2745 - accuracy: 0.8922 3s - l\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 12s 2ms/step - loss: 0.2696 - accuracy: 0.8939 0s - loss: 0.2697 - accuracy: 0.89\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2674 - accuracy: 0.8948\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.2626 - accuracy: 0.8973\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 20s 3ms/step - loss: 0.2596 - accuracy: 0.8981\n",
      "Score for fold 42: loss of 0.6741923458899346; accuracy of 77.47584541062803%, F1 of 0.7747965480209996\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 43 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.5049 - accuracy: 0.8023 0s - loss: 0\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.3464 - accuracy: 0.8668\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3171 - accuracy: 0.8760\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3028 - accuracy: 0.8822\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2917 - accuracy: 0.8872\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2849 - accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2790 - accuracy: 0.8919 \n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 11s 2ms/step - loss: 0.2756 - accuracy: 0.8921\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 15s 2ms/step - loss: 0.2715 - accuracy: 0.8950 0s\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2719 - accuracy: 0.8943\n",
      "Score for fold 43: loss of 0.6073256587721336; accuracy of 78.24074074074075%, F1 of 0.8003814165954211\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 44 ...\n",
      "Epoch 1/10\n",
      "6599/6599 [==============================] - 13s 2ms/step - loss: 0.4926 - accuracy: 0.8081  - ETA: 0s\n",
      "Epoch 2/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3367 - accuracy: 0.8716 0s - loss: 0.3372 - accuracy\n",
      "Epoch 3/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.3084 - accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "6599/6599 [==============================] - 10s 2ms/step - loss: 0.2963 - accuracy: 0.8837 2s - loss: 0.2 - E - ETA: 0s - loss: 0.296\n",
      "Epoch 5/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2867 - accuracy: 0.8877 8s - loss: - ETA: 5s - loss: 0\n",
      "Epoch 6/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2803 - accuracy: 0.8902: 0s - los\n",
      "Epoch 7/10\n",
      "6599/6599 [==============================] - 9s 1ms/step - loss: 0.2773 - accuracy: 0.8906\n",
      "Epoch 8/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2724 - accuracy: 0.8925 1s - loss: 0.2715  - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.2\n",
      "Epoch 9/10\n",
      "6599/6599 [==============================] - 10s 1ms/step - loss: 0.2720 - accuracy: 0.8923\n",
      "Epoch 10/10\n",
      "6599/6599 [==============================] - 14s 2ms/step - loss: 0.2674 - accuracy: 0.8941\n",
      "Score for fold 44: loss of 2.2710276028341494; accuracy of 45.93397745571659%, F1 of 0.49696744264929255\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 45 ...\n",
      "Epoch 1/10\n",
      "6676/6676 [==============================] - 15s 2ms/step - loss: 0.4863 - accuracy: 0.8114\n",
      "Epoch 2/10\n",
      "6676/6676 [==============================] - 16s 2ms/step - loss: 0.3379 - accuracy: 0.8718 0s - loss: 0.3384 - \n",
      "Epoch 3/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.3101 - accuracy: 0.8809\n",
      "Epoch 4/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2969 - accuracy: 0.8858\n",
      "Epoch 5/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2889 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2826 - accuracy: 0.8910 1s - loss: 0.2833 -  - ETA: 1s - loss: 0.2831 \n",
      "Epoch 7/10\n",
      "6676/6676 [==============================] - 12s 2ms/step - loss: 0.2756 - accuracy: 0.8936 0s -\n",
      "Epoch 8/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2717 - accuracy: 0.8945 0s - loss: 0.271\n",
      "Epoch 9/10\n",
      "6676/6676 [==============================] - 11s 2ms/step - loss: 0.2695 - accuracy: 0.8948\n",
      "Epoch 10/10\n",
      "6676/6676 [==============================] - 13s 2ms/step - loss: 0.2656 - accuracy: 0.8964\n",
      "Score for fold 45: loss of 1.133500351275103; accuracy of 53.05958132045089%, F1 of 0.45208514222606194\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7279043721971121 - Accuracy: 75.54347826086956% - F1:0.7217814579225815%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.1269179219365024 - Accuracy: 47.52415458937198% - F1:0.468007069855597%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.320735329694371 - Accuracy: 68.49838969404188% - F1:0.683833126314028%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.933882490777801 - Accuracy: 53.22061191626409% - F1:0.5224419520825596%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.5375648307758 - Accuracy: 44.14251207729469% - F1:0.33753154925833734%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.9995661624802054 - Accuracy: 79.81078904991948% - F1:0.7543788193342718%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.46170328207383 - Accuracy: 80.61594202898551% - F1:0.7716449154475873%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.1703203459896991 - Accuracy: 42.310789049919485% - F1:0.4673006181458903%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.6760739295832873 - Accuracy: 81.13929146537842% - F1:0.7497557798905224%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 2.2320190051010202 - Accuracy: 58.9573268921095% - F1:0.6213655161846366%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 11 - Loss: 4.032384144486454 - Accuracy: 30.3341384863124% - F1:0.18821804673781192%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 12 - Loss: 1.7746444513716138 - Accuracy: 49.73832528180354% - F1:0.41135910677684656%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 13 - Loss: 1.0430887365965937 - Accuracy: 73.45008051529791% - F1:0.7201527867016063%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 14 - Loss: 2.215742040205001 - Accuracy: 47.00080515297906% - F1:0.44876849599794666%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 15 - Loss: 1.0566557856772891 - Accuracy: 67.65297906602254% - F1:0.6705697758112833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 16 - Loss: 11.300597836249066 - Accuracy: 17.652979066022546% - F1:0.18009375974130296%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 17 - Loss: 0.4163872363251284 - Accuracy: 78.98550724637681% - F1:0.7710407821940911%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 18 - Loss: 2.004098236953747 - Accuracy: 28.26086956521739% - F1:0.208967700153585%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 19 - Loss: 0.7769496936891708 - Accuracy: 70.0684380032206% - F1:0.6656490127527848%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 20 - Loss: 0.3138655421869067 - Accuracy: 87.86231884057972% - F1:0.8434849181175504%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 21 - Loss: 3.5060109015735654 - Accuracy: 45.69243156199678% - F1:0.32905792068454315%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 22 - Loss: 1.7317743389059201 - Accuracy: 52.79790660225443% - F1:0.4474313219920702%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 23 - Loss: 1.2432553950738194 - Accuracy: 59.44041867954911% - F1:0.5532798843104608%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 24 - Loss: 0.1729148007478147 - Accuracy: 94.30354267310788% - F1:0.9381431156405197%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 25 - Loss: 0.3650140242678863 - Accuracy: 84.84299516908213% - F1:0.8186145405507917%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 26 - Loss: 1.3247153194847896 - Accuracy: 69.24315619967794% - F1:0.7027839962616054%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 27 - Loss: 0.946427883793474 - Accuracy: 63.808373590982285% - F1:0.6353394144206869%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 28 - Loss: 3.144970661684595 - Accuracy: 38.486312399355874% - F1:0.280539110671669%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 29 - Loss: 1.7210771195479637 - Accuracy: 79.99194847020934% - F1:0.8165881068915417%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 30 - Loss: 2.6585882966263075 - Accuracy: 64.69404186795491% - F1:0.5864122225581415%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 31 - Loss: 0.16119741820146508 - Accuracy: 95.28985507246377% - F1:0.9526325000181911%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 32 - Loss: 1.4505378906634154 - Accuracy: 63.72785829307569% - F1:0.6408824754105032%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 33 - Loss: 0.508922589749033 - Accuracy: 82.89049919484702% - F1:0.789970745068947%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 34 - Loss: 1.3149725903019056 - Accuracy: 71.67874396135265% - F1:0.7231715266953751%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 35 - Loss: 0.9211439458884547 - Accuracy: 77.29468599033817% - F1:0.7296281657391288%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 36 - Loss: 0.6678197909972338 - Accuracy: 74.03381642512076% - F1:0.7016853123023704%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 37 - Loss: 0.549186755375047 - Accuracy: 79.30756843800322% - F1:0.7446683599916358%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 38 - Loss: 0.5380013489902044 - Accuracy: 78.58293075684381% - F1:0.7569992157666181%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 39 - Loss: 1.478617203185592 - Accuracy: 62.5805152979066% - F1:0.6211882535294345%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 40 - Loss: 3.5158612222790167 - Accuracy: 20.853462157809986% - F1:0.1839961664458732%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 41 - Loss: 0.5426572277436746 - Accuracy: 82.10547504025764% - F1:0.8047940936250424%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 42 - Loss: 0.6741923458899346 - Accuracy: 77.47584541062803% - F1:0.7747965480209996%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 43 - Loss: 0.6073256587721336 - Accuracy: 78.24074074074075% - F1:0.8003814165954211%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 44 - Loss: 2.2710276028341494 - Accuracy: 45.93397745571659% - F1:0.49696744264929255%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 45 - Loss: 1.133500351275103 - Accuracy: 53.05958132045089% - F1:0.45208514222606194%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 63.9805868670603 (+- 18.941902175577923)\n",
      "> F1: 0.6108529374997277 (+- 0.20345928868376434)\n",
      "> Loss: 1.6282403124045137\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = X_train_df['Subject_ID'].values \n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "# LOGO\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax')) #4 outputs are possible \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              verbose=1)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    y_pred = np.argmax(model.predict(inputs[test]), axis=-1)\n",
    "    f1 = (f1_score(np.argmax(targets[test], axis=1), (y_pred), average = 'weighted'))\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%, F1 of {f1}')\n",
    "    f1_per_fold.append(f1)\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - F1:{f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')                     \n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please edit the name of the model below. This will be used to save the model and figures associated with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '10_TF_ANN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/10_TF_ANN/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save(f'saved_model/{model_name}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the predicted class for each test set sample by using the argmax function on the predicted probabilities that are output from our model. Argmax returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'saved_model/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369/369 [==============================] - 0s 825us/step - loss: 1.4306 - accuracy: 0.6684\n",
      "Test loss, Test acc: [1.430555538317202, 0.6684252902788372]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18825,  2402,   187,    52],\n",
       "       [ 1588,  8786,   753,  2264],\n",
       "       [  931,  3749,  2714,   281],\n",
       "       [  715,  2562,   165,  1222]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the confusion matrix to better understand the proportions of classes classified correctly and incorrectly for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87696823, 0.11189789, 0.00871145, 0.00242244],\n",
       "       [0.11858711, 0.65611231, 0.0562318 , 0.16906878],\n",
       "       [0.12130293, 0.48846906, 0.35361564, 0.03661238],\n",
       "       [0.15330189, 0.54931389, 0.03537736, 0.26200686]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm= cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 362
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = le.classes_, yticklabels = le.classes_)\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('Duke Data Rolled ANN - Confusion Matrix')\n",
    "plt.savefig(f'10_figures/{model_name}_CF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.668 \n",
      "F1 Score: 0.677\n"
     ]
    }
   ],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "print(f'Accuracy Score: {a_s:.3f} \\nF1 Score: {f1_s:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

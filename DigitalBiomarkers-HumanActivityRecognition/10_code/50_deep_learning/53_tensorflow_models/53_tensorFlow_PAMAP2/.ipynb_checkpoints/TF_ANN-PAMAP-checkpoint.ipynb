{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAMAP 2 Model 1: Artificial Neural Network\n",
    "#### Dataset Source: https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring\n",
    "#### Cleaning code source: https://www.kaggle.com/avrahamcalev/time-series-models-pamap2-dataset\n",
    "\n",
    "This is the same model as our ANN, but tested on the dataset PAMAP2 to validate the architecture of our model.\n",
    "\n",
    "INPUT: **pamap2_clean.csv**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('../../../../10_code/40_usable_data_for_models/42_PAMAP2/pamap2_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only relevant classes\n",
    "Classes 1, 3 and 4 represent the classes most representative for our data. \n",
    "\n",
    "**Classes**\n",
    "- 1: Lying (DB)\n",
    "- 3: Standing\n",
    "- 4: Walking\n",
    "\n",
    "Take a look at the readme in the data source to pick other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['activity_id'].isin([1, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activity_id'] = df['activity_id'].astype(int)\n",
    "df['id'] = df['id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and train sets (by Subject ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = list(df['id'].unique())\n",
    "random.shuffle(ID_list)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the test train split can be changed by changing the index below. For our purposes, n = 6 for train and n = 3 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['id'].isin(ID_list[:6])]\n",
    "test = df[df['id'].isin(ID_list[6:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463666, 55) (157549, 55)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for input into neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_act = ['id', 'heart_rate', 'hand_temperature',\n",
    "       'hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y',\n",
    "       'hand_3D_acceleration_16_z', 'hand_3D_acceleration_6_x',\n",
    "       'hand_3D_acceleration_6_y', 'hand_3D_acceleration_6_z',\n",
    "       'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y', 'hand_3D_gyroscope_z',\n",
    "       'hand_3D_magnetometer_x', 'hand_3D_magnetometer_y',\n",
    "       'hand_3D_magnetometer_z', 'hand_4D_orientation_x',\n",
    "       'hand_4D_orientation_y', 'hand_4D_orientation_z',\n",
    "       'hand_4D_orientation_w', 'chest_temperature',\n",
    "       'chest_3D_acceleration_16_x', 'chest_3D_acceleration_16_y',\n",
    "       'chest_3D_acceleration_16_z', 'chest_3D_acceleration_6_x',\n",
    "       'chest_3D_acceleration_6_y', 'chest_3D_acceleration_6_z',\n",
    "       'chest_3D_gyroscope_x', 'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z',\n",
    "       'chest_3D_magnetometer_x', 'chest_3D_magnetometer_y',\n",
    "       'chest_3D_magnetometer_z', 'chest_4D_orientation_x',\n",
    "       'chest_4D_orientation_y', 'chest_4D_orientation_z',\n",
    "       'chest_4D_orientation_w', 'ankle_temperature',\n",
    "       'ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y',\n",
    "       'ankle_3D_acceleration_16_z', 'ankle_3D_acceleration_6_x',\n",
    "       'ankle_3D_acceleration_6_y', 'ankle_3D_acceleration_6_z',\n",
    "       'ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z',\n",
    "       'ankle_3D_magnetometer_x', 'ankle_3D_magnetometer_y',\n",
    "       'ankle_3D_magnetometer_z', 'ankle_4D_orientation_x',\n",
    "       'ankle_4D_orientation_y', 'ankle_4D_orientation_z',\n",
    "       'ankle_4D_orientation_w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose the relevant columns, excluding only the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463666, 53) (157549, 53)\n"
     ]
    }
   ],
   "source": [
    "X_train = train[no_act]\n",
    "X_test = test[no_act]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "train_shape = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get y labels (Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['activity_id'].values\n",
    "y_test = test['activity_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding to Subject ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is applied so subject_ID, so that it may be used as a variable in our model. This allows the model to understand that testing data contains new subjects that are not present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621215, 59)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = X_train.append(X_test)\n",
    "full_df_dummy = pd.concat([full_df, pd.get_dummies(full_df['id'], prefix='id', drop_first = True)],axis=1)\n",
    "full_df_dummy = full_df_dummy.drop(['id'], axis = 1)\n",
    "full_df_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463666, 59) (463666,) (157549, 59) (157549,)\n"
     ]
    }
   ],
   "source": [
    "X_train = full_df_dummy.iloc[:train_shape]\n",
    "X_test = full_df_dummy.iloc[train_shape:]\n",
    "print(X_train.shape,  y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is used to change values without distorting differences in the range of values for each sensor. We do this because different sensor values are not in similar ranges of each other and if we did not scale the data, gradients may oscillate back and forth and take a long time before finding the local minimum. It may not be necessary for this data, but to be sure, we normalized the features.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$$z = \\frac{x-u}{s}$$\n",
    "\n",
    "Where u is the mean of the data, and s is the standard deviation of the data of a single sample. The scaling is fit on the training set and applied to both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow requires one-hot encoding for more than two classes in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = np_utils.to_categorical(y_train)\n",
    "y_test_dummy = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "#### Architecture:\n",
    "- 7 hidden **fully connected** layers with 128 nodes\n",
    "\n",
    "- The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- **Softmax** acitvation function - Used to generate probabilities for each class as an output in the final fully connected layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ADAM as our optimizer as it is computationally efficient and updates the learning rate on a per-parameter basis, based on a moving estimate per-parameter gradient, and the per-parameter squared gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "model_checkpoint = ModelCheckpoint('./models/HARnet.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model checkpoint is used to save weights of the best model. In the code below, we set the best model to be defined as the model with the least validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = dict.fromkeys({'id', 'acc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/5\n",
      "12260/12266 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9984WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12266/12266 [==============================] - 36s 3ms/step - loss: 0.0055 - accuracy: 0.9984\n",
      "Epoch 2/5\n",
      "12254/12266 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12266/12266 [==============================] - 42s 3ms/step - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "12259/12266 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12266/12266 [==============================] - 49s 4ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "12255/12266 [============================>.] - ETA: 0s - loss: 5.0892e-04 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12266/12266 [==============================] - 42s 3ms/step - loss: 5.0847e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "12261/12266 [============================>.] - ETA: 0s - loss: 1.0949e-08 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12266/12266 [==============================] - 42s 3ms/step - loss: 1.0944e-08 - accuracy: 1.0000\n",
      "Score for fold 1: loss of 37.284629821777344; accuracy of 69.47875618934631%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/5\n",
      "11938/11942 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9983WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11942/11942 [==============================] - 37s 3ms/step - loss: 0.0056 - accuracy: 0.9983\n",
      "Epoch 2/5\n",
      "11936/11942 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11942/11942 [==============================] - 38s 3ms/step - loss: 0.0016 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "11934/11942 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11942/11942 [==============================] - 38s 3ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "11935/11942 [============================>.] - ETA: 0s - loss: 1.1113e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11942/11942 [==============================] - 38s 3ms/step - loss: 1.1107e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "11941/11942 [============================>.] - ETA: 0s - loss: 2.8702e-11 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11942/11942 [==============================] - 44s 4ms/step - loss: 2.8701e-11 - accuracy: 1.0000\n",
      "Score for fold 2: loss of 2.0920610427856445; accuracy of 98.62642288208008%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/5\n",
      "12250/12252 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9983WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12252/12252 [==============================] - 54s 4ms/step - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 2/5\n",
      "12251/12252 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12252/12252 [==============================] - 43s 4ms/step - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "12247/12252 [============================>.] - ETA: 0s - loss: 7.7778e-04 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12252/12252 [==============================] - 43s 3ms/step - loss: 7.7748e-04 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "12238/12252 [============================>.] - ETA: 0s - loss: 7.1039e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12252/12252 [==============================] - 39s 3ms/step - loss: 7.0960e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "12239/12252 [============================>.] - ETA: 0s - loss: 3.6592e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12252/12252 [==============================] - 36s 3ms/step - loss: 3.6554e-04 - accuracy: 1.0000\n",
      "Score for fold 3: loss of 0.07124839723110199; accuracy of 99.03648495674133%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/5\n",
      "12192/12196 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12196/12196 [==============================] - 37s 3ms/step - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 2/5\n",
      "12182/12196 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12196/12196 [==============================] - 37s 3ms/step - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "12191/12196 [============================>.] - ETA: 0s - loss: 1.2521e-07 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12196/12196 [==============================] - 35s 3ms/step - loss: 1.2517e-07 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "12189/12196 [============================>.] - ETA: 0s - loss: 3.7101e-10 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12196/12196 [==============================] - 36s 3ms/step - loss: 3.7082e-10 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "12183/12196 [============================>.] - ETA: 0s - loss: 6.1156e-13 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12196/12196 [==============================] - 38s 3ms/step - loss: 6.1094e-13 - accuracy: 1.0000\n",
      "Score for fold 4: loss of 0.28446343541145325; accuracy of 98.81771206855774%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/5\n",
      "11816/11831 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9982WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11831/11831 [==============================] - 36s 3ms/step - loss: 0.0061 - accuracy: 0.9982\n",
      "Epoch 2/5\n",
      "11821/11831 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11831/11831 [==============================] - 32s 3ms/step - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "11816/11831 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11831/11831 [==============================] - 32s 3ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "11824/11831 [============================>.] - ETA: 0s - loss: 8.9915e-04 - accuracy: 0.9999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11831/11831 [==============================] - 32s 3ms/step - loss: 8.9864e-04 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      " 9497/11831 [=======================>......] - ETA: 7s - loss: 5.9792e-04 - accuracy: 0.9999"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Lists to store metrics\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "dict_list = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "groups = X_train_df['id'].values\n",
    "inputs = X_train\n",
    "targets = y_train_dummy\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "logo.get_n_splits(inputs, targets, groups)\n",
    "\n",
    "cv = logo.split(inputs, targets, groups)\n",
    "\n",
    "fold_no = 1\n",
    "for train, test in cv:\n",
    "    #Define the model architecture\n",
    "    network3 = models.Sequential()\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dropout(0.5))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(128, activation='relu', input_dim = 59))\n",
    "    network3.add(layers.Dense(5, activation='softmax'))\n",
    "    network3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = network3.fit(inputs[train], targets[train],\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              verbose=1,\n",
    "            callbacks = [model_checkpoint])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = network3.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {network3.metrics_names[0]} of {scores[0]}; {network3.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "    dict_new = dict1.copy()\n",
    "    dict_new['id'] = ID_list[i]\n",
    "    dict_new['acc'] = acc_per_fold[i]\n",
    "    dict_list.append(dict_new)\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argmax is used to select the output class with the highest probability in the output as these are the prediction labels for our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(network3.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = network3.evaluate(X_test, y_test_dummy, batch_size=128)\n",
    "print(\"Test loss, Test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is generated to observe where the model is classifying well and to see classes which the model is not classifying well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "cm = cm/cm.astype(np.float).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot = True, fmt = '.2f',cmap = 'Blues', xticklabels = ['Lying', 'Standing', 'Walking'], yticklabels = ['Lying', 'Standing', 'Walking'])\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel('Actual labels')\n",
    "plt.title('PAMAP2 ANN Confusion Matrix')\n",
    "plt.savefig('PAMAP2_ANN_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** score represents the proportion of correct classifications over all classifications.\n",
    "\n",
    "The **F1 score** is a composite metric of two other metrics:\n",
    "\n",
    "Specificity: proportion of correct 'positive predictions' over all 'positive' predictions.\n",
    "\n",
    "Sensitivity: number of correct 'negative' predictions over all 'negative' predictions.\n",
    "\n",
    "The F1 score gives insight as to whether all classes are predicted correctly at the same rate. A low F1 score and high accuracy can indicate that only a majority class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s = accuracy_score(y_test, y_pred)\n",
    "f1_s = f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(f'Accuracy Score: {a_s} \\nF1 Score: {f1_s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network3.save(\"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_list).to_csv('ANN_both_results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
